<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>EMQ X Kuiper与EdgeX Foundry集成实践</title>
    <url>/2020/12/01/EMQ-X-Kuiper%E4%B8%8EEdgeX-Foundry%E9%9B%86%E6%88%90%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<p>Kuiper是什么? EdgeX Foundry又是什么？</p>
<h1 id="Kuiper"><a href="#Kuiper" class="headerlink" title="Kuiper"></a>Kuiper</h1><p>EMQ X Kuiper 是 Golang 实现的轻量级物联网边缘分析、流式处理开源软件，可以运行在各类<strong>资源受限的边缘设备</strong>上。Kuiper 设计的一个主要目标就是将在云端运行的实时流式计算框架（比如 <a href="https://spark.apache.org/">Apache Spark</a>，<a href="https://storm.apache.org/">Apache Storm</a> 和 <a href="https://flink.apache.org/">Apache Flink</a> 等）迁移到边缘端。Kuiper 参考了上述云端流式处理项目的架构与实现，结合边缘流式数据处理的特点，采用了编写<strong>基于<code>源 (Source)</code>，<code>SQL (业务逻辑处理)</code>, <code>目标 (Sink)</code> 的规则引擎来实现边缘端的流式数据处理</strong>。<br>其架构如下：<br><img src="https://upload-images.jianshu.io/upload_images/10839544-82b09a3d6c9e5c33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<ul>
<li>源 (Sources) ：内置支持 MQTT 数据的接入，扩展支持与EdgeX Foundry集成</li>
<li>SQL：流式数据逻辑处理，具备完整的数据分析处理能力<ul>
<li>支持丰富的数据类型</li>
<li>支持4种时间窗口（滚动窗口、跳跃窗口、滑动窗口、会话窗口）</li>
<li>内置60+处理函数</li>
<li>提供类SQL语句对数据进行抽取、过滤、转换</li>
</ul>
</li>
<li>目标(Sinks)：内置支持 MQTT、HTTP等</li>
</ul>
<p>EMQ公司的相关产品，可以登陆其官网查询了解<br><img src="https://upload-images.jianshu.io/upload_images/10839544-b1643f3d02bea968.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="EdgeX-Foundry"><a href="#EdgeX-Foundry" class="headerlink" title="EdgeX Foundry"></a>EdgeX Foundry</h1><p>EdgeX Foundry是一个Linux 基金会运营的开源的，基于与硬件和操作系统完全无关的边缘计算物联网软件框架项目。其是一系列松耦合、开源的微服务集合，位于网络的边缘，可以与设备、传感器、执行器和其他物联网对象的物理世界进行交互。EdgeX Foundry 旨在创造一个互操作性、即插即用、模块化的物联网边缘计算的生态系统。<br>其架构如下：<br><img src="https://upload-images.jianshu.io/upload_images/10839544-fd06dab15f5598af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>从架构图可以看出：<br><strong>南侧（SouthBound）</strong>:在物理领域内的所有物联网对象，以及与这些设备、传感器、执行器和其他物联网对象直接通信并从中收集数据的网络边缘，统称为“南侧”。<br><strong>北侧（NorthBound）</strong>:将数据收集、存储、聚合、分析并转换为信息的云(或企业系统)，以及与云通信的网络部分称为网络的“北侧”。<br>因此，EdgeX使数据可以向北移动到云，也可以横向移动到其他网关，或返回到设备、传感器和执行器。<br>EdgeX的重要服务层及微服务：<br><img src="https://upload-images.jianshu.io/upload_images/10839544-d0e4c5a219ab700d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="1、安装edgex"><a href="#1、安装edgex" class="headerlink" title="1、安装edgex"></a>1、安装edgex</h2><p>参照官网文档：<a href="https://fuji-docs.edgexfoundry.org/Ch-GettingStartedUsers.html">https://fuji-docs.edgexfoundry.org/Ch-GettingStartedUsers.html</a><br>docker-compose启动<br><img src="https://upload-images.jianshu.io/upload_images/10839544-f5a8d9ed341b66b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>相关服务正常<br><img src="https://upload-images.jianshu.io/upload_images/10839544-8ab54653eca76cc1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="2、安装并启动kuiper"><a href="#2、安装并启动kuiper" class="headerlink" title="2、安装并启动kuiper"></a>2、安装并启动kuiper</h2><p>sudo docker run -d –name kuiper –restart always -e EDGEX_SERVER=10.0.105.143 -e EDGEX_PORT=5563 -e EDGEX_SERVICE_SERVER=<a href="http://10.0.105.143:48080/">http://10.0.105.143:48080</a> emqx/kuiper:0.2.1</p>
<p>环境变量具体参考：<a href="https://hub.docker.com/r/emqx/kuiper">https://hub.docker.com/r/emqx/kuiper</a> 中的说明<br>EDGEX_SERVER：edgex中zeromq的地址（zeromq集成到core data服务中了，可以看到core data服务暴露了两个端口一个5563，一个48080）<br>EDGEX_PORT：edgex中zeromq的端口<br>EDGEX_SERVICE_SERVER：edgex中core data的地址及端口<br>这里我使用的kuiper镜像为 emqx/kuiper:0.2.1，为目前最新版本</p>
<h2 id="3、进入kuiper容器"><a href="#3、进入kuiper容器" class="headerlink" title="3、进入kuiper容器"></a>3、进入kuiper容器</h2><p>sudo docker exec -it kuiper /bin/sh</p>
<h2 id="4、查看日志"><a href="#4、查看日志" class="headerlink" title="4、查看日志"></a>4、查看日志</h2><p>/kuiper # cat log/stream.log</p>
<h2 id="5、创建流，订阅来自edgex的消息流"><a href="#5、创建流，订阅来自edgex的消息流" class="headerlink" title="5、创建流，订阅来自edgex的消息流"></a>5、创建流，订阅来自edgex的消息流</h2><p>/kuiper # bin/cli create stream demo’() WITH (FORMAT=”JSON”, TYPE=”edgex”)’</p>
<h2 id="6、创建规则文件，内容如下"><a href="#6、创建规则文件，内容如下" class="headerlink" title="6、创建规则文件，内容如下"></a>6、创建规则文件，内容如下</h2><p>/kuiper # cat rule.txt</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;sql&quot;: &quot;SELECT * from demo GROUP BY TUMBLINGWINDOW(ss, 10)&quot;,</span><br><span class="line">  &quot;actions&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;mqtt&quot;: &#123;</span><br><span class="line">        &quot;server&quot;: &quot;tcp:&#x2F;&#x2F;broker.emqx.io:1883&quot;,</span><br><span class="line">        &quot;topic&quot;: &quot;result&quot;,</span><br><span class="line">        &quot;clientId&quot;: &quot;demo_001&quot;</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述规则：Kuiper将接受edgex的数据，执行select操作（每10s钟），然后将处理后的数据发布到tcp://broker.emqx.io:1883（也可以换成其他的，比如broker.hivemq.com或者自己搭建的EMQ X edge）<br><strong>注意：</strong>Kuiper SQL相关的参考，见<a href="https://docs.emqx.io/kuiper/latest/cn/sqls/overview.html">https://docs.emqx.io/kuiper/latest/cn/sqls/overview.html</a></p>
<h2 id="7、创建规则，命名为rule1"><a href="#7、创建规则，命名为rule1" class="headerlink" title="7、创建规则，命名为rule1"></a>7、创建规则，命名为rule1</h2><p>/kuiper # bin/cli create rule rule1 -f rule.txt</p>
<h2 id="8、查看日志，可以看到已经连通edgex，相关的规则也已经创建"><a href="#8、查看日志，可以看到已经连通edgex，相关的规则也已经创建" class="headerlink" title="8、查看日志，可以看到已经连通edgex，相关的规则也已经创建"></a>8、查看日志，可以看到已经连通edgex，相关的规则也已经创建</h2><p><img src="https://upload-images.jianshu.io/upload_images/10839544-4f07b16e3735e2dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="9、查看规则状态"><a href="#9、查看规则状态" class="headerlink" title="9、查看规则状态"></a>9、查看规则状态</h2><p><img src="https://upload-images.jianshu.io/upload_images/10839544-1349f6fd6210e627.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="10、使用mosquitto订阅broker-emqx-io中主题为result的消息"><a href="#10、使用mosquitto订阅broker-emqx-io中主题为result的消息" class="headerlink" title="10、使用mosquitto订阅broker.emqx.io中主题为result的消息"></a>10、使用mosquitto订阅broker.emqx.io中主题为result的消息</h2><p><img src="https://upload-images.jianshu.io/upload_images/10839544-4fdbca2eb0e7af64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>分析结果，发布到文件（<a href="https://github.com/emqx/kuiper/blob/master/docs/zh_CN/plugins/sinks/file.md">https://github.com/emqx/kuiper/blob/master/docs/zh_CN/plugins/sinks/file.md</a>）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;sql&quot;: &quot;SELECT * from demo&quot;,</span><br><span class="line">  &quot;actions&quot;: [</span><br><span class="line">   &#123;</span><br><span class="line">      &quot;file&quot;: &#123;</span><br><span class="line">        &quot;path&quot;: &quot;&#x2F;tmp&#x2F;result.txt&quot;,</span><br><span class="line">        &quot;interval&quot;: 5000</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>分析结果，发布到zmq（<a href="https://github.com/emqx/kuiper/blob/master/docs/zh_CN/plugins/sinks/zmq.md">https://github.com/emqx/kuiper/blob/master/docs/zh_CN/plugins/sinks/zmq.md</a>）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;sql&quot;: &quot;SELECT * from demo&quot;,</span><br><span class="line">  &quot;actions&quot;: [</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;zmq&quot;: &#123;</span><br><span class="line">       &quot;server&quot;: &quot;tcp:&#x2F;&#x2F;127.0.0.1:5563&quot;,</span><br><span class="line">       &quot;topic&quot;: &quot;temp&quot;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>分析结果，通过调用rest（<a href="https://github.com/emqx/kuiper/blob/master/docs/en_US/rules/sinks/rest.md">https://github.com/emqx/kuiper/blob/master/docs/en_US/rules/sinks/rest.md</a>）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;sql&quot;: &quot;SELECT * from demo&quot;,</span><br><span class="line">  &quot;actions&quot;: [</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;rest&quot;: &#123;</span><br><span class="line">      &quot;url&quot;: &quot;http:&#x2F;&#x2F;127.0.0.1:48082&#x2F;api&#x2F;v1&#x2F;device&#x2F;cc622d99-f835-4e94-b5cb-b1eff8699dc4&#x2F;command&#x2F;51fce08a-ae19-4bce-b431-b9f363bba705&quot;,       </span><br><span class="line">      &quot;method&quot;: &quot;post&quot;,</span><br><span class="line">      &quot;dataTemplate&quot;: &quot;\&quot;newKey\&quot;:\&quot;&#123;&#123;.key&#125;&#125;\&quot;&quot;,</span><br><span class="line">      &quot;sendSingle&quot;: true</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>这个类似于EdgeX中core services中的command服务</strong></p>
<p>分析结果，发布到edgex（<a href="https://github.com/emqx/kuiper/blob/master/docs/en_US/rules/sinks/edgex.md">https://github.com/emqx/kuiper/blob/master/docs/en_US/rules/sinks/edgex.md</a>）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;sql&quot;: &quot;SELECT * from demo&quot;,</span><br><span class="line">  &quot;actions&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;edgex&quot;: &#123;</span><br><span class="line">        &quot;protocol&quot;: &quot;tcp&quot;,</span><br><span class="line">        &quot;host&quot;: &quot;*&quot;,</span><br><span class="line">        &quot;port&quot;: 5571,</span><br><span class="line">        &quot;topic&quot;: &quot;application&quot;,</span><br><span class="line">        &quot;deviceName&quot;: &quot;kuiper&quot;,</span><br><span class="line">        &quot;contentType&quot;: &quot;application&#x2F;json&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>分析结果，发布到日志文件，默认在log/stream.log</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;sql&quot;: &quot;SELECT * from demo&quot;,</span><br><span class="line">  &quot;actions&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;log&quot;: &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>经验证，有些插件不完整</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;kuiper # .&#x2F;bin&#x2F;cli getstatus rule rule1</span><br><span class="line">Connecting to 127.0.0.1:20498... </span><br><span class="line">Stopped: cannot open &#x2F;kuiper&#x2F;plugins&#x2F;sinks&#x2F;File.so: plugin.Open(&quot;&#x2F;kuiper&#x2F;plugins&#x2F;sinks&#x2F;File.so&quot;): Error relocating &#x2F;kuiper&#x2F;plugins&#x2F;sinks&#x2F;File.so: __fprintf_chk: symbol not found.</span><br></pre></td></tr></table></figure>


<p>参考：</p>
<p>1、kuiper官方文档及github地址</p>
<p><a href="https://docs.emqx.io/kuiper/latest/cn/">https://docs.emqx.io/kuiper/latest/cn/</a></p>
<p><a href="https://github.com/emqx/kuiper">https://github.com/emqx/kuiper</a></p>
<p>2、kuiper集成edgex文档<a href="https://github.com/emqx/kuiper/blob/master/docs/en_US/edgex/edgex_rule_engine_tutorial.md">https://github.com/emqx/kuiper/blob/master/docs/en_US/edgex/edgex_rule_engine_tutorial.md</a></p>
]]></content>
      <categories>
        <category>kuiper</category>
        <category>edgex</category>
      </categories>
      <tags>
        <tag>kuiper</tag>
        <tag>edgex</tag>
      </tags>
  </entry>
  <entry>
    <title>Ubuntu搭建Kubernetes集群</title>
    <url>/2020/11/26/kubernetes_install_on_ubuntu/</url>
    <content><![CDATA[<p>本文介绍如何在Ubuntu系统上搭建Kubernetes集群。</p>
<h1 id="前提："><a href="#前提：" class="headerlink" title="前提："></a>前提：</h1><p>1、操作系统Ubuntu 19.10</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# uname -a</span><br><span class="line">Linux ubuntu-001 5.3.0-51-generic #44-Ubuntu SMP Wed Apr 22 21:09:44 UTC 2020 x86_64 x86_64 x86_64 GNU&#x2F;Linux</span><br><span class="line">root@ubuntu-001:~# lsb_release -a</span><br><span class="line">No LSB modules are available.</span><br><span class="line">Distributor ID:	Ubuntu</span><br><span class="line">Description:	Ubuntu 19.10</span><br><span class="line">Release:	19.10</span><br><span class="line">Codename:	eoan</span><br></pre></td></tr></table></figure>
<p>2、通过阿里云相关镜像源安装</p>
<p><strong>以下均为root用户下操作</strong></p>
<h1 id="操作步骤："><a href="#操作步骤：" class="headerlink" title="操作步骤："></a>操作步骤：</h1><h2 id="1、修改主机名"><a href="#1、修改主机名" class="headerlink" title="1、修改主机名"></a>1、修改主机名</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hostnamectl set-hostname ubuntu-001</span><br></pre></td></tr></table></figure>

<h2 id="2、关闭防火墙"><a href="#2、关闭防火墙" class="headerlink" title="2、关闭防火墙"></a>2、关闭防火墙</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt-get install ufw</span><br><span class="line">ufw disable</span><br></pre></td></tr></table></figure>

<h2 id="3、安装docker"><a href="#3、安装docker" class="headerlink" title="3、安装docker"></a>3、安装docker</h2><p><strong>安装必要的工具及GPG证书</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt-get update</span><br><span class="line">apt-get -y install apt-transport-https ca-certificates curl software-properties-common</span><br><span class="line">curl -fsSL https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;ubuntu&#x2F;gpg | sudo apt-key add -</span><br></pre></td></tr></table></figure>

<p><strong>配置阿里云docker源</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">add-apt-repository &quot;deb [arch&#x3D;amd64] https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;ubuntu $(lsb_release -cs) stable&quot;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>注意： 如果是arm架构系统，请对应更换， 如：add-apt-repository “deb [arch=arm64] <a href="https://mirrors.aliyun.com/docker-ce/linux/ubuntu">https://mirrors.aliyun.com/docker-ce/linux/ubuntu</a> $(lsb_release -cs) stable”</p>
<p><strong>查询docker版本（否则默认安装最新的版本）</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt-cache madison docker-ce</span><br><span class="line">#   docker-ce | 5:19.03.8~3-0~ubuntu-eoan | https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;ubuntu eoan&#x2F;stable amd64 Packages</span><br><span class="line">#   docker-ce | 5:19.03.7~3-0~ubuntu-eoan | https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;ubuntu eoan&#x2F;stable amd64 Packages</span><br><span class="line">#   docker-ce | 5:19.03.6~3-0~ubuntu-eoan | https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;docker-ce&#x2F;linux&#x2F;ubuntu eoan&#x2F;stable amd64 Packages</span><br></pre></td></tr></table></figure>

<p><strong>安装指定版本docker</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt-get -y update</span><br><span class="line">apt-get -y install docker-ce&#x3D;5:19.03.8~3-0~ubuntu-eoan</span><br></pre></td></tr></table></figure>

<p><strong>启动docker</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable docker &amp;&amp; systemctl start docker</span><br></pre></td></tr></table></figure>

<p><strong>查询docker服务状态</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl status docker</span><br></pre></td></tr></table></figure>

<p><strong>查看docker版本</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">docker version</span><br></pre></td></tr></table></figure>

<h2 id="4、安装kubelet、kubeadm-和-kubectl"><a href="#4、安装kubelet、kubeadm-和-kubectl" class="headerlink" title="4、安装kubelet、kubeadm 和 kubectl"></a>4、安装kubelet、kubeadm 和 kubectl</h2><p><strong>配置kubernetes.repo的源，由于官方源国内无法访问，这里使用阿里云源</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt&#x2F;doc&#x2F;apt-key.gpg | apt-key add - </span><br><span class="line">cat &lt;&lt;EOF &gt;&#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;kubernetes.list</span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt&#x2F; kubernetes-xenial main</span><br><span class="line">EOF  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>在所有节点上安装指定版本 kubelet、kubeadm 和 kubectl</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt-get update</span><br><span class="line">apt-cache madison kubectl</span><br><span class="line">apt-cache madison kubeadm</span><br><span class="line">apt-cache madison kubelet</span><br><span class="line">apt-get install -y kubelet&#x3D;1.18.0-00 kubeadm&#x3D;1.18.0-00 kubectl&#x3D;1.18.0-00</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>启动kubelet服务</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable kubelet &amp;&amp; systemctl start kubelet </span><br></pre></td></tr></table></figure>

<h2 id="5、关闭swap分区"><a href="#5、关闭swap分区" class="headerlink" title="5、关闭swap分区"></a>5、关闭swap分区</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">swapoff -a</span><br></pre></td></tr></table></figure>

<h2 id="6、初始化master节点"><a href="#6、初始化master节点" class="headerlink" title="6、初始化master节点"></a>6、初始化master节点</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubeadm init --image-repository registry.aliyuncs.com&#x2F;google_containers --kubernetes-version v1.18.0 --pod-network-cidr&#x3D;10.244.0.0&#x2F;16 --token-ttl 0 --ignore-preflight-errors&#x3D;Swap</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# kubeadm init --image-repository registry.aliyuncs.com&#x2F;google_containers --kubernetes-version v1.18.0 --pod-network-cidr&#x3D;10.244.0.0&#x2F;16 --token-ttl 0 --ignore-preflight-errors&#x3D;Swap</span><br><span class="line">W0507 17:11:30.761887   14961 configset.go:202] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]</span><br><span class="line">[init] Using Kubernetes version: v1.18.0</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">	[WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;setup&#x2F;cri&#x2F;</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[certs] Using certificateDir folder &quot;&#x2F;etc&#x2F;kubernetes&#x2F;pki&quot;</span><br><span class="line">[certs] Generating &quot;ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [ubuntu-001 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 10.0.105.107]</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd&#x2F;ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd&#x2F;server&quot; certificate and key</span><br><span class="line">[certs] etcd&#x2F;server serving cert is signed for DNS names [ubuntu-001 localhost] and IPs [10.0.105.107 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd&#x2F;peer&quot; certificate and key</span><br><span class="line">[certs] etcd&#x2F;peer serving cert is signed for DNS names [ubuntu-001 localhost] and IPs [10.0.105.107 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd&#x2F;healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;sa&quot; key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;&#x2F;etc&#x2F;kubernetes&quot;</span><br><span class="line">[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[control-plane] Using manifest folder &quot;&#x2F;etc&#x2F;kubernetes&#x2F;manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">W0507 17:11:34.680144   14961 manifests.go:225] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">W0507 17:11:34.682546   14961 manifests.go:225] the default kube-apiserver authorization-mode is &quot;Node,RBAC&quot;; using &quot;Node,RBAC&quot;</span><br><span class="line">[etcd] Creating static Pod manifest for local etcd in &quot;&#x2F;etc&#x2F;kubernetes&#x2F;manifests&quot;</span><br><span class="line">[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;&#x2F;etc&#x2F;kubernetes&#x2F;manifests&quot;. This can take up to 4m0s</span><br><span class="line">[apiclient] All control plane components are healthy after 20.502602 seconds</span><br><span class="line">[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.18&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[upload-certs] Skipping phase. Please see --upload-certs</span><br><span class="line">[mark-control-plane] Marking the node ubuntu-001 as control-plane by adding the label &quot;node-role.kubernetes.io&#x2F;master&#x3D;&#39;&#39;&quot;</span><br><span class="line">[mark-control-plane] Marking the node ubuntu-001 as control-plane by adding the taints [node-role.kubernetes.io&#x2F;master:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: 10y2lc.v55p1f47j3lp15gg</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[kubelet-finalize] Updating &quot;&#x2F;etc&#x2F;kubernetes&#x2F;kubelet.conf&quot; to point to a rotatable kubelet client certificate and key</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME&#x2F;.kube</span><br><span class="line">  sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;cluster-administration&#x2F;addons&#x2F;</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 10.0.105.107:6443 --token 10y2lc.v55p1f47j3lp15gg \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:1093b027cf31d755dcaa7109ba890962c140e2d6e9c46bf4207a0c519ce7bf36 </span><br><span class="line"></span><br></pre></td></tr></table></figure>
<p>按照kubeadm init成功后打印提示，继续操作：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mkdir -p $HOME&#x2F;.kube</span><br><span class="line">sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br></pre></td></tr></table></figure>

<p>kubectl get nodes查询到节点处于NotReady状态，是因为网络插件还未就位，也就是这里要求运行的</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;cluster-administration&#x2F;addons&#x2F;</span><br></pre></td></tr></table></figure>

<p>如果要暂时忽略让节点Ready，将如下文件的–network-plugin=cni字段去掉, (该文件是在kubeadm init或者kubeadm join过程中生成的), 修改完后重启kubelet即可(systemctl restart kubelet)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# cat &#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;kubeadm-flags.env</span><br><span class="line">KUBELET_KUBEADM_ARGS&#x3D;&quot;--cgroup-driver&#x3D;cgroupfs --network-plugin&#x3D;cni --pod-infra-container-image&#x3D;registry.aliyuncs.com&#x2F;google_containers&#x2F;pause:3.2 --resolv-conf&#x3D;&#x2F;run&#x2F;systemd&#x2F;resolve&#x2F;resolv.conf&quot;</span><br></pre></td></tr></table></figure>
<p>安装flannel，kubectl apply -f kube-flannel.yaml</p>
<h2 id="7、join-node节点"><a href="#7、join-node节点" class="headerlink" title="7、join node节点"></a>7、join node节点</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-002:~# kubeadm join 10.0.105.107:6443 --token 10y2lc.v55p1f47j3lp15gg --discovery-token-ca-cert-hash sha256:1093b027cf31d755dcaa7109ba890962c140e2d6e9c46bf4207a0c519ce7bf36</span><br><span class="line">W0507 17:16:18.524687   26000 join.go:346] [preflight] WARNING: JoinControlPane.controlPlane settings will be ignored when control-plane flag is not set.</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">	[WARNING IsDockerSystemdCheck]: detected &quot;cgroupfs&quot; as the Docker cgroup driver. The recommended driver is &quot;systemd&quot;. Please follow the guide at https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;setup&#x2F;cri&#x2F;</span><br><span class="line">[preflight] Reading configuration from the cluster...</span><br><span class="line">[preflight] FYI: You can look at this config file with &#39;kubectl -n kube-system get cm kubeadm-config -oyaml&#39;</span><br><span class="line">[kubelet-start] Downloading configuration for the kubelet from the &quot;kubelet-config-1.18&quot; ConfigMap in the kube-system namespace</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml&quot;</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[kubelet-start] Waiting for the kubelet to perform the TLS Bootstrap...</span><br><span class="line"></span><br><span class="line">This node has joined the cluster:</span><br><span class="line">* Certificate signing request was sent to apiserver and a response was received.</span><br><span class="line">* The Kubelet was informed of the new secure connection details.</span><br><span class="line"></span><br><span class="line">Run &#39;kubectl get nodes&#39; on the control-plane to see this node join the cluster.</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h2 id="查询所有node及pod状态"><a href="#查询所有node及pod状态" class="headerlink" title="查询所有node及pod状态"></a>查询所有node及pod状态</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl get node -o wide</span><br><span class="line">kubectl get pod --all-namespaces -o wide</span><br></pre></td></tr></table></figure>

<h2 id="增加kubectl命令自动补全功能"><a href="#增加kubectl命令自动补全功能" class="headerlink" title="增加kubectl命令自动补全功能"></a>增加kubectl命令自动补全功能</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">source &lt;(kubectl completion bash)</span><br></pre></td></tr></table></figure>

<h2 id="默认master节点不会调度pod，去掉此限制-可选做"><a href="#默认master节点不会调度pod，去掉此限制-可选做" class="headerlink" title="默认master节点不会调度pod，去掉此限制(可选做)"></a>默认master节点不会调度pod，去掉此限制(可选做)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl taint node ubuntu-001 node-role.kubernetes.io&#x2F;master:NoSchedule-</span><br></pre></td></tr></table></figure>
<p>ubuntu-001为主机名</p>
<h2 id="node节点ROLES默认显示none，将其修改为显示worker-可选做"><a href="#node节点ROLES默认显示none，将其修改为显示worker-可选做" class="headerlink" title="node节点ROLES默认显示none，将其修改为显示worker(可选做)"></a>node节点ROLES默认显示none，将其修改为显示worker(可选做)</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kubectl label node ubuntu-002 node-role.kubernetes.io&#x2F;worker&#x3D; --overwrite</span><br><span class="line">kubectl label node ubuntu-003 node-role.kubernetes.io&#x2F;worker&#x3D; --overwrite</span><br><span class="line">root@ubuntu-001:~# kubectl get node -o wide</span><br><span class="line">NAME         STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE       KERNEL-VERSION     CONTAINER-RUNTIME</span><br><span class="line">ubuntu-001   Ready    master   16h   v1.18.0   10.0.105.107   &lt;none&gt;        Ubuntu 19.10   5.3.0-51-generic   docker:&#x2F;&#x2F;19.3.8</span><br><span class="line">ubuntu-002   Ready    worker   16h   v1.18.0   10.0.105.21    &lt;none&gt;        Ubuntu 19.10   5.3.0-46-generic   docker:&#x2F;&#x2F;19.3.8</span><br><span class="line">ubuntu-003   Ready    worker   61m   v1.18.0   10.0.105.62    &lt;none&gt;        Ubuntu 19.10   5.3.0-46-generic   docker:&#x2F;&#x2F;19.3.8</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>云计算</tag>
        <tag>边缘计算</tag>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes的python-client体验</title>
    <url>/2020/12/01/kubernetes%E7%9A%84python-client%E4%BD%93%E9%AA%8C/</url>
    <content><![CDATA[<p>kubernetes提供了丰富的API接口，用户不仅可以通过CLI命令行去调用API接口，而且还可以通过client库方便的调用API接口</p>
<p><a href="https://kubernetes.io/docs/reference/using-api/client-libraries/%EF%BC%8C%E6%8F%90%E4%BE%9B%E4%BA%86%E5%AE%98%E6%96%B9%E6%94%AF%E6%8C%81%E7%9A%84%E5%92%8C%E7%A4%BE%E5%8C%BA%E6%94%AF%E6%8C%81%E7%9A%84%E4%B8%8D%E5%90%8C%E8%AF%AD%E8%A8%80%E7%9A%84client%E5%BA%93">https://kubernetes.io/docs/reference/using-api/client-libraries/，提供了官方支持的和社区支持的不同语言的client库</a></p>
<p>本文将着重体验python版本的client库的使用</p>
<h2 id="源码安装"><a href="#源码安装" class="headerlink" title="源码安装"></a>源码安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone --recursive https:&#x2F;&#x2F;github.com&#x2F;kubernetes-client&#x2F;python.git</span><br><span class="line">cd python</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure>

<h2 id="pip安装"><a href="#pip安装" class="headerlink" title="pip安装"></a>pip安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install kubernetes</span><br></pre></td></tr></table></figure>

<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from kubernetes import client, config</span><br><span class="line"></span><br><span class="line"># Configs can be set in Configuration class directly or using helper utility</span><br><span class="line">config.load_kube_config()</span><br><span class="line"></span><br><span class="line">v1 &#x3D; client.CoreV1Api()</span><br><span class="line">print(&quot;Listing pods with their IPs:&quot;)</span><br><span class="line">ret &#x3D; v1.list_pod_for_all_namespaces(watch&#x3D;False)</span><br><span class="line">for i in ret.items:</span><br><span class="line">    print(&quot;%s\t%s\t%s&quot; % (i.status.pod_ip, i.metadata.namespace, i.metadata.name))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~&#x2F;python# python test_list_pods.py</span><br><span class="line">Listing pods with their IPs:</span><br><span class="line">10.244.0.8      default edgex-app-service-configurable-rules-d66d779c7-ksw7c</span><br><span class="line">10.244.0.9      default edgex-core-command-6f7cd7d57f-tnmmp</span><br><span class="line">10.244.0.18     default edgex-core-consul-64b88766-7q7kr</span><br><span class="line">10.244.0.10     default edgex-core-data-57b99d7f89-gpfgs</span><br><span class="line">10.244.0.11     default edgex-core-metadata-58dcc95ff4-9ssjg</span><br><span class="line">10.244.0.12     default edgex-device-rest-7944449548-nzcfj</span><br><span class="line">10.244.0.13     default edgex-device-virtual-6c7b8d7499-csz85</span><br><span class="line">10.244.0.14     default edgex-redis-67fffb7666-pvqgw</span><br><span class="line">10.244.0.15     default edgex-support-notifications-58dc7f76b4-tsdxn</span><br><span class="line">10.244.0.16     default edgex-support-rulesengine-8657b7c988-zr8pv</span><br><span class="line">10.244.0.17     default edgex-support-scheduler-d6fd467db-tfqw7</span><br><span class="line">10.244.0.19     default edgex-sys-mgmt-agent-79b476d8dc-2jdw4</span><br><span class="line">10.244.0.20     default edgex-ui-8cfc7f95d-vvhzc</span><br><span class="line">10.244.0.4      kube-system     coredns-f9fd979d6-cqpss</span><br><span class="line">10.244.0.3      kube-system     coredns-f9fd979d6-kvbzm</span><br><span class="line">172.18.0.2      kube-system     etcd-kind-control-plane</span><br><span class="line">172.18.0.2      kube-system     kindnet-br2qc</span><br><span class="line">172.18.0.2      kube-system     kube-apiserver-kind-control-plane</span><br><span class="line">172.18.0.2      kube-system     kube-controller-manager-kind-control-plane</span><br><span class="line">172.18.0.2      kube-system     kube-proxy-4chlm</span><br><span class="line">172.18.0.2      kube-system     kube-scheduler-kind-control-plane</span><br><span class="line">10.244.0.7      kube-system     metrics-server-8dc97c749-h6w8n</span><br><span class="line">10.244.0.6      kubernetes-dashboard    dashboard-metrics-scraper-7b59f7d4df-5rwc6</span><br><span class="line">10.244.0.5      kubernetes-dashboard    kubernetes-dashboard-665f4c5ff-l6tqn</span><br><span class="line">10.244.0.2      local-path-storage      local-path-provisioner-78776bfc44-xl4ht</span><br><span class="line">root@ubuntu:~&#x2F;python#</span><br><span class="line">root@ubuntu:~&#x2F;python# kubectl get pods --all-namespaces -o wide</span><br><span class="line">NAMESPACE              NAME                                                   READY   STATUS             RESTARTS   AGE    IP            NODE                 NOMINATED NODE   READINESS GATES</span><br><span class="line">default                edgex-app-service-configurable-rules-d66d779c7-ksw7c   1&#x2F;1     Running            14         21h    10.244.0.8    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-core-command-6f7cd7d57f-tnmmp                    1&#x2F;1     Running            21         21h    10.244.0.9    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-core-consul-64b88766-7q7kr                       1&#x2F;1     Running            0          21h    10.244.0.18   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-core-data-57b99d7f89-gpfgs                       1&#x2F;1     Running            19         21h    10.244.0.10   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-core-metadata-58dcc95ff4-9ssjg                   1&#x2F;1     Running            21         21h    10.244.0.11   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-device-rest-7944449548-nzcfj                     1&#x2F;1     Running            15         21h    10.244.0.12   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-device-virtual-6c7b8d7499-csz85                  1&#x2F;1     Running            13         21h    10.244.0.13   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-redis-67fffb7666-pvqgw                           1&#x2F;1     Running            0          21h    10.244.0.14   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-support-notifications-58dc7f76b4-tsdxn           1&#x2F;1     Running            13         21h    10.244.0.15   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-support-rulesengine-8657b7c988-zr8pv             1&#x2F;1     Running            0          21h    10.244.0.16   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-support-scheduler-d6fd467db-tfqw7                1&#x2F;1     Running            14         21h    10.244.0.17   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-sys-mgmt-agent-79b476d8dc-2jdw4                  1&#x2F;1     Running            0          21h    10.244.0.19   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-ui-8cfc7f95d-vvhzc                               0&#x2F;1     ImagePullBackOff   0          21h    10.244.0.20   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            coredns-f9fd979d6-cqpss                                1&#x2F;1     Running            0          4d6h   10.244.0.4    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            coredns-f9fd979d6-kvbzm                                1&#x2F;1     Running            0          4d6h   10.244.0.3    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            etcd-kind-control-plane                                1&#x2F;1     Running            0          4d6h   172.18.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            kindnet-br2qc                                          1&#x2F;1     Running            0          4d6h   172.18.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            kube-apiserver-kind-control-plane                      1&#x2F;1     Running            0          23h    172.18.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            kube-controller-manager-kind-control-plane             1&#x2F;1     Running            1          4d6h   172.18.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            kube-proxy-4chlm                                       1&#x2F;1     Running            0          4d6h   172.18.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            kube-scheduler-kind-control-plane                      1&#x2F;1     Running            1          4d6h   172.18.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            metrics-server-8dc97c749-h6w8n                         1&#x2F;1     Running            0          21h    10.244.0.7    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard   dashboard-metrics-scraper-7b59f7d4df-5rwc6             1&#x2F;1     Running            0          21h    10.244.0.6    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard   kubernetes-dashboard-665f4c5ff-l6tqn                   1&#x2F;1     Running            0          21h    10.244.0.5    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">local-path-storage     local-path-provisioner-78776bfc44-xl4ht                1&#x2F;1     Running            1          4d6h   10.244.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>更多的examples：<a href="https://github.com/kubernetes-client/python/tree/master/example">https://github.com/kubernetes-client/python/tree/master/example</a></p>
]]></content>
      <categories>
        <category>python</category>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes部署httpd服务</title>
    <url>/2020/12/02/kubernetes%E9%83%A8%E7%BD%B2httpd%E6%9C%8D%E5%8A%A1/</url>
    <content><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# cat httpd.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: httpd</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      run: httpd</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        run: httpd</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: httpd</span><br><span class="line">        image: httpd</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: httpd-svc</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    run: httpd</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    nodePort: 30000</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl apply -f httpd.yaml</span><br><span class="line">deployment.apps&#x2F;httpd created</span><br><span class="line">service&#x2F;httpd-svc created</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl get pods</span><br><span class="line">NAME                    READY   STATUS    RESTARTS   AGE</span><br><span class="line">httpd-ff8d77b9b-zz4d4   1&#x2F;1     Running   0          10s</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">httpd-svc    NodePort    10.97.219.48   &lt;none&gt;        8080:30000&#x2F;TCP   14s</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443&#x2F;TCP          69d</span><br><span class="line"></span><br><span class="line">进入pod</span><br><span class="line">root@ubuntu-001:~# kubectl exec -it httpd-ff8d77b9b-zz4d4 -- &#x2F;bin&#x2F;bash</span><br><span class="line">root@httpd-ff8d77b9b-zz4d4:&#x2F;usr&#x2F;local&#x2F;apache2#</span><br><span class="line"></span><br><span class="line">root@httpd-ff8d77b9b-zz4d4:&#x2F;usr&#x2F;local&#x2F;apach2# cat htdocs&#x2F;index.html</span><br><span class="line">&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;&#x2F;h1&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure>
<p>浏览器输入：http://主机IP:30000/</p>
<p><img src="/images/2020-12-02/1.jpg" alt="1"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# cat httpd.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: httpd</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      run: httpd</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        run: httpd</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: httpd</span><br><span class="line">        image: httpd</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: host-path</span><br><span class="line">          mountPath: &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs</span><br><span class="line">      volumes:</span><br><span class="line">      - name: host-path</span><br><span class="line">        hostPath:</span><br><span class="line">           path: &#x2F;root&#x2F;test</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: httpd-svc</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    run: httpd</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    nodePort: 30000</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line">这里进一步使用hostPath将本地test目录，映射到容器的&#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs目录下</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl apply -f httpd.yaml</span><br><span class="line">deployment.apps&#x2F;httpd configured</span><br><span class="line">service&#x2F;httpd-svc configured</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl get pods</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">httpd-589bbcf648-28n9x   1&#x2F;1     Running   0          7m19s</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">httpd-svc    NodePort    10.104.70.26   &lt;none&gt;        8080:30000&#x2F;TCP   3h52m</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443&#x2F;TCP          69d</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>浏览器输入：http://主机IP:30000/</p>
<p><img src="/images/2020-12-02/2.jpg" alt="2"></p>
<p>特别说明：这里我使用了hostPath，且httpd副本我修改成了1，实际上，httpd pod可能落在其他node上，所以严格来说，hostPath难以保证所在node上test的存在</p>
<p>可以考虑使用glusterfs做统一的后端存储</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>如何使用github作为Helm的chart仓库</title>
    <url>/2020/12/01/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8github%E4%BD%9C%E4%B8%BAHelm%E7%9A%84chart%E4%BB%93%E5%BA%93/</url>
    <content><![CDATA[<h1 id="前提条件："><a href="#前提条件：" class="headerlink" title="前提条件："></a>前提条件：</h1><p>1、已安装git<br>2、已注册github账户<br>3、安装helm（推荐helm3，下载地址：[<a href="https://github.com/helm/helm/releases/(https://github.com/helm/helm/releases/)%EF%BC%89">https://github.com/helm/helm/releases/(https://github.com/helm/helm/releases/)）</a></p>
<h1 id="操作步骤："><a href="#操作步骤：" class="headerlink" title="操作步骤："></a>操作步骤：</h1><h2 id="1、在github创建仓库，取名为helm-chart"><a href="#1、在github创建仓库，取名为helm-chart" class="headerlink" title="1、在github创建仓库，取名为helm-chart"></a>1、在github创建仓库，取名为helm-chart</h2><p><img src="https://upload-images.jianshu.io/upload_images/10839544-8d2d6093ca2aff4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="创建仓库"></p>
<h2 id="2、下载helm-chart仓库到本地"><a href="#2、下载helm-chart仓库到本地" class="headerlink" title="2、下载helm-chart仓库到本地"></a>2、下载helm-chart仓库到本地</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zj@zj-Z390-UD:~&#x2F;code$ git clone https:&#x2F;&#x2F;github.com&#x2F;yushanjin&#x2F;helm-chart.git</span><br><span class="line">正克隆到 &#39;helm-chart&#39;...</span><br><span class="line">remote: Enumerating objects: 3, done.</span><br><span class="line">remote: Counting objects: 100% (3&#x2F;3), done.</span><br><span class="line">remote: Compressing objects: 100% (2&#x2F;2), done.</span><br><span class="line">remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0</span><br><span class="line">展开对象中: 100% (3&#x2F;3), 完成.</span><br></pre></td></tr></table></figure>
<h2 id="3、创建chart目录"><a href="#3、创建chart目录" class="headerlink" title="3、创建chart目录"></a>3、创建chart目录</h2> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zj@zj-Z390-UD:~&#x2F;code$ cd helm-chart&#x2F;</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ ll</span><br><span class="line">总用量 16</span><br><span class="line">drwxr-xr-x 3 zj zj 4096 4月   8 14:04 .&#x2F;</span><br><span class="line">drwxr-xr-x 4 zj zj 4096 4月   8 14:04 ..&#x2F;</span><br><span class="line">drwxr-xr-x 8 zj zj 4096 4月   8 14:04 .git&#x2F;</span><br><span class="line">-rw-r--r-- 1 zj zj   77 4月   8 14:04 README.md</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm create test</span><br><span class="line">Creating test</span><br><span class="line"></span><br><span class="line">注意：可以在终端执行 source &lt;(helm completion bash)，启动helm命令自动补全功能</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ tree test&#x2F;</span><br><span class="line">test&#x2F;</span><br><span class="line">├── charts</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── serviceaccount.yaml</span><br><span class="line">│   ├── service.yaml</span><br><span class="line">│   └── tests</span><br><span class="line">│       └── test-connection.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">3 directories, 9 files</span><br></pre></td></tr></table></figure>
<h2 id="4、打包chart包"><a href="#4、打包chart包" class="headerlink" title="4、打包chart包"></a>4、打包chart包</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm package test&#x2F;</span><br><span class="line">Successfully packaged chart and saved it to: &#x2F;home&#x2F;zj&#x2F;code&#x2F;helm-chart&#x2F;test-0.1.0.tgz</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm repo index --url https:&#x2F;&#x2F;yushanjin.github.io&#x2F;helm-chart&#x2F; .</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ cat index.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">entries:</span><br><span class="line">  test:</span><br><span class="line">  - apiVersion: v2</span><br><span class="line">    appVersion: 1.16.0</span><br><span class="line">    created: &quot;2020-04-08T14:19:31.079089336+08:00&quot;</span><br><span class="line">    description: A Helm chart for Kubernetes</span><br><span class="line">    digest: 6de7ab4f2da011db9ef8e1def8d2fca7d4e79bb4e81e46152a8d3a2969b73820</span><br><span class="line">    name: test</span><br><span class="line">    type: application</span><br><span class="line">    urls:</span><br><span class="line">    - https:&#x2F;&#x2F;yushanjin.github.io&#x2F;helm-chart&#x2F;test-0.1.0.tgz</span><br><span class="line">    version: 0.1.0</span><br><span class="line">generated: &quot;2020-04-08T14:19:31.078672885+08:00&quot;</span><br></pre></td></tr></table></figure>
<h2 id="5、push到github"><a href="#5、push到github" class="headerlink" title="5、push到github"></a>5、push到github</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ git status</span><br><span class="line">位于分支 master</span><br><span class="line">您的分支与上游分支 &#39;origin&#x2F;master&#39; 一致。</span><br><span class="line"></span><br><span class="line">未跟踪的文件:</span><br><span class="line">  （使用 &quot;git add &lt;文件&gt;...&quot; 以包含要提交的内容）</span><br><span class="line"></span><br><span class="line">	index.yaml</span><br><span class="line">	test-0.1.0.tgz</span><br><span class="line">	test&#x2F;</span><br><span class="line"></span><br><span class="line">提交为空，但是存在尚未跟踪的文件（使用 &quot;git add&quot; 建立跟踪）</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ git add .</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ git status</span><br><span class="line">位于分支 master</span><br><span class="line">您的分支与上游分支 &#39;origin&#x2F;master&#39; 一致。</span><br><span class="line"></span><br><span class="line">要提交的变更：</span><br><span class="line">  （使用 &quot;git reset HEAD &lt;文件&gt;...&quot; 以取消暂存）</span><br><span class="line"></span><br><span class="line">	新文件：   index.yaml</span><br><span class="line">	新文件：   test-0.1.0.tgz</span><br><span class="line">	新文件：   test&#x2F;.helmignore</span><br><span class="line">	新文件：   test&#x2F;Chart.yaml</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;NOTES.txt</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;_helpers.tpl</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;deployment.yaml</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;ingress.yaml</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;service.yaml</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;serviceaccount.yaml</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;tests&#x2F;test-connection.yaml</span><br><span class="line">	新文件：   test&#x2F;values.yaml</span><br><span class="line"></span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ git commit -m &quot;创建test的chart包&quot;</span><br><span class="line">[master 5ae813c] 创建test的chart包</span><br><span class="line"> 12 files changed, 348 insertions(+)</span><br><span class="line"> create mode 100644 index.yaml</span><br><span class="line"> create mode 100644 test-0.1.0.tgz</span><br><span class="line"> create mode 100644 test&#x2F;.helmignore</span><br><span class="line"> create mode 100644 test&#x2F;Chart.yaml</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;NOTES.txt</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;_helpers.tpl</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;deployment.yaml</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;ingress.yaml</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;service.yaml</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;serviceaccount.yaml</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;tests&#x2F;test-connection.yaml</span><br><span class="line"> create mode 100644 test&#x2F;values.yaml</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ git push origin master</span><br><span class="line">Username for &#39;https:&#x2F;&#x2F;github.com&#39;: yushanjin</span><br><span class="line">Password for &#39;https:&#x2F;&#x2F;yushanjin@github.com&#39;: </span><br><span class="line">对象计数中: 17, 完成.</span><br><span class="line">Delta compression using up to 8 threads.</span><br><span class="line">压缩对象中: 100% (16&#x2F;16), 完成.</span><br><span class="line">写入对象中: 100% (17&#x2F;17), 8.32 KiB | 1.66 MiB&#x2F;s, 完成.</span><br><span class="line">Total 17 (delta 0), reused 0 (delta 0)</span><br><span class="line">To https:&#x2F;&#x2F;github.com&#x2F;yushanjin&#x2F;helm-chart.git</span><br><span class="line">   48e2023..5ae813c  master -&gt; master</span><br></pre></td></tr></table></figure>
<p>注意： 我这里本地没有创建其他分支，所以直接push到master分支了</p>
<h2 id="6、设置github上helm-chart仓库的GitHub-Pages（在仓库的settings里设置）"><a href="#6、设置github上helm-chart仓库的GitHub-Pages（在仓库的settings里设置）" class="headerlink" title="6、设置github上helm-chart仓库的GitHub Pages（在仓库的settings里设置）"></a>6、设置github上helm-chart仓库的GitHub Pages（在仓库的settings里设置）</h2><p><img src="https://upload-images.jianshu.io/upload_images/10839544-fece4f06795a48d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>登录：<a href="https://yushanjin.github.io/helm-chart%EF%BC%8C%E9%A1%B5%E9%9D%A2%E5%86%85%E5%AE%B9%E4%B8%BAREADME.md%E7%9A%84%E5%86%85%E5%AE%B9">https://yushanjin.github.io/helm-chart，页面内容为README.md的内容</a><br><img src="https://upload-images.jianshu.io/upload_images/10839544-9f84217a461cce46.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><strong>注意</strong>： 我修改过README.md文件</p>
<h2 id="7、本地添加自己的chart仓库"><a href="#7、本地添加自己的chart仓库" class="headerlink" title="7、本地添加自己的chart仓库"></a>7、本地添加自己的chart仓库</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm repo list</span><br><span class="line">Error: no repositories to show</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm repo add myrepo https:&#x2F;&#x2F;yushanjin.github.io&#x2F;helm-chart</span><br><span class="line">&quot;myrepo&quot; has been added to your repositories</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm repo list</span><br><span class="line">NAME  	URL                                   </span><br><span class="line">myrepo	https:&#x2F;&#x2F;yushanjin.github.io&#x2F;helm-chart</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm search repo test</span><br><span class="line">NAME       	CHART VERSION	APP VERSION	DESCRIPTION                </span><br><span class="line">myrepo&#x2F;test	0.1.0        	1.16.0     	A Helm chart for Kubernetes</span><br></pre></td></tr></table></figure>

<p>至此，就可以使用<strong>helm install xxx myrepo/test</strong> 安装test了</p>
]]></content>
      <categories>
        <category>helm</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>helm</tag>
      </tags>
  </entry>
  <entry>
    <title>如何配置kubernetes的pod从私有仓库拉取镜像</title>
    <url>/2020/12/01/%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AEkubernetes%E7%9A%84pod%E4%BB%8E%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F/</url>
    <content><![CDATA[<p>在实际使用中，用户往往搭建了自己的私有镜像仓库。kubernetes用户创建pod的过程中，如何从私有镜像仓库拉取容器镜像？</p>
<p>本文重点要介绍：如何让使用secret从私有的 Docker 镜像仓库或代码仓库拉取镜像来创建 Pod</p>
<h1 id="前提条件："><a href="#前提条件：" class="headerlink" title="前提条件："></a>前提条件：</h1><ol>
<li><p>kubernetes集群</p>
</li>
<li><p>docker私有镜像仓库或者docker hub上有Docker ID（这里使用Docker ID来演示）</p>
</li>
</ol>
<p>创建secret有两种方法：</p>
<ol>
<li><p>使用config.json文件</p>
</li>
<li><p>直接使用用户名+密码</p>
</li>
</ol>
<h1 id="生成config-json文件"><a href="#生成config-json文件" class="headerlink" title="生成config.json文件"></a>生成config.json文件</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# docker login</span><br><span class="line">Login with your Docker ID to push and pull images from Docker Hub. If you don&#39;t have a Docker ID, head over to https:&#x2F;&#x2F;hub.docker.com to create one.</span><br><span class="line">Username: shayu</span><br><span class="line">Password:</span><br><span class="line">WARNING! Your password will be stored unencrypted in &#x2F;root&#x2F;.docker&#x2F;config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https:&#x2F;&#x2F;docs.docker.com&#x2F;engine&#x2F;reference&#x2F;commandline&#x2F;login&#x2F;#credentials-store</span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br></pre></td></tr></table></figure>
<p>根据提示，输入用户名及密码。</p>
<p>用户名及密码会被存储到本地文件config.json中。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# cat &#x2F;root&#x2F;.docker&#x2F;config.json</span><br><span class="line">&#123;</span><br><span class="line">        &quot;auths&quot;: &#123;</span><br><span class="line">                &quot;https:&#x2F;&#x2F;index.docker.io&#x2F;v1&#x2F;&quot;: &#123;</span><br><span class="line">                        &quot;auth&quot;: &quot;c2hheXU6MDI4Mxxxxxxx4&#x3D;&quot;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;HttpHeaders&quot;: &#123;</span><br><span class="line">                &quot;User-Agent&quot;: &quot;Docker-Client&#x2F;19.03.13 (linux)&quot;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# echo &quot;&quot;c2hheXU6MDI4Mxxxxxxx4&#x3D;&quot; |base64 -d</span><br><span class="line">shayu:xxxxxxx</span><br></pre></td></tr></table></figure>

<h1 id="创建secret"><a href="#创建secret" class="headerlink" title="创建secret"></a>创建secret</h1><p>方法一：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kubectl create secret generic regcred --from-file&#x3D;.dockerconfigjson&#x3D;.docker&#x2F;config.json --type&#x3D;kubernetes.io&#x2F;dockerconfigjson</span><br><span class="line">secret&#x2F;regcred created</span><br><span class="line">root@ubuntu:~#</span><br><span class="line">root@ubuntu:~# kubectl get secret regcred -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOiB7CgkWFkZXJzIjogewoJCSJVc2VyLUFnZW50IjogIkRvY2tlci1DbGllbnQvMTkuMDMuMTMgKGxpbnV4KSIKCX0KfQ&#x3D;&#x3D;</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2020-12-01T07:09:19Z&quot;</span><br><span class="line">  managedFields:</span><br><span class="line">  - apiVersion: v1</span><br><span class="line">    fieldsType: FieldsV1</span><br><span class="line">    fieldsV1:</span><br><span class="line">      f:data:</span><br><span class="line">        .: &#123;&#125;</span><br><span class="line">        f:.dockerconfigjson: &#123;&#125;</span><br><span class="line">      f:type: &#123;&#125;</span><br><span class="line">    manager: kubectl-create</span><br><span class="line">    operation: Update</span><br><span class="line">    time: &quot;2020-12-01T07:09:19Z&quot;</span><br><span class="line">  name: regcred</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;974703&quot;</span><br><span class="line">  selfLink: &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;default&#x2F;secrets&#x2F;regcred</span><br><span class="line">  uid: 96ed63a0-90d8-47e3-a1d9-aa841a9d1813</span><br><span class="line">type: kubernetes.io&#x2F;dockerconfigjson</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get secret regcred --output&#x3D;&quot;jsonpath&#x3D;&#123;.data.\.dockerconfigjson&#125;&quot; | base64 --decode</span><br><span class="line">&#123;</span><br><span class="line">        &quot;auths&quot;: &#123;</span><br><span class="line">                &quot;https:&#x2F;&#x2F;index.docker.io&#x2F;v1&#x2F;&quot;: &#123;</span><br><span class="line">                        &quot;auth&quot;: &quot;c2hheXU6MDI4Mxxxxxxxxxx4&#x3D;&quot;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;HttpHeaders&quot;: &#123;</span><br><span class="line">                &quot;User-Agent&quot;: &quot;Docker-Client&#x2F;19.03.13 (linux)&quot;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>方法二：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kubectl create secret docker-registry regcred1 --docker-username&#x3D;shayu --docker-password&#x3D;xxxxxx --docker-email&#x3D;xxxxxxx@163.com</span><br><span class="line">secret&#x2F;regcred1 created</span><br><span class="line"></span><br><span class="line">其中:</span><br><span class="line">regcred1: 指定密钥的键名称, 可自行定义</span><br><span class="line">--docker-server: 指定docker仓库地址</span><br><span class="line">--docker-username: 指定docker仓库账号</span><br><span class="line">--docker-password: 指定docker仓库密码</span><br><span class="line">--docker-email: 指定邮件地址</span><br><span class="line"></span><br><span class="line">root@ubuntu:~#kubectl get secret regcred1 -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  .dockerconfigjson: eyJhdXRocyI6eyJodHRwczovL2luZGV4LmRvY2tlci5pbxxxxxxxxxxxxxFpbCI6Inl1c2hhbmppbjA3NjdAMTYzLmNvbSIsImF1dGgiOiJjMmhoZVhVNk1ESTRNVEV3ZVhOcUxDND0ifX19</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2020-12-01T07:26:30Z&quot;</span><br><span class="line">  managedFields:</span><br><span class="line">  - apiVersion: v1</span><br><span class="line">    fieldsType: FieldsV1</span><br><span class="line">    fieldsV1:</span><br><span class="line">      f:data:</span><br><span class="line">        .: &#123;&#125;</span><br><span class="line">        f:.dockerconfigjson: &#123;&#125;</span><br><span class="line">      f:type: &#123;&#125;</span><br><span class="line">    manager: kubectl-create</span><br><span class="line">    operation: Update</span><br><span class="line">    time: &quot;2020-12-01T07:26:30Z&quot;</span><br><span class="line">  name: regcred1</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;977436&quot;</span><br><span class="line">  selfLink: &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;default&#x2F;secrets&#x2F;regcred1</span><br><span class="line">  uid: b552c5ac-92eb-424e-bf4b-2a2c281cb75a</span><br><span class="line">type: kubernetes.io&#x2F;dockerconfigjson</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get secret regcred1 --output&#x3D;&quot;jsonpath&#x3D;&#123;.data.\.dockerconfigjson&#125;&quot; | base64 --decode</span><br><span class="line">&#123;&quot;auths&quot;:&#123;&quot;https:&#x2F;&#x2F;index.docker.io&#x2F;v1&#x2F;&quot;:&#123;&quot;username&quot;:&quot;shayu&quot;,&quot;password&quot;:&quot;xxxxxxx&quot;,&quot;email&quot;:&quot;xxxxxxxx@163.com&quot;,&quot;auth&quot;:&quot;c2hheXU6MDI4xxxxxxxxx4&#x3D;&quot;&#125;&#125;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="pod指定imagePullSecrets为上述创建的secret"><a href="#pod指定imagePullSecrets为上述创建的secret" class="headerlink" title="pod指定imagePullSecrets为上述创建的secret"></a>pod指定imagePullSecrets为上述创建的secret</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: private-reg</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: private-reg-container</span><br><span class="line">    image: xxxxxxx:yyyy</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: regcred</span><br></pre></td></tr></table></figure>

<p>参考：</p>
<p>(1) <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry">https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry</a></p>
<p>(2) <a href="https://kubernetes.io/docs/concepts/containers/images/">https://kubernetes.io/docs/concepts/containers/images/</a></p>
]]></content>
      <categories>
        <category>registry</category>
        <category>docker</category>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>kind快速部署Kubernetes环境</title>
    <url>/2020/11/26/kind%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2Kubernetes%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<h1 id="什么是kind"><a href="#什么是kind" class="headerlink" title="什么是kind"></a>什么是kind</h1><p>kind：Kubernetes In Docker，顾名思义，就是将kubernetes所需要的所有组件，全部部署在一个docker容器中，是一套开箱即用的kubernetes环境搭建方案。使用kind搭建的集群无法在生产中使用，但是如果你只是想在本地测试或者开发使用，不想占用太多的资源，那么使用kind是不错的选择。同样，kind还可以很方便的帮你本地的kubernetes源代码打成对应的镜像，方便测试。</p>
<p>GitHub: <a href="https://github.com/kubernetes-sigs/kind">https://github.com/kubernetes-sigs/kind</a></p>
<p>Documentation: <a href="https://kind.sigs.k8s.io/">https://kind.sigs.k8s.io/</a></p>
<h1 id="安装kind"><a href="#安装kind" class="headerlink" title="安装kind"></a>安装kind</h1><p>以Linux下安装为例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -Lo .&#x2F;kind https:&#x2F;&#x2F;kind.sigs.k8s.io&#x2F;dl&#x2F;v0.9.0&#x2F;kind-linux-amd64</span><br><span class="line">chmod +x .&#x2F;kind</span><br><span class="line">mv .&#x2F;kind &#x2F;usr&#x2F;local&#x2F;bin&#x2F;kind</span><br></pre></td></tr></table></figure>

<h1 id="创建、查询集群"><a href="#创建、查询集群" class="headerlink" title="创建、查询集群"></a>创建、查询集群</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kind create cluster</span><br></pre></td></tr></table></figure>
<p>该命令将默认创建名为kind的集群</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kind create cluster</span><br><span class="line">Creating cluster &quot;kind&quot; ...</span><br><span class="line"> ✓ Ensuring node image (kindest&#x2F;node:v1.19.1) 🖼</span><br><span class="line"> ✓ Preparing nodes 📦</span><br><span class="line"> ✓ Writing configuration 📜</span><br><span class="line"> ✓ Starting control-plane 🕹️</span><br><span class="line"> ✓ Installing CNI 🔌</span><br><span class="line"> ✓ Installing StorageClass 💾</span><br><span class="line">Set kubectl context to &quot;kind-kind&quot;</span><br><span class="line">You can now use your cluster with:</span><br><span class="line"></span><br><span class="line">kubectl cluster-info --context kind-kind</span><br><span class="line"></span><br><span class="line">Have a question, bug, or feature request? Let us know! https:&#x2F;&#x2F;kind.sigs.k8s.io&#x2F;#community 🙂</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kind get clusters</span><br><span class="line">kind</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kind create cluster --name test</span><br></pre></td></tr></table></figure>
<p>该命令将创建名为test的集群</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kind create cluster --name test</span><br><span class="line">Creating cluster &quot;test&quot; ...</span><br><span class="line"> ✓ Ensuring node image (kindest&#x2F;node:v1.19.1) 🖼</span><br><span class="line"> ✓ Preparing nodes 📦</span><br><span class="line"> ✓ Writing configuration 📜</span><br><span class="line"> ✓ Starting control-plane 🕹️</span><br><span class="line"> ✓ Installing CNI 🔌</span><br><span class="line"> ✓ Installing StorageClass 💾</span><br><span class="line">Set kubectl context to &quot;kind-test&quot;</span><br><span class="line">You can now use your cluster with:</span><br><span class="line"></span><br><span class="line">kubectl cluster-info --context kind-test</span><br><span class="line"></span><br><span class="line">Thanks for using kind! 😊</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kind get clusters</span><br><span class="line">kind</span><br><span class="line">test</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl cluster-info --context kind-test</span><br><span class="line">Kubernetes master is running at https:&#x2F;&#x2F;127.0.0.1:44543</span><br><span class="line">KubeDNS is running at https:&#x2F;&#x2F;127.0.0.1:44543&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;services&#x2F;kube-dns:dns&#x2F;proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.</span><br></pre></td></tr></table></figure>

<p>kubectl安装：<a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">https://kubernetes.io/docs/tasks/tools/install-kubectl/</a></p>
<p>查询集群节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kubectl get nodes --context kind-test</span><br><span class="line">NAME                 STATUS   ROLES    AGE     VERSION</span><br><span class="line">test-control-plane   Ready    master   4m55s   v1.19.1</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get nodes --context kind-kind</span><br><span class="line">NAME                 STATUS   ROLES    AGE     VERSION</span><br><span class="line">kind-control-plane   Ready    master   9m14s   v1.19.1</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# docker ps</span><br><span class="line">CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                       NAMES</span><br><span class="line">7fa7f8313453        kindest&#x2F;node:v1.19.1   &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;entr…&quot;   16 minutes ago      Up 16 minutes       127.0.0.1:44543-&gt;6443&#x2F;tcp   test-control-plane</span><br><span class="line">6fee527273e2        kindest&#x2F;node:v1.19.1   &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;entr…&quot;   20 minutes ago      Up 20 minutes       127.0.0.1:36585-&gt;6443&#x2F;tcp   kind-control-plane</span><br><span class="line">root@ubuntu:~# docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                       NAMES</span><br><span class="line">7fa7f8313453        kindest&#x2F;node:v1.19.1   &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;entr…&quot;   16 minutes ago      Up 16 minutes       127.0.0.1:44543-&gt;6443&#x2F;tcp   test-control-plane</span><br><span class="line">6fee527273e2        kindest&#x2F;node:v1.19.1   &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;entr…&quot;   20 minutes ago      Up 20 minutes       127.0.0.1:36585-&gt;6443&#x2F;tcp   kind-control-plane</span><br></pre></td></tr></table></figure>
<p>test-control-plane、kind-control-plane是运行在容器内的kubernetes节点，也正是kubernetes in Docker的意思</p>
<p>查询集群内运行的pod</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kubectl get pods --all-namespaces --context kind-kind</span><br><span class="line">NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system          coredns-f9fd979d6-cqpss                      1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          coredns-f9fd979d6-kvbzm                      1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          etcd-kind-control-plane                      1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          kindnet-br2qc                                1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          kube-apiserver-kind-control-plane            1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          kube-controller-manager-kind-control-plane   1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          kube-proxy-4chlm                             1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          kube-scheduler-kind-control-plane            1&#x2F;1     Running   0          22m</span><br><span class="line">local-path-storage   local-path-provisioner-78776bfc44-xl4ht      1&#x2F;1     Running   0          22m</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get pods --all-namespaces --context kind-test</span><br><span class="line">NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system          coredns-f9fd979d6-f7wsz                      1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          coredns-f9fd979d6-j9xtx                      1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          etcd-test-control-plane                      1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          kindnet-jq867                                1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          kube-apiserver-test-control-plane            1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          kube-controller-manager-test-control-plane   1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          kube-proxy-swn5j                             1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          kube-scheduler-test-control-plane            1&#x2F;1     Running   0          18m</span><br><span class="line">local-path-storage   local-path-provisioner-78776bfc44-nwqjq      1&#x2F;1     Running   0          18m</span><br></pre></td></tr></table></figure>

<p>查询pod使用的镜像</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# docker exec -it kind-control-plane crictl images</span><br><span class="line">IMAGE                                      TAG                  IMAGE ID            SIZE</span><br><span class="line">docker.io&#x2F;kindest&#x2F;kindnetd                 v20200725-4d6bea59   b77790820d015       119MB</span><br><span class="line">docker.io&#x2F;rancher&#x2F;local-path-provisioner   v0.0.14              e422121c9c5f9       42MB</span><br><span class="line">k8s.gcr.io&#x2F;build-image&#x2F;debian-base         v2.1.0               c7c6c86897b63       53.9MB</span><br><span class="line">k8s.gcr.io&#x2F;coredns                         1.7.0                bfe3a36ebd252       45.4MB</span><br><span class="line">k8s.gcr.io&#x2F;etcd                            3.4.13-0             0369cf4303ffd       255MB</span><br><span class="line">k8s.gcr.io&#x2F;kube-apiserver                  v1.19.1              8cba89a89aaa8       95MB</span><br><span class="line">k8s.gcr.io&#x2F;kube-controller-manager         v1.19.1              7dafbafe72c90       84.1MB</span><br><span class="line">k8s.gcr.io&#x2F;kube-proxy                      v1.19.1              47e289e332426       136MB</span><br><span class="line">k8s.gcr.io&#x2F;kube-scheduler                  v1.19.1              4d648fc900179       65.1MB</span><br><span class="line">k8s.gcr.io&#x2F;pause                           3.3                  0184c1613d929       686kB</span><br><span class="line">root@ubuntu:~# docker exec -it kind-control-plane crictl ps</span><br><span class="line">CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID</span><br><span class="line">1c881f7f0d306       bfe3a36ebd252       25 minutes ago      Running             coredns                   0                   75e161b37ab2d</span><br><span class="line">c64dd6bb666f3       bfe3a36ebd252       25 minutes ago      Running             coredns                   0                   a8305dc572af5</span><br><span class="line">fea2b69f5472d       e422121c9c5f9       26 minutes ago      Running             local-path-provisioner    0                   fb704b6340b63</span><br><span class="line">52f0995ba00f8       47e289e332426       26 minutes ago      Running             kube-proxy                0                   4c787e616d5a7</span><br><span class="line">b87cdcc514f59       b77790820d015       26 minutes ago      Running             kindnet-cni               0                   14eb70f9ca549</span><br><span class="line">ce2c4e5b2b57f       0369cf4303ffd       27 minutes ago      Running             etcd                      0                   744a99a558714</span><br><span class="line">93b5084a29992       8cba89a89aaa8       27 minutes ago      Running             kube-apiserver            0                   91f88afc5a39b</span><br><span class="line">8b9579313058f       7dafbafe72c90       27 minutes ago      Running             kube-controller-manager   0                   8d54fdffef86e</span><br><span class="line">10fbb8244ad3b       4d648fc900179       27 minutes ago      Running             kube-scheduler            0                   a985ae4a105bc</span><br></pre></td></tr></table></figure>
<p>其中，crictl命令可以理解为docker命令</p>
<h1 id="删除集群"><a href="#删除集群" class="headerlink" title="删除集群"></a>删除集群</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kind delete cluster --name test</span><br></pre></td></tr></table></figure>
<p>删除名为test的集群，–name未指定的话，将默认删除kind集群</p>
<h1 id="创建高可用kubernetes集群"><a href="#创建高可用kubernetes集群" class="headerlink" title="创建高可用kubernetes集群"></a>创建高可用kubernetes集群</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# cat kind-config.yaml</span><br><span class="line">kind: Cluster</span><br><span class="line">apiVersion: kind.x-k8s.io&#x2F;v1alpha4</span><br><span class="line">nodes:</span><br><span class="line">- role: control-plane</span><br><span class="line">- role: control-plane</span><br><span class="line">- role: control-plane</span><br><span class="line">- role: worker</span><br><span class="line">- role: worker</span><br><span class="line">- role: worker</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kind create cluster --config kind-config.yaml --name test2</span><br><span class="line">Creating cluster &quot;test2&quot; ...</span><br><span class="line"> ✓ Ensuring node image (kindest&#x2F;node:v1.19.1) 🖼</span><br><span class="line"> ✓ Preparing nodes 📦 📦 📦 📦 📦 📦</span><br><span class="line"> ✓ Configuring the external load balancer ⚖️</span><br><span class="line"> ✓ Writing configuration 📜</span><br><span class="line"> ✓ Starting control-plane 🕹️</span><br><span class="line"> ✓ Installing CNI 🔌</span><br><span class="line"> ✓ Installing StorageClass 💾</span><br><span class="line"> ✓ Joining more control-plane nodes 🎮</span><br><span class="line"> ✓ Joining worker nodes 🚜</span><br><span class="line">Set kubectl context to &quot;kind-test2&quot;</span><br><span class="line">You can now use your cluster with:</span><br><span class="line"></span><br><span class="line">kubectl cluster-info --context kind-test2</span><br><span class="line"></span><br><span class="line">Have a nice day! 👋</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get nodes --context kind-test2</span><br><span class="line">NAME                   STATUS   ROLES    AGE     VERSION</span><br><span class="line">test2-control-plane    Ready    master   5m12s   v1.19.1</span><br><span class="line">test2-control-plane2   Ready    master   4m38s   v1.19.1</span><br><span class="line">test2-control-plane3   Ready    master   3m22s   v1.19.1</span><br><span class="line">test2-worker           Ready    &lt;none&gt;   2m5s    v1.19.1</span><br><span class="line">test2-worker2          Ready    &lt;none&gt;   2m5s    v1.19.1</span><br><span class="line">test2-worker3          Ready    &lt;none&gt;   2m5s    v1.19.1</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get pods --all-namespaces --context kind-test2</span><br><span class="line">NAMESPACE            NAME                                           READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system          coredns-f9fd979d6-jbpcz                        1&#x2F;1     Running   0          5m16s</span><br><span class="line">kube-system          coredns-f9fd979d6-ng4qp                        1&#x2F;1     Running   0          5m16s</span><br><span class="line">kube-system          etcd-test2-control-plane                       1&#x2F;1     Running   0          5m15s</span><br><span class="line">kube-system          etcd-test2-control-plane2                      1&#x2F;1     Running   0          4m54s</span><br><span class="line">kube-system          etcd-test2-control-plane3                      1&#x2F;1     Running   0          2m59s</span><br><span class="line">kube-system          kindnet-gxpjn                                  1&#x2F;1     Running   0          4m55s</span><br><span class="line">kube-system          kindnet-jqnx5                                  1&#x2F;1     Running   0          2m20s</span><br><span class="line">kube-system          kindnet-lczmx                                  1&#x2F;1     Running   0          2m18s</span><br><span class="line">kube-system          kindnet-q8bcn                                  1&#x2F;1     Running   0          2m19s</span><br><span class="line">kube-system          kindnet-q9ng2                                  1&#x2F;1     Running   0          3m37s</span><br><span class="line">kube-system          kindnet-s7kfb                                  1&#x2F;1     Running   0          5m14s</span><br><span class="line">kube-system          kube-apiserver-test2-control-plane             1&#x2F;1     Running   0          5m15s</span><br><span class="line">kube-system          kube-apiserver-test2-control-plane2            1&#x2F;1     Running   0          4m54s</span><br><span class="line">kube-system          kube-apiserver-test2-control-plane3            1&#x2F;1     Running   1          3m9s</span><br><span class="line">kube-system          kube-controller-manager-test2-control-plane    1&#x2F;1     Running   2          5m14s</span><br><span class="line">kube-system          kube-controller-manager-test2-control-plane2   1&#x2F;1     Running   0          4m54s</span><br><span class="line">kube-system          kube-controller-manager-test2-control-plane3   1&#x2F;1     Running   0          2m8s</span><br><span class="line">kube-system          kube-proxy-47nc7                               1&#x2F;1     Running   0          5m16s</span><br><span class="line">kube-system          kube-proxy-5799m                               1&#x2F;1     Running   0          4m55s</span><br><span class="line">kube-system          kube-proxy-cvm49                               1&#x2F;1     Running   0          2m18s</span><br><span class="line">kube-system          kube-proxy-s7rsp                               1&#x2F;1     Running   0          2m18s</span><br><span class="line">kube-system          kube-proxy-sxwgl                               1&#x2F;1     Running   0          3m37s</span><br><span class="line">kube-system          kube-proxy-wvskh                               1&#x2F;1     Running   0          2m20s</span><br><span class="line">kube-system          kube-scheduler-test2-control-plane             0&#x2F;1     Running   2          5m15s</span><br><span class="line">kube-system          kube-scheduler-test2-control-plane2            1&#x2F;1     Running   0          4m54s</span><br><span class="line">kube-system          kube-scheduler-test2-control-plane3            1&#x2F;1     Running   0          2m31s</span><br><span class="line">local-path-storage   local-path-provisioner-78776bfc44-fkdwq        1&#x2F;1     Running   1          5m12s</span><br></pre></td></tr></table></figure>

<p>上述过程，我们创建了两个kubernetes集群，kind和test2；可以看到我们在使用kubectl访问集群时，增加了参数：–context，这在本地的配置文件里指定了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# cat .kube&#x2F;config</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01URXlOekF3TkRJeU9Wb1hEVE13TVRFeU5UQXdOREl5T1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTERTCm5TU1NHS05PUUlCVWpQeXpYOFRDZm5IamExV2JxeXZJeHV3Y1Aybmh6Zi9EdG9wVDRnSTVjMlBMV3Qrd05BUFUKUHA2dGV3ZkhNQ3N6MnJLbnhaRmtWS2c5NXVFdW53V1ZuZnlmeGV0TjlOU1JTZ2s2dkJuVUt6SFliRXIybEY0LwpicFVxT2IzWnkxQXdYNlpyRTN3Y1I1RjdLV2trT0FZbHdobUtiLzIwOVZJRG4yMW9CMHMzNXgrM3Z2L2gzQ3VaCkJCQjJnNVBKMm4xc1pwd05scnZDMmh0RmJDSjQwQVNXZmNsUksyejBYUEIvdzdNQlBXMHp1cnpEMUVBcGdZVUcKUGUwOEx3dW1zMllZR3Y5TDJXMERhWW90c2JoVXlWZWRDNnpjeWtUZ0hOb1B6LytvZlBHQmxYTko3S2lwd2Z6ZQo2dzdHZGJhbjlpRms4cjAyaVQ4Q0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZEY2liT2RWZTN4U0w0RmMxOWVJdmZnSTBHcjZNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFCS1lKSVFaenhSblFWWWp4TnlTaXJualFvZUM3OGtDMERNYjZpRFBvMFdEbmdTUjhRYgpqRjZxR2ZPMTFTcDNjeVl3V1IrM3UzU2pzQUJUUG5jcWZpWnEyN3VDMlRYNjdMQzRPNVFoVkdlRlBDdXJNNHluCi8weDZuOUZrMko5YmpTT0o1aDB2NmQ1eXBibk5sNVgvN1czWFVyK2tjOSswWG1sMFN0dmJVZ0hCaWVmWGxtZ3AKUXl5TlFtNU1yWlRFcWJOT0JubW1RWUJYWDdydWVLZXhVYUJ4QXQrRVJVWHBVOWNhbkNWWWhuZFBuMVNBYVladApsQkZyamRSNzg1SE1EV21qTW5UalJXSUhOSWd2NUgwKyt2MmN5cjRSK1lUWGo3S0JrdEZTRDA2U0I0RDZVeFk2CmRUZ25UMngzRXJvSERPZURjWWRrVmt2TkF5aC9JUGFTd2R6cwotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg&#x3D;&#x3D;</span><br><span class="line">    server: https:&#x2F;&#x2F;127.0.0.1:36585</span><br><span class="line">  name: kind-kind</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01URXlOekF6TWpJeE9Gb1hEVE13TVRFeU5UQXpNakl4T0Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSi85CjlZQ0VseTNRLzRuc2t5dGZZY2Zpby9jcDJoaSsxRWpPOGMrVmF4YTkrbTIrdldBT1VhU0xQZnQ0VmRSL1hIWjYKT3FQSEY5K3ozNEZIb1p4RTJIY3kxd3R0Y1hOTXk1Qy9VMUdnakgramZLaWNmbVJmS3luS1M4SU80T3pLRHBKUQpodHQzb0JxeDVHdU0wSUpkVmsydVg5RjhmVHEyTllaN2lzK0NOVWdXM2dxMXA4SzNkZWxjaDYyM2NBSHhSS0JLCmZEZC9iZnZiZitvR0ZQS29BNWhLcXVKb3BDVFcrN1VZdnA4ZCs0QTgwYkZ2cG5CSmlUK1pGU0sxRExKZHBuRlgKV0pDOHdwNzNBYU9KSHAydnNTZ0p6ajFvUHUvL1lnSWNPNStoSGl6bThVcnVGQWxkWmE2ZzN4VnFCazNDZnducApsaU50MzdiblJrZ3VUNUU5cFFNQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZNdnpFOXJORVFUWGNyUmJHcllmQ0J6VEpZby9NQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFBMjJZS1htS29kTkVmUklGS1ZOMXk1alRjQ2FIVnAyd3QvR1RNT3dHU1pYQXhlN2xBOQpIZDVCWHJoaXB6bkNPR1hxakRoZkVkaWVIa3VlcDNQWGxVTWxxa20yUGZkdUp0cTJ3T0piR0ZTVWZRR2xpS09MCkJ0eDRwVmd1dC9EQW11ZHNNNFlSeGRTS1R3bkppcjBBbkNnSWprd0gvZzdzekFYTVppMmZkYk9oL3NzcVFlZU4KREYrc01ld0czK2pSaVVqTEM2ck9sb0UySzArVjNhMDlMbmNiYlRCNDBVM0pZR0VvallzVExEdXZPakhuZElkKwpyM0FHMlNXMDRYek8wcHF4VDhlRjlkUE5qVG83SlpIeDc3RUtuNVNYU1YweGRuTTR1eHVBWDBoVExLenFyMlRNCmVJTzlZNGZWeHdCQk9BaFBQVmFTMW5SZy9GMXFIWWNKK20wUgotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg&#x3D;&#x3D;</span><br><span class="line">    server: https:&#x2F;&#x2F;127.0.0.1:34927</span><br><span class="line">  name: kind-test2</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: kind-kind</span><br><span class="line">    user: kind-kind</span><br><span class="line">  name: kind-kind</span><br><span class="line">- context:</span><br><span class="line">    cluster: kind-test2</span><br><span class="line">    user: kind-test2</span><br><span class="line">  name: kind-test2</span><br><span class="line">current-context: kind-test2</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: kind-kind</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURFekNDQWZ1Z0F3SUJBZ0lJTHRQZkozeVliVHN3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TURFeE1qY3dNRFF5TWpsYUZ3MHlNVEV4TWpjd01EUXlNekZhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXJEaURMOWhSUGduVzNwdlcKQ1pNekVlTG0vRTJuVmlXdG9icTVoZUdrdG1Udi9aWEUydG9uKy96ZWROOThvM3J4ZEdneDNTZDRndjZtSnpQbgpFZ3hTRDcrQnErSmt5WG5oQThPM1RQakN0aHdES1JIZjEzbkI4NDNlbjZ3S3E5K3A1RHB2UFNNSkgyQkJLckkrCjlHWTFvTWp2SnRWSjMyZ1dhTnlJV0hzTkE0QmdPM0RRT2Y2cGN6bXZhbllKZFliYTFpQmpKUEVaRURPNEpFdHIKZWYrTUFHb1diSFlaeDdySWo5eTJoNzdOQi9DV0k1alVSUXp6MWQ0ZlFnV05WR1cyblVXbGlGZjlEWlRVR25CMwpHMHBUTjc5N1U0bUNYdWVBUjFxY25idS9YNjBMb2l6bEpKVWJzcjZYUWFaRmtKS2FzSGx2Tmdydm11cWhnSjZTCmxCWWNIUUlEQVFBQm8wZ3dSakFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0h3WURWUjBqQkJnd0ZvQVVOeUpzNTFWN2ZGSXZnVnpYMTRpOStBalFhdm93RFFZSktvWklodmNOQVFFTApCUUFEZ2dFQkFBay8xbjJqLzhGdlhISm94V0Q3dUl4WUxTUmlrTjcxWWdOOW1yckVYRjVFUkxBMjI5eEtyT1c1ClV2Tm9nZGlqdWpEbDhoVjRoU1hkY3M0dE9ZNmZYbExkN3JhN25vL054UUtHM1lPdktWM2g0eS9wUEYxMENQL0sKTTFhSnNkY1U2aG8wbnZrL1dQSDB2ckxtNE1jUHpBbFUvazdvd3FiemcybHZQdDBCMTJjZHh4bk44UHp5VU4zZwpQQVBzT3hab1hwQnVpNjkxcEt2a1VDUm1MY0dUTHowT0Y5YXVOaUhRRG1qMG1hSEg2ckI2c0kvQzBuZ08vaCtSCmNsUnBGYk9uQnZRbW1nRjZER0E4cGxUSUM2ZE1JOGtXRzVaQWFSbzMrblFLME5sMERaUDVBTjM1cE1yS0M1R3cKeTdHaVlRQkFCVlJOZ2FJemg1bktHUXFJVVFXRUU5TT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo&#x3D;</span><br><span class="line">    client-key-data: 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&#x3D;&#x3D;</span><br><span class="line">- name: kind-test2</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURFekNDQWZ1Z0F3SUJBZ0lJR2toZno1Qy9Yc2d3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TURFeE1qY3dNekl5TVRoYUZ3MHlNVEV4TWpjd016SXlNakJhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXJ6azZ2RFJsMjI4V1RndUEKYnNNN0tLVUVaaWRmbVBKVW1kaitmdCs1UTZFSHFvUitzOHVycDVVc0tOY0krZzdoYkNJa2VzdzE4OGZ0NDVlSwp4eGpmV1NiUXNiSjYyb3RQRUVoSGpPZElRTHVxQlBqR1J1N3NwUE8yRzFnMFZTMm9UY0VUVlYzQ2RCUFFPRjNOCm9WblhPdzQwSWttWXVURDZHUEdNT2VwN05Rak4vZERybVphMy94WWxJK2loVHZJbnZXNjVYYmd0NTNwS203YXgKOUNSVGhjNUtONjg2TzVLekR6ZnhPZGtpUUhJdzNXL3BpTXRvOVV5QUF3V2RoUmc0K0VacW5OTjZXREl4RlNPNQpJN0Z6c3dtT2s0L29wcW55ZHhmNnFYamF6MGlvTTNyM0I4bUY0c09VakhtaDJyUnB3a3ZpQUxkV3AvbVo0ek5uClJYYVp3UUlEQVFBQm8wZ3dSakFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0h3WURWUjBqQkJnd0ZvQVV5L01UMnMwUkJOZHl0RnNhdGg4SUhOTWxpajh3RFFZSktvWklodmNOQVFFTApCUUFEZ2dFQkFIMHg2am5uajNSQWxBY0dOZmxyY1Zob01XVUpOZCtVak5OSXNSTHRaSzJTYTh6SThCK0VIeUxFCndhRjJGMzZ0VTV6L1JiSFluK3d3Q3ByQVJjLzhYTUx2OEpnMVU2aDg2SkxpNm9qc29Vb0dhUFNoWmI3S1YyOUQKSE5JQ2p5M2QrenhjdEV4OWFTRDB3WkR0cFpVaS9tZlQ3MklRekVUazQ2QTQ4bldYdkx0MWJ6TEZ4T1hQZVB1WgpiZG1lcWR5QmN1TXI0MUppcEJVUlpOWDgxQ0xIQ1ZQU2tsRHkySVl0OUgxeVNmb3RHMUV4ZHlDZE1vRjZTVUdhCkpLY0psNGFSQ3JNLzNqSGx3YVpGcWR0WkFFZ2pzM3lOYXllTW9SYi93bEdpWkdaQTFhQ3JNZlU5bXRnTlMrMWIKbmE1M0RxVmRKZHBTSnhWSjNPQXpUY2s0OVV1Z2c4ND0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo&#x3D;</span><br><span class="line">    client-key-data: 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&#x3D;&#x3D;</span><br></pre></td></tr></table></figure>
<p>可以看到两个cluster，两个context。context指定了cluster及user，users字段列出了两个user</p>
<p>context名为kind-kind，cluser为kind-kind，user为kind-kind，意思就是当指定–context为kind-kind时，将使用kind-kind的user去访问cluster为kind-kind的集群。</p>
<p>current-context: kind-test2说明当前的context使用的是kind-test2，如果不指定–context，将默认使用kind-test2</p>
<p>当然，用户可以修改当然默认的current-context</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kubectl get node</span><br><span class="line">NAME                   STATUS   ROLES    AGE     VERSION</span><br><span class="line">test2-control-plane    Ready    master   2d23h   v1.19.1</span><br><span class="line">test2-control-plane2   Ready    master   2d23h   v1.19.1</span><br><span class="line">test2-control-plane3   Ready    master   2d23h   v1.19.1</span><br><span class="line">test2-worker           Ready    &lt;none&gt;   2d23h   v1.19.1</span><br><span class="line">test2-worker2          Ready    &lt;none&gt;   2d23h   v1.19.1</span><br><span class="line">test2-worker3          Ready    &lt;none&gt;   2d23h   v1.19.1</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl config</span><br><span class="line">Modify kubeconfig files using subcommands like &quot;kubectl config set current-context my-context&quot;</span><br><span class="line"></span><br><span class="line"> The loading order follows these rules:</span><br><span class="line"></span><br><span class="line">  1.  If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes</span><br><span class="line">place.</span><br><span class="line">  2.  If $KUBECONFIG environment variable is set, then it is used as a list of paths (normal path delimiting rules for</span><br><span class="line">your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When</span><br><span class="line">a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the</span><br><span class="line">last file in the list.</span><br><span class="line">  3.  Otherwise, $&#123;HOME&#125;&#x2F;.kube&#x2F;config is used and no merging takes place.</span><br><span class="line"></span><br><span class="line">Available Commands:</span><br><span class="line">  current-context Displays the current-context</span><br><span class="line">  delete-cluster  Delete the specified cluster from the kubeconfig</span><br><span class="line">  delete-context  Delete the specified context from the kubeconfig</span><br><span class="line">  get-clusters    Display clusters defined in the kubeconfig</span><br><span class="line">  get-contexts    Describe one or many contexts</span><br><span class="line">  rename-context  Renames a context from the kubeconfig file.</span><br><span class="line">  set             Sets an individual value in a kubeconfig file</span><br><span class="line">  set-cluster     Sets a cluster entry in kubeconfig</span><br><span class="line">  set-context     Sets a context entry in kubeconfig</span><br><span class="line">  set-credentials Sets a user entry in kubeconfig</span><br><span class="line">  unset           Unsets an individual value in a kubeconfig file</span><br><span class="line">  use-context     Sets the current-context in a kubeconfig file</span><br><span class="line">  view            Display merged kubeconfig settings or a specified kubeconfig file</span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">  kubectl config SUBCOMMAND [options]</span><br><span class="line"></span><br><span class="line">Use &quot;kubectl &lt;command&gt; --help&quot; for more information about a given command.</span><br><span class="line">Use &quot;kubectl options&quot; for a list of global command-line options (applies to all commands).</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl config use-context</span><br><span class="line">Sets the current-context in a kubeconfig file</span><br><span class="line"></span><br><span class="line">Aliases:</span><br><span class="line">use-context, use</span><br><span class="line"></span><br><span class="line">Examples:</span><br><span class="line">  # Use the context for the minikube cluster</span><br><span class="line">  kubectl config use-context minikube</span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">  kubectl config use-context CONTEXT_NAME [options]</span><br><span class="line"></span><br><span class="line">Use &quot;kubectl options&quot; for a list of global command-line options (applies to all commands).</span><br><span class="line">error: Unexpected args: []</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl config use-context kind-kind</span><br><span class="line">Switched to context &quot;kind-kind&quot;.</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get node</span><br><span class="line">NAME                 STATUS   ROLES    AGE    VERSION</span><br><span class="line">kind-control-plane   Ready    master   3d1h   v1.19.1</span><br></pre></td></tr></table></figure>
<p>实际上，kind内部创建集群过程中，也是使用kubeadm，从上述创建过程中的打印也似乎能猜到这一点</p>
<p>后面，就可以使用搭建好的集群环境，开始你的开发、测试工作了~~~</p>
]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>云原生</tag>
      </tags>
  </entry>
</search>

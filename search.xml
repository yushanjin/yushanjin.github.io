<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>EMQ X Kuiper与EdgeX Foundry集成实践</title>
    <url>/2020/12/01/EMQ-X-Kuiper%E4%B8%8EEdgeX-Foundry%E9%9B%86%E6%88%90%E5%AE%9E%E8%B7%B5/</url>
    <content><![CDATA[<p>Kuiper是什么? EdgeX Foundry又是什么？</p>
<h1 id="Kuiper"><a href="#Kuiper" class="headerlink" title="Kuiper"></a>Kuiper</h1><p>EMQ X Kuiper 是 Golang 实现的轻量级物联网边缘分析、流式处理开源软件，可以运行在各类<strong>资源受限的边缘设备</strong>上。Kuiper 设计的一个主要目标就是将在云端运行的实时流式计算框架（比如 <a href="https://spark.apache.org/">Apache Spark</a>，<a href="https://storm.apache.org/">Apache Storm</a> 和 <a href="https://flink.apache.org/">Apache Flink</a> 等）迁移到边缘端。Kuiper 参考了上述云端流式处理项目的架构与实现，结合边缘流式数据处理的特点，采用了编写<strong>基于<code>源 (Source)</code>，<code>SQL (业务逻辑处理)</code>, <code>目标 (Sink)</code> 的规则引擎来实现边缘端的流式数据处理</strong>。<br>其架构如下：<br><img src="https://upload-images.jianshu.io/upload_images/10839544-82b09a3d6c9e5c33.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<ul>
<li>源 (Sources) ：内置支持 MQTT 数据的接入，扩展支持与EdgeX Foundry集成</li>
<li>SQL：流式数据逻辑处理，具备完整的数据分析处理能力<ul>
<li>支持丰富的数据类型</li>
<li>支持4种时间窗口（滚动窗口、跳跃窗口、滑动窗口、会话窗口）</li>
<li>内置60+处理函数</li>
<li>提供类SQL语句对数据进行抽取、过滤、转换</li>
</ul>
</li>
<li>目标(Sinks)：内置支持 MQTT、HTTP等</li>
</ul>
<p>EMQ公司的相关产品，可以登陆其官网查询了解<br><img src="https://upload-images.jianshu.io/upload_images/10839544-b1643f3d02bea968.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h1 id="EdgeX-Foundry"><a href="#EdgeX-Foundry" class="headerlink" title="EdgeX Foundry"></a>EdgeX Foundry</h1><p>EdgeX Foundry是一个Linux 基金会运营的开源的，基于与硬件和操作系统完全无关的边缘计算物联网软件框架项目。其是一系列松耦合、开源的微服务集合，位于网络的边缘，可以与设备、传感器、执行器和其他物联网对象的物理世界进行交互。EdgeX Foundry 旨在创造一个互操作性、即插即用、模块化的物联网边缘计算的生态系统。<br>其架构如下：<br><img src="https://upload-images.jianshu.io/upload_images/10839544-fd06dab15f5598af.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>从架构图可以看出：<br><strong>南侧（SouthBound）</strong>:在物理领域内的所有物联网对象，以及与这些设备、传感器、执行器和其他物联网对象直接通信并从中收集数据的网络边缘，统称为“南侧”。<br><strong>北侧（NorthBound）</strong>:将数据收集、存储、聚合、分析并转换为信息的云(或企业系统)，以及与云通信的网络部分称为网络的“北侧”。<br>因此，EdgeX使数据可以向北移动到云，也可以横向移动到其他网关，或返回到设备、传感器和执行器。<br>EdgeX的重要服务层及微服务：<br><img src="https://upload-images.jianshu.io/upload_images/10839544-d0e4c5a219ab700d.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="1、安装edgex"><a href="#1、安装edgex" class="headerlink" title="1、安装edgex"></a>1、安装edgex</h2><p>参照官网文档：<a href="https://fuji-docs.edgexfoundry.org/Ch-GettingStartedUsers.html">https://fuji-docs.edgexfoundry.org/Ch-GettingStartedUsers.html</a><br>docker-compose启动<br><img src="https://upload-images.jianshu.io/upload_images/10839544-f5a8d9ed341b66b4.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>相关服务正常<br><img src="https://upload-images.jianshu.io/upload_images/10839544-8ab54653eca76cc1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="2、安装并启动kuiper"><a href="#2、安装并启动kuiper" class="headerlink" title="2、安装并启动kuiper"></a>2、安装并启动kuiper</h2><p>sudo docker run -d –name kuiper –restart always -e EDGEX_SERVER=10.0.105.143 -e EDGEX_PORT=5563 -e EDGEX_SERVICE_SERVER=<a href="http://10.0.105.143:48080/">http://10.0.105.143:48080</a> emqx/kuiper:0.2.1</p>
<p>环境变量具体参考：<a href="https://hub.docker.com/r/emqx/kuiper">https://hub.docker.com/r/emqx/kuiper</a> 中的说明<br>EDGEX_SERVER：edgex中zeromq的地址（zeromq集成到core data服务中了，可以看到core data服务暴露了两个端口一个5563，一个48080）<br>EDGEX_PORT：edgex中zeromq的端口<br>EDGEX_SERVICE_SERVER：edgex中core data的地址及端口<br>这里我使用的kuiper镜像为 emqx/kuiper:0.2.1，为目前最新版本</p>
<h2 id="3、进入kuiper容器"><a href="#3、进入kuiper容器" class="headerlink" title="3、进入kuiper容器"></a>3、进入kuiper容器</h2><p>sudo docker exec -it kuiper /bin/sh</p>
<h2 id="4、查看日志"><a href="#4、查看日志" class="headerlink" title="4、查看日志"></a>4、查看日志</h2><p>/kuiper # cat log/stream.log</p>
<h2 id="5、创建流，订阅来自edgex的消息流"><a href="#5、创建流，订阅来自edgex的消息流" class="headerlink" title="5、创建流，订阅来自edgex的消息流"></a>5、创建流，订阅来自edgex的消息流</h2><p>/kuiper # bin/cli create stream demo’() WITH (FORMAT=”JSON”, TYPE=”edgex”)’</p>
<h2 id="6、创建规则文件，内容如下"><a href="#6、创建规则文件，内容如下" class="headerlink" title="6、创建规则文件，内容如下"></a>6、创建规则文件，内容如下</h2><p>/kuiper # cat rule.txt</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;sql&quot;: &quot;SELECT * from demo GROUP BY TUMBLINGWINDOW(ss, 10)&quot;,</span><br><span class="line">  &quot;actions&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;mqtt&quot;: &#123;</span><br><span class="line">        &quot;server&quot;: &quot;tcp:&#x2F;&#x2F;broker.emqx.io:1883&quot;,</span><br><span class="line">        &quot;topic&quot;: &quot;result&quot;,</span><br><span class="line">        &quot;clientId&quot;: &quot;demo_001&quot;</span><br><span class="line">      &#125;</span><br><span class="line">&#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>上述规则：Kuiper将接受edgex的数据，执行select操作（每10s钟），然后将处理后的数据发布到tcp://broker.emqx.io:1883（也可以换成其他的，比如broker.hivemq.com或者自己搭建的EMQ X edge）<br><strong>注意：</strong>Kuiper SQL相关的参考，见<a href="https://docs.emqx.io/kuiper/latest/cn/sqls/overview.html">https://docs.emqx.io/kuiper/latest/cn/sqls/overview.html</a></p>
<h2 id="7、创建规则，命名为rule1"><a href="#7、创建规则，命名为rule1" class="headerlink" title="7、创建规则，命名为rule1"></a>7、创建规则，命名为rule1</h2><p>/kuiper # bin/cli create rule rule1 -f rule.txt</p>
<h2 id="8、查看日志，可以看到已经连通edgex，相关的规则也已经创建"><a href="#8、查看日志，可以看到已经连通edgex，相关的规则也已经创建" class="headerlink" title="8、查看日志，可以看到已经连通edgex，相关的规则也已经创建"></a>8、查看日志，可以看到已经连通edgex，相关的规则也已经创建</h2><p><img src="https://upload-images.jianshu.io/upload_images/10839544-4f07b16e3735e2dc.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="9、查看规则状态"><a href="#9、查看规则状态" class="headerlink" title="9、查看规则状态"></a>9、查看规则状态</h2><p><img src="https://upload-images.jianshu.io/upload_images/10839544-1349f6fd6210e627.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<h2 id="10、使用mosquitto订阅broker-emqx-io中主题为result的消息"><a href="#10、使用mosquitto订阅broker-emqx-io中主题为result的消息" class="headerlink" title="10、使用mosquitto订阅broker.emqx.io中主题为result的消息"></a>10、使用mosquitto订阅broker.emqx.io中主题为result的消息</h2><p><img src="https://upload-images.jianshu.io/upload_images/10839544-4fdbca2eb0e7af64.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p>分析结果，发布到文件（<a href="https://github.com/emqx/kuiper/blob/master/docs/zh_CN/plugins/sinks/file.md">https://github.com/emqx/kuiper/blob/master/docs/zh_CN/plugins/sinks/file.md</a>）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;sql&quot;: &quot;SELECT * from demo&quot;,</span><br><span class="line">  &quot;actions&quot;: [</span><br><span class="line">   &#123;</span><br><span class="line">      &quot;file&quot;: &#123;</span><br><span class="line">        &quot;path&quot;: &quot;&#x2F;tmp&#x2F;result.txt&quot;,</span><br><span class="line">        &quot;interval&quot;: 5000</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>分析结果，发布到zmq（<a href="https://github.com/emqx/kuiper/blob/master/docs/zh_CN/plugins/sinks/zmq.md">https://github.com/emqx/kuiper/blob/master/docs/zh_CN/plugins/sinks/zmq.md</a>）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;sql&quot;: &quot;SELECT * from demo&quot;,</span><br><span class="line">  &quot;actions&quot;: [</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;zmq&quot;: &#123;</span><br><span class="line">       &quot;server&quot;: &quot;tcp:&#x2F;&#x2F;127.0.0.1:5563&quot;,</span><br><span class="line">       &quot;topic&quot;: &quot;temp&quot;</span><br><span class="line">      &#125;</span><br><span class="line">   &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>分析结果，通过调用rest（<a href="https://github.com/emqx/kuiper/blob/master/docs/en_US/rules/sinks/rest.md">https://github.com/emqx/kuiper/blob/master/docs/en_US/rules/sinks/rest.md</a>）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;sql&quot;: &quot;SELECT * from demo&quot;,</span><br><span class="line">  &quot;actions&quot;: [</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;rest&quot;: &#123;</span><br><span class="line">      &quot;url&quot;: &quot;http:&#x2F;&#x2F;127.0.0.1:48082&#x2F;api&#x2F;v1&#x2F;device&#x2F;cc622d99-f835-4e94-b5cb-b1eff8699dc4&#x2F;command&#x2F;51fce08a-ae19-4bce-b431-b9f363bba705&quot;,       </span><br><span class="line">      &quot;method&quot;: &quot;post&quot;,</span><br><span class="line">      &quot;dataTemplate&quot;: &quot;\&quot;newKey\&quot;:\&quot;&#123;&#123;.key&#125;&#125;\&quot;&quot;,</span><br><span class="line">      &quot;sendSingle&quot;: true</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>这个类似于EdgeX中core services中的command服务</strong></p>
<p>分析结果，发布到edgex（<a href="https://github.com/emqx/kuiper/blob/master/docs/en_US/rules/sinks/edgex.md">https://github.com/emqx/kuiper/blob/master/docs/en_US/rules/sinks/edgex.md</a>）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;sql&quot;: &quot;SELECT * from demo&quot;,</span><br><span class="line">  &quot;actions&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;edgex&quot;: &#123;</span><br><span class="line">        &quot;protocol&quot;: &quot;tcp&quot;,</span><br><span class="line">        &quot;host&quot;: &quot;*&quot;,</span><br><span class="line">        &quot;port&quot;: 5571,</span><br><span class="line">        &quot;topic&quot;: &quot;application&quot;,</span><br><span class="line">        &quot;deviceName&quot;: &quot;kuiper&quot;,</span><br><span class="line">        &quot;contentType&quot;: &quot;application&#x2F;json&quot;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>分析结果，发布到日志文件，默认在log/stream.log</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">  &quot;sql&quot;: &quot;SELECT * from demo&quot;,</span><br><span class="line">  &quot;actions&quot;: [</span><br><span class="line">    &#123;</span><br><span class="line">      &quot;log&quot;: &#123;&#125;</span><br><span class="line">    &#125;</span><br><span class="line">  ]</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>经验证，有些插件不完整</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;kuiper # .&#x2F;bin&#x2F;cli getstatus rule rule1</span><br><span class="line">Connecting to 127.0.0.1:20498... </span><br><span class="line">Stopped: cannot open &#x2F;kuiper&#x2F;plugins&#x2F;sinks&#x2F;File.so: plugin.Open(&quot;&#x2F;kuiper&#x2F;plugins&#x2F;sinks&#x2F;File.so&quot;): Error relocating &#x2F;kuiper&#x2F;plugins&#x2F;sinks&#x2F;File.so: __fprintf_chk: symbol not found.</span><br></pre></td></tr></table></figure>


<p>参考：</p>
<p>1、kuiper官方文档及github地址</p>
<p><a href="https://docs.emqx.io/kuiper/latest/cn/">https://docs.emqx.io/kuiper/latest/cn/</a></p>
<p><a href="https://github.com/emqx/kuiper">https://github.com/emqx/kuiper</a></p>
<p>2、kuiper集成edgex文档<a href="https://github.com/emqx/kuiper/blob/master/docs/en_US/edgex/edgex_rule_engine_tutorial.md">https://github.com/emqx/kuiper/blob/master/docs/en_US/edgex/edgex_rule_engine_tutorial.md</a></p>
]]></content>
      <categories>
        <category>kuiper</category>
        <category>edgex</category>
      </categories>
      <tags>
        <tag>kuiper</tag>
        <tag>edgex</tag>
      </tags>
  </entry>
  <entry>
    <title>K8S部署有状态服务mysql</title>
    <url>/2021/03/03/K8S%E9%83%A8%E7%BD%B2%E6%9C%89%E7%8A%B6%E6%80%81%E6%9C%8D%E5%8A%A1mysql/</url>
    <content><![CDATA[<h2 id="参考：-https-kubernetes-io-zh-docs-tasks-run-application-run-replicated-stateful-application-https-kubernetes-io-zh-docs-tasks-run-application-run-replicated-stateful-application"><a href="#参考：-https-kubernetes-io-zh-docs-tasks-run-application-run-replicated-stateful-application-https-kubernetes-io-zh-docs-tasks-run-application-run-replicated-stateful-application" class="headerlink" title="参考：[https://kubernetes.io/zh/docs/tasks/run-application/run-replicated-stateful-application/]https://kubernetes.io/zh/docs/tasks/run-application/run-replicated-stateful-application/"></a>参考：[<a href="https://kubernetes.io/zh/docs/tasks/run-application/run-replicated-stateful-application/]https://kubernetes.io/zh/docs/tasks/run-application/run-replicated-stateful-application/">https://kubernetes.io/zh/docs/tasks/run-application/run-replicated-stateful-application/]https://kubernetes.io/zh/docs/tasks/run-application/run-replicated-stateful-application/</a></h2><h1 id="部署-MySQL"><a href="#部署-MySQL" class="headerlink" title="部署 MySQL"></a>部署 MySQL</h1><p>MySQL 示例部署包含一个 ConfigMap、两个 Service 与一个 StatefulSet。</p>
<h2 id="创建-ConfigMap"><a href="#创建-ConfigMap" class="headerlink" title="创建 ConfigMap"></a>创建 ConfigMap</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ConfigMap</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql</span><br><span class="line">  labels:</span><br><span class="line">    app: mysql</span><br><span class="line">data:</span><br><span class="line">  master.cnf: |</span><br><span class="line">    # Apply this config only on the master.</span><br><span class="line">    [mysqld]</span><br><span class="line">    log-bin</span><br><span class="line">  slave.cnf: |</span><br><span class="line">    # Apply this config only on slaves.</span><br><span class="line">    [mysqld]</span><br><span class="line">    super-read-only</span><br></pre></td></tr></table></figure>
<p>这个 ConfigMap 提供 my.cnf 覆盖设置，使你可以独立控制 MySQL 主服务器和从服务器的配置。</p>
<h2 id="创建-Service"><a href="#创建-Service" class="headerlink" title="创建 Service"></a>创建 Service</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Headless service for stable DNS entries of StatefulSet members.</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql</span><br><span class="line">  labels:</span><br><span class="line">    app: mysql</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: mysql</span><br><span class="line">    port: 3306</span><br><span class="line">  clusterIP: None</span><br><span class="line">  selector:</span><br><span class="line">    app: mysql</span><br><span class="line">---</span><br><span class="line"># Client service for connecting to any MySQL instance for reads.</span><br><span class="line"># For writes, you must instead connect to the master: mysql-0.mysql.</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql-read</span><br><span class="line">  labels:</span><br><span class="line">    app: mysql</span><br><span class="line">spec:</span><br><span class="line">  ports:</span><br><span class="line">  - name: mysql</span><br><span class="line">    port: 3306</span><br><span class="line">  selector:</span><br><span class="line">    app: mysql</span><br></pre></td></tr></table></figure>
<p>无头服务给 StatefulSet 控制器为集合中每个 Pod 创建的 DNS 条目提供了一个宿主。 因为服务名为 mysql，所以可以通过在同一 Kubernetes 集群和名字中的任何其他 Pod 内解析 &lt;Pod 名称&gt;.mysql 来访问 Pod。</p>
<p>客户端服务称为 mysql-read，是一种常规服务，具有其自己的集群 IP。 该集群 IP 在报告就绪的所有MySQL Pod 之间分配连接。 可能的端点集合包括 MySQL 主节点和所有副本节点。</p>
<p>请注意，只有读查询才能使用负载平衡的客户端服务。 因为只有一个 MySQL 主服务器，所以客户端应直接连接到 MySQL 主服务器 Pod （通过其在无头服务中的 DNS 条目）以执行写入操作。</p>
<h2 id="创建-StatefulSet"><a href="#创建-StatefulSet" class="headerlink" title="创建 StatefulSet"></a>创建 StatefulSet</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: StatefulSet</span><br><span class="line">metadata:</span><br><span class="line">  name: mysql</span><br><span class="line">spec:</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: mysql</span><br><span class="line">  serviceName: mysql</span><br><span class="line">  replicas: 3</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: mysql</span><br><span class="line">    spec:</span><br><span class="line">      imagePullSecrets:</span><br><span class="line">      - name: regcred</span><br><span class="line">      initContainers:</span><br><span class="line">      - name: init-mysql</span><br><span class="line">        image: mysql:5.7</span><br><span class="line">        command:</span><br><span class="line">        - bash</span><br><span class="line">        - &quot;-c&quot;</span><br><span class="line">        - |</span><br><span class="line">          set -ex</span><br><span class="line">          # Generate mysql server-id from pod ordinal index.</span><br><span class="line">          [[ &#96;hostname&#96; &#x3D;~ -([0-9]+)$ ]] || exit 1</span><br><span class="line">          ordinal&#x3D;$&#123;BASH_REMATCH[1]&#125;</span><br><span class="line">          echo [mysqld] &gt; &#x2F;mnt&#x2F;conf.d&#x2F;server-id.cnf</span><br><span class="line">          # Add an offset to avoid reserved server-id&#x3D;0 value.</span><br><span class="line">          echo server-id&#x3D;$((100 + $ordinal)) &gt;&gt; &#x2F;mnt&#x2F;conf.d&#x2F;server-id.cnf</span><br><span class="line">          # Copy appropriate conf.d files from config-map to emptyDir.</span><br><span class="line">          if [[ $ordinal -eq 0 ]]; then</span><br><span class="line">            cp &#x2F;mnt&#x2F;config-map&#x2F;master.cnf &#x2F;mnt&#x2F;conf.d&#x2F;</span><br><span class="line">          else</span><br><span class="line">            cp &#x2F;mnt&#x2F;config-map&#x2F;slave.cnf &#x2F;mnt&#x2F;conf.d&#x2F;</span><br><span class="line">          fi</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: conf</span><br><span class="line">          mountPath: &#x2F;mnt&#x2F;conf.d</span><br><span class="line">        - name: config-map</span><br><span class="line">          mountPath: &#x2F;mnt&#x2F;config-map</span><br><span class="line">      - name: clone-mysql</span><br><span class="line">        image: shayu&#x2F;xtrabackup:1.0</span><br><span class="line">        command:</span><br><span class="line">        - bash</span><br><span class="line">        - &quot;-c&quot;</span><br><span class="line">        - |</span><br><span class="line">          set -ex</span><br><span class="line">          # Skip the clone if data already exists.</span><br><span class="line">          [[ -d &#x2F;var&#x2F;lib&#x2F;mysql&#x2F;mysql ]] &amp;&amp; exit 0</span><br><span class="line">          # Skip the clone on master (ordinal index 0).</span><br><span class="line">          [[ &#96;hostname&#96; &#x3D;~ -([0-9]+)$ ]] || exit 1</span><br><span class="line">          ordinal&#x3D;$&#123;BASH_REMATCH[1]&#125;</span><br><span class="line">          [[ $ordinal -eq 0 ]] &amp;&amp; exit 0</span><br><span class="line">          # Clone data from previous peer.</span><br><span class="line">          ncat --recv-only mysql-$(($ordinal-1)).mysql 3307 | xbstream -x -C &#x2F;var&#x2F;lib&#x2F;mysql</span><br><span class="line">          # Prepare the backup.</span><br><span class="line">          xtrabackup --prepare --target-dir&#x3D;&#x2F;var&#x2F;lib&#x2F;mysql</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: data</span><br><span class="line">          mountPath: &#x2F;var&#x2F;lib&#x2F;mysql</span><br><span class="line">          subPath: mysql</span><br><span class="line">        - name: conf</span><br><span class="line">          mountPath: &#x2F;etc&#x2F;mysql&#x2F;conf.d</span><br><span class="line">      containers:</span><br><span class="line">      - name: mysql</span><br><span class="line">        image: mysql:5.7</span><br><span class="line">        env:</span><br><span class="line">        - name: MYSQL_ALLOW_EMPTY_PASSWORD</span><br><span class="line">          value: &quot;1&quot;</span><br><span class="line">        ports:</span><br><span class="line">        - name: mysql</span><br><span class="line">          containerPort: 3306</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: data</span><br><span class="line">          mountPath: &#x2F;var&#x2F;lib&#x2F;mysql</span><br><span class="line">          subPath: mysql</span><br><span class="line">        - name: conf</span><br><span class="line">          mountPath: &#x2F;etc&#x2F;mysql&#x2F;conf.d</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 1Gi</span><br><span class="line">        livenessProbe:</span><br><span class="line">          exec:</span><br><span class="line">            command: [&quot;mysqladmin&quot;, &quot;ping&quot;]</span><br><span class="line">          initialDelaySeconds: 30</span><br><span class="line">          periodSeconds: 10</span><br><span class="line">          timeoutSeconds: 5</span><br><span class="line">        readinessProbe:</span><br><span class="line">          exec:</span><br><span class="line">            # Check we can execute queries over TCP (skip-networking is off).</span><br><span class="line">            command: [&quot;mysql&quot;, &quot;-h&quot;, &quot;127.0.0.1&quot;, &quot;-e&quot;, &quot;SELECT 1&quot;]</span><br><span class="line">          initialDelaySeconds: 5</span><br><span class="line">          periodSeconds: 2</span><br><span class="line">          timeoutSeconds: 1</span><br><span class="line">      - name: xtrabackup</span><br><span class="line">        image: shayu&#x2F;xtrabackup:1.0</span><br><span class="line">        ports:</span><br><span class="line">        - name: xtrabackup</span><br><span class="line">          containerPort: 3307</span><br><span class="line">        command:</span><br><span class="line">        - bash</span><br><span class="line">        - &quot;-c&quot;</span><br><span class="line">        - |</span><br><span class="line">          set -ex</span><br><span class="line">          cd &#x2F;var&#x2F;lib&#x2F;mysql</span><br><span class="line"></span><br><span class="line">          # Determine binlog position of cloned data, if any.</span><br><span class="line">          if [[ -f xtrabackup_slave_info &amp;&amp; &quot;x$(&lt;xtrabackup_slave_info)&quot; !&#x3D; &quot;x&quot; ]]; then</span><br><span class="line">            # XtraBackup already generated a partial &quot;CHANGE MASTER TO&quot; query</span><br><span class="line">            # because we&#39;re cloning from an existing slave. (Need to remove the tailing semicolon!)</span><br><span class="line">            cat xtrabackup_slave_info | sed -E &#39;s&#x2F;;$&#x2F;&#x2F;g&#39; &gt; change_master_to.sql.in</span><br><span class="line">            # Ignore xtrabackup_binlog_info in this case (it&#39;s useless).</span><br><span class="line">            rm -f xtrabackup_slave_info xtrabackup_binlog_info</span><br><span class="line">          elif [[ -f xtrabackup_binlog_info ]]; then</span><br><span class="line">            # We&#39;re cloning directly from master. Parse binlog position.</span><br><span class="line">            [[ &#96;cat xtrabackup_binlog_info&#96; &#x3D;~ ^(.*?)[[:space:]]+(.*?)$ ]] || exit 1</span><br><span class="line">            rm -f xtrabackup_binlog_info xtrabackup_slave_info</span><br><span class="line">            echo &quot;CHANGE MASTER TO MASTER_LOG_FILE&#x3D;&#39;$&#123;BASH_REMATCH[1]&#125;&#39;,\</span><br><span class="line">                  MASTER_LOG_POS&#x3D;$&#123;BASH_REMATCH[2]&#125;&quot; &gt; change_master_to.sql.in</span><br><span class="line">          fi</span><br><span class="line"></span><br><span class="line">          # Check if we need to complete a clone by starting replication.</span><br><span class="line">          if [[ -f change_master_to.sql.in ]]; then</span><br><span class="line">            echo &quot;Waiting for mysqld to be ready (accepting connections)&quot;</span><br><span class="line">            until mysql -h 127.0.0.1 -e &quot;SELECT 1&quot;; do sleep 1; done</span><br><span class="line"></span><br><span class="line">            echo &quot;Initializing replication from clone position&quot;</span><br><span class="line">            mysql -h 127.0.0.1 \</span><br><span class="line">                  -e &quot;$(&lt;change_master_to.sql.in), \</span><br><span class="line">                          MASTER_HOST&#x3D;&#39;mysql-0.mysql&#39;, \</span><br><span class="line">                          MASTER_USER&#x3D;&#39;root&#39;, \</span><br><span class="line">                          MASTER_PASSWORD&#x3D;&#39;&#39;, \</span><br><span class="line">                          MASTER_CONNECT_RETRY&#x3D;10; \</span><br><span class="line">                        START SLAVE;&quot; || exit 1</span><br><span class="line">            # In case of container restart, attempt this at-most-once.</span><br><span class="line">            mv change_master_to.sql.in change_master_to.sql.orig</span><br><span class="line">          fi</span><br><span class="line"></span><br><span class="line">          # Start a server to send backups when requested by peers.</span><br><span class="line">          exec ncat --listen --keep-open --send-only --max-conns&#x3D;1 3307 -c \</span><br><span class="line">            &quot;xtrabackup --backup --slave-info --stream&#x3D;xbstream --host&#x3D;127.0.0.1 --user&#x3D;root&quot;</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: data</span><br><span class="line">          mountPath: &#x2F;var&#x2F;lib&#x2F;mysql</span><br><span class="line">          subPath: mysql</span><br><span class="line">        - name: conf</span><br><span class="line">          mountPath: &#x2F;etc&#x2F;mysql&#x2F;conf.d</span><br><span class="line">        resources:</span><br><span class="line">          requests:</span><br><span class="line">            cpu: 100m</span><br><span class="line">            memory: 100Mi</span><br><span class="line">      volumes:</span><br><span class="line">      - name: conf</span><br><span class="line">        emptyDir: &#123;&#125;</span><br><span class="line">      - name: config-map</span><br><span class="line">        configMap:</span><br><span class="line">          name: mysql</span><br><span class="line">  volumeClaimTemplates:</span><br><span class="line">  - metadata:</span><br><span class="line">      name: data</span><br><span class="line">      annotations:</span><br><span class="line">        volume.beta.kubernetes.io&#x2F;storage-class: managed-nfs-storage</span><br><span class="line">    spec:</span><br><span class="line">      accessModes: [&quot;ReadWriteOnce&quot;]</span><br><span class="line">      resources:</span><br><span class="line">        requests:</span><br><span class="line">          storage: 1Gi</span><br></pre></td></tr></table></figure>
<h2 id="设置动态获取PV，PV申请自nfs"><a href="#设置动态获取PV，PV申请自nfs" class="headerlink" title="设置动态获取PV，PV申请自nfs"></a>设置动态获取PV，PV申请自nfs</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kind: Deployment</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">  namespace: kube-system</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  strategy:</span><br><span class="line">    type: Recreate</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      app: nfs-client-provisioner</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        app: nfs-client-provisioner</span><br><span class="line">    spec:</span><br><span class="line">      serviceAccountName: nfs-client-provisioner</span><br><span class="line">      containers:</span><br><span class="line">        - name: nfs-client-provisioner</span><br><span class="line">          image: quay.io&#x2F;external_storage&#x2F;nfs-client-provisioner:latest</span><br><span class="line">          volumeMounts:</span><br><span class="line">            - name: nfs-client-root</span><br><span class="line">              mountPath: &#x2F;persistentvolumes</span><br><span class="line">          env:</span><br><span class="line">            - name: PROVISIONER_NAME</span><br><span class="line">              value: shayu</span><br><span class="line">            - name: NFS_SERVER</span><br><span class="line">              value: 10.0.104.27</span><br><span class="line">            - name: NFS_PATH</span><br><span class="line">              value: &#x2F;data&#x2F;mysql</span><br><span class="line">      volumes:</span><br><span class="line">        - name: nfs-client-root</span><br><span class="line">          nfs:</span><br><span class="line">            server: 10.0.104.27</span><br><span class="line">            path: &#x2F;data&#x2F;mysql</span><br></pre></td></tr></table></figure>
<p>10.0.104.27需要填写nfs-server服务器地址，需要提前搭建好。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: storage.k8s.io&#x2F;v1</span><br><span class="line">kind: StorageClass</span><br><span class="line">metadata:</span><br><span class="line">  name: managed-nfs-storage</span><br><span class="line">provisioner: shayu</span><br><span class="line">parameters:</span><br><span class="line">  archiveOnDelete: &quot;false&quot;</span><br></pre></td></tr></table></figure>
<h2 id="设置相关的ServiceAccount集群权限设置"><a href="#设置相关的ServiceAccount集群权限设置" class="headerlink" title="设置相关的ServiceAccount集群权限设置"></a>设置相关的ServiceAccount集群权限设置</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: ServiceAccount</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner</span><br><span class="line">  # replace with namespace where provisioner is deployed</span><br><span class="line">  namespace: kube-system</span><br><span class="line">---</span><br><span class="line">kind: ClusterRole</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumes&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;delete&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;persistentvolumeclaims&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;update&quot;]</span><br><span class="line">  - apiGroups: [&quot;storage.k8s.io&quot;]</span><br><span class="line">    resources: [&quot;storageclasses&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;]</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;events&quot;]</span><br><span class="line">    verbs: [&quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">---</span><br><span class="line">kind: ClusterRoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: run-nfs-client-provisioner</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    # replace with namespace where provisioner is deployed</span><br><span class="line">    namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  kind: ClusterRole</span><br><span class="line">  name: nfs-client-provisioner-runner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br><span class="line">---</span><br><span class="line">kind: Role</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">  # replace with namespace where provisioner is deployed</span><br><span class="line">  namespace: kube-system</span><br><span class="line">rules:</span><br><span class="line">  - apiGroups: [&quot;&quot;]</span><br><span class="line">    resources: [&quot;endpoints&quot;]</span><br><span class="line">    verbs: [&quot;get&quot;, &quot;list&quot;, &quot;watch&quot;, &quot;create&quot;, &quot;update&quot;, &quot;patch&quot;]</span><br><span class="line">---</span><br><span class="line">kind: RoleBinding</span><br><span class="line">apiVersion: rbac.authorization.k8s.io&#x2F;v1</span><br><span class="line">metadata:</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">  # replace with namespace where provisioner is deployed</span><br><span class="line">  namespace: kube-system</span><br><span class="line">subjects:</span><br><span class="line">  - kind: ServiceAccount</span><br><span class="line">    name: nfs-client-provisioner</span><br><span class="line">    # replace with namespace where provisioner is deployed</span><br><span class="line">    namespace: kube-system</span><br><span class="line">roleRef:</span><br><span class="line">  kind: Role</span><br><span class="line">  name: leader-locking-nfs-client-provisioner</span><br><span class="line">  apiGroup: rbac.authorization.k8s.io</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# kubectl get pv</span><br><span class="line">NAME                                       CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM                  STORAGECLASS          REASON   AGE</span><br><span class="line">pvc-473f6785-becc-448c-a63c-5c5adafaffac   1Gi        RWO            Delete           Bound    default&#x2F;data-mysql-1   managed-nfs-storage            3h3m</span><br><span class="line">pvc-6a73a3e8-893b-47c2-bd60-1425d15865bf   1Gi        RWO            Delete           Bound    default&#x2F;data-mysql-2   managed-nfs-storage            172m</span><br><span class="line">pvc-fa8ed1fb-4f9c-403c-83b7-4a8038ccfb3b   1Gi        RWO            Delete           Bound    default&#x2F;data-mysql-0   managed-nfs-storage            3h16m</span><br><span class="line">root@ubuntu-001:~# kubectl get pvc</span><br><span class="line">NAME           STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS          AGE</span><br><span class="line">data-mysql-0   Bound    pvc-fa8ed1fb-4f9c-403c-83b7-4a8038ccfb3b   1Gi        RWO            managed-nfs-storage   3h18m</span><br><span class="line">data-mysql-1   Bound    pvc-473f6785-becc-448c-a63c-5c5adafaffac   1Gi        RWO            managed-nfs-storage   3h3m</span><br><span class="line">data-mysql-2   Bound    pvc-6a73a3e8-893b-47c2-bd60-1425d15865bf   1Gi        RWO            managed-nfs-storage   172m</span><br><span class="line">root@ubuntu-001:~# kubectl get pods</span><br><span class="line">NAME      READY   STATUS    RESTARTS   AGE</span><br><span class="line">mysql-0   2&#x2F;2     Running   0          48s</span><br><span class="line">mysql-1   2&#x2F;2     Running   0          31s</span><br><span class="line">mysql-2   2&#x2F;2     Running   0          24s</span><br></pre></td></tr></table></figure>

<h2 id="了解有状态的-Pod-初始化"><a href="#了解有状态的-Pod-初始化" class="headerlink" title="了解有状态的 Pod 初始化"></a>了解有状态的 Pod 初始化</h2><p>StatefulSet 控制器按序数索引顺序地每次启动一个 Pod。 它一直等到每个 Pod 报告就绪才再启动下一个 Pod。</p>
<p>此外，控制器为每个 Pod 分配一个唯一、稳定的名称，形如 &lt;statefulset 名称&gt;-&lt;序数索引&gt;， 其结果是 Pods 名为 mysql-0、mysql-1 和 mysql-2。</p>
<p>上述 StatefulSet 清单中的 Pod 模板利用这些属性来执行 MySQL 副本的有序启动。</p>
<h2 id="生成配置"><a href="#生成配置" class="headerlink" title="生成配置"></a>生成配置</h2><p>在启动 Pod 规约中的任何容器之前，Pod 首先按顺序运行所有的 Init 容器。</p>
<p>第一个名为 init-mysql 的 Init 容器根据序号索引生成特殊的 MySQL 配置文件。</p>
<p>该脚本通过从 Pod 名称的末尾提取索引来确定自己的序号索引，而 Pod 名称由 hostname 命令返回。 然后将序数（带有数字偏移量以避免保留值）保存到 MySQL conf.d 目录中的文件 server-id.cnf。 这一操作将 StatefulSet 所提供的唯一、稳定的标识转换为 MySQL 服务器的 ID， 而这些 ID 也是需要唯一性、稳定性保证的。</p>
<p>通过将内容复制到 conf.d 中，init-mysql 容器中的脚本也可以应用 ConfigMap 中的 primary.cnf 或 replica.cnf。 由于示例部署结构由单个 MySQL 主节点和任意数量的副本节点组成，因此脚本仅将序数 0 指定为主节点，而将其他所有节点指定为副本节点。</p>
<p>与 StatefulSet 控制器的 部署顺序保证 相结合， 可以确保 MySQL 主服务器在创建副本服务器之前已准备就绪，以便它们可以开始复制。</p>
<h2 id="克隆现有数据"><a href="#克隆现有数据" class="headerlink" title="克隆现有数据"></a>克隆现有数据</h2><p>通常，当新 Pod 作为副本节点加入集合时，必须假定 MySQL 主节点可能已经有数据。 还必须假设复制日志可能不会一直追溯到时间的开始。</p>
<p>这些保守的假设是允许正在运行的 StatefulSet 随时间扩大和缩小而不是固定在其初始大小的关键。</p>
<p>第二个名为 clone-mysql 的 Init 容器，第一次在带有空 PersistentVolume 的副本 Pod 上启动时，会在从属 Pod 上执行克隆操作。 这意味着它将从另一个运行中的 Pod 复制所有现有数据，使此其本地状态足够一致， 从而可以开始从主服务器复制。</p>
<p>MySQL 本身不提供执行此操作的机制，因此本示例使用了一种流行的开源工具 Percona XtraBackup。 在克隆期间，源 MySQL 服务器性能可能会受到影响。 为了最大程度地减少对 MySQL 主服务器的影响，该脚本指示每个 Pod 从序号较低的 Pod 中克隆。 可以这样做的原因是 StatefulSet 控制器始终确保在启动 Pod N + 1 之前 Pod N 已准备就绪。</p>
<h2 id="开始复制"><a href="#开始复制" class="headerlink" title="开始复制"></a>开始复制</h2><p>Init 容器成功完成后，应用容器将运行。 MySQL Pod 由运行实际 mysqld 服务的 mysql 容器和充当 辅助工具 的 xtrabackup 容器组成。</p>
<p>xtrabackup sidecar 容器查看克隆的数据文件，并确定是否有必要在副本服务器上初始化 MySQL 复制。 如果是这样，它将等待 mysqld 准备就绪，然后使用从 XtraBackup 克隆文件中提取的复制参数 执行 CHANGE MASTER TO 和 START SLAVE 命令。</p>
<p>一旦副本服务器开始复制后，它会记住其 MySQL 主服务器，并且如果服务器重新启动或 连接中断也会自动重新连接。 另外，因为副本服务器会以其稳定的 DNS 名称查找主服务器（mysql-0.mysql）， 即使由于重新调度而获得新的 Pod IP，它们也会自动找到主服务器。</p>
<p>最后，开始复制后，xtrabackup 容器监听来自其他 Pod 的连接，处理其数据克隆请求。 如果 StatefulSet 扩大规模，或者下一个 Pod 失去其 PersistentVolumeClaim 并需要重新克隆， 则此服务器将无限期保持运行。</p>
]]></content>
      <categories>
        <category>kubernetes</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>containerd with kata-container</title>
    <url>/2021/02/07/containerd-with-kata-container/</url>
    <content><![CDATA[<p>参考：<a href="https://github.com/kata-containers/documentation/blob/master/how-to/containerd-kata.md">https://github.com/kata-containers/documentation/blob/master/how-to/containerd-kata.md</a></p>
<h1 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h1><p>已安装kata container及containerd with CRI plugin</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# kata-runtime --version</span><br><span class="line">kata-runtime  : 1.13.0-alpha0</span><br><span class="line">   commit   : &lt;&lt;unknown&gt;&gt;</span><br><span class="line">   OCI specs: 1.0.1-dev</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# containerd --version</span><br><span class="line">containerd containerd.io 1.4.3 269548fa27e0089a8b8278fc4fc781d7f65a939b</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# ctr --version</span><br><span class="line">ctr containerd.io 1.4.3</span><br></pre></td></tr></table></figure>
<p>注意：</p>
<p>containerd在安装docker（新版本docker）已经作为docker依赖项安装了</p>
<p>ctr是containerd的命令行工具（containerd CLI）</p>
<p>cri is a native plugin of containerd 1.1 and above. It is built into containerd and enabled by default. You do not need to install cri if you have containerd 1.1 or above. Just remove the cri plugin from the list of disabled_plugins in the containerd configuration file (/etc/containerd/config.toml).</p>
<h1 id="配置containerd配置文件"><a href="#配置containerd配置文件" class="headerlink" title="配置containerd配置文件"></a>配置containerd配置文件</h1><p>1、获取containerd默认配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# containerd config default &gt; &#x2F;etc&#x2F;containerd&#x2F;config.toml</span><br></pre></td></tr></table></figure>

<p>2、修改/etc/containerd/config.toml，增加kata配置</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# cat &#x2F;etc&#x2F;containerd&#x2F;config.toml </span><br><span class="line">version &#x3D; 2</span><br><span class="line">root &#x3D; &quot;&#x2F;var&#x2F;lib&#x2F;containerd&quot;</span><br><span class="line">state &#x3D; &quot;&#x2F;run&#x2F;containerd&quot;</span><br><span class="line">plugin_dir &#x3D; &quot;&quot;</span><br><span class="line">disabled_plugins &#x3D; []</span><br><span class="line">required_plugins &#x3D; []</span><br><span class="line">oom_score &#x3D; 0</span><br><span class="line"></span><br><span class="line">[grpc]</span><br><span class="line">  address &#x3D; &quot;&#x2F;run&#x2F;containerd&#x2F;containerd.sock&quot;</span><br><span class="line">  tcp_address &#x3D; &quot;&quot;</span><br><span class="line">  tcp_tls_cert &#x3D; &quot;&quot;</span><br><span class="line">  tcp_tls_key &#x3D; &quot;&quot;</span><br><span class="line">  uid &#x3D; 0</span><br><span class="line">  gid &#x3D; 0</span><br><span class="line">  max_recv_message_size &#x3D; 16777216</span><br><span class="line">  max_send_message_size &#x3D; 16777216</span><br><span class="line"></span><br><span class="line">[ttrpc]</span><br><span class="line">  address &#x3D; &quot;&quot;</span><br><span class="line">  uid &#x3D; 0</span><br><span class="line">  gid &#x3D; 0</span><br><span class="line"></span><br><span class="line">[debug]</span><br><span class="line">  address &#x3D; &quot;&quot;</span><br><span class="line">  uid &#x3D; 0</span><br><span class="line">  gid &#x3D; 0</span><br><span class="line">  level &#x3D; &quot;&quot;</span><br><span class="line"></span><br><span class="line">[metrics]</span><br><span class="line">  address &#x3D; &quot;&quot;</span><br><span class="line">  grpc_histogram &#x3D; false</span><br><span class="line"></span><br><span class="line">[cgroup]</span><br><span class="line">  path &#x3D; &quot;&quot;</span><br><span class="line"></span><br><span class="line">[timeouts]</span><br><span class="line">  &quot;io.containerd.timeout.shim.cleanup&quot; &#x3D; &quot;5s&quot;</span><br><span class="line">  &quot;io.containerd.timeout.shim.load&quot; &#x3D; &quot;5s&quot;</span><br><span class="line">  &quot;io.containerd.timeout.shim.shutdown&quot; &#x3D; &quot;3s&quot;</span><br><span class="line">  &quot;io.containerd.timeout.task.state&quot; &#x3D; &quot;2s&quot;</span><br><span class="line"></span><br><span class="line">[plugins]</span><br><span class="line">  [plugins.&quot;io.containerd.gc.v1.scheduler&quot;]</span><br><span class="line">    pause_threshold &#x3D; 0.02</span><br><span class="line">    deletion_threshold &#x3D; 0</span><br><span class="line">    mutation_threshold &#x3D; 100</span><br><span class="line">    schedule_delay &#x3D; &quot;0s&quot;</span><br><span class="line">    startup_delay &#x3D; &quot;100ms&quot;</span><br><span class="line">  [plugins.&quot;io.containerd.grpc.v1.cri&quot;]</span><br><span class="line">    disable_tcp_service &#x3D; true</span><br><span class="line">    stream_server_address &#x3D; &quot;127.0.0.1&quot;</span><br><span class="line">    stream_server_port &#x3D; &quot;0&quot;</span><br><span class="line">    stream_idle_timeout &#x3D; &quot;4h0m0s&quot;</span><br><span class="line">    enable_selinux &#x3D; false</span><br><span class="line">    selinux_category_range &#x3D; 1024</span><br><span class="line">    sandbox_image &#x3D; &quot;registry.aliyuncs.com&#x2F;google_containers&#x2F;pause:3.2&quot;</span><br><span class="line">    stats_collect_period &#x3D; 10</span><br><span class="line">    systemd_cgroup &#x3D; false</span><br><span class="line">    enable_tls_streaming &#x3D; false</span><br><span class="line">    max_container_log_line_size &#x3D; 16384</span><br><span class="line">    disable_cgroup &#x3D; false</span><br><span class="line">    disable_apparmor &#x3D; false</span><br><span class="line">    restrict_oom_score_adj &#x3D; false</span><br><span class="line">    max_concurrent_downloads &#x3D; 3</span><br><span class="line">    disable_proc_mount &#x3D; false</span><br><span class="line">    unset_seccomp_profile &#x3D; &quot;&quot;</span><br><span class="line">    tolerate_missing_hugetlb_controller &#x3D; true</span><br><span class="line">    disable_hugetlb_controller &#x3D; true</span><br><span class="line">    ignore_image_defined_volumes &#x3D; false</span><br><span class="line">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd]</span><br><span class="line">      snapshotter &#x3D; &quot;overlayfs&quot;</span><br><span class="line">      default_runtime_name &#x3D; &quot;runc&quot;</span><br><span class="line">      no_pivot &#x3D; false</span><br><span class="line">      disable_snapshot_annotations &#x3D; true</span><br><span class="line">      discard_unpacked_layers &#x3D; false</span><br><span class="line">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.default_runtime]</span><br><span class="line">        runtime_type &#x3D; &quot;&quot;</span><br><span class="line">        runtime_engine &#x3D; &quot;&quot;</span><br><span class="line">        runtime_root &#x3D; &quot;&quot;</span><br><span class="line">        privileged_without_host_devices &#x3D; false</span><br><span class="line">        base_runtime_spec &#x3D; &quot;&quot;</span><br><span class="line">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.untrusted_workload_runtime]</span><br><span class="line">        runtime_type &#x3D; &quot;&quot;</span><br><span class="line">        runtime_engine &#x3D; &quot;&quot;</span><br><span class="line">        runtime_root &#x3D; &quot;&quot;</span><br><span class="line">        privileged_without_host_devices &#x3D; false</span><br><span class="line">        base_runtime_spec &#x3D; &quot;&quot;</span><br><span class="line">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes]</span><br><span class="line">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc]</span><br><span class="line">          runtime_type &#x3D; &quot;io.containerd.runc.v2&quot;</span><br><span class="line">          runtime_engine &#x3D; &quot;&quot;</span><br><span class="line">          runtime_root &#x3D; &quot;&quot;</span><br><span class="line">          privileged_without_host_devices &#x3D; false</span><br><span class="line">          base_runtime_spec &#x3D; &quot;&quot;</span><br><span class="line">          [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.runc.options]</span><br><span class="line">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.kata]</span><br><span class="line">          runtime_type &#x3D; &quot;io.containerd.kata.v2&quot;</span><br><span class="line">          runtime_engine &#x3D; &quot;&quot;</span><br><span class="line">          runtime_root &#x3D; &quot;&quot;</span><br><span class="line">          privileged_without_host_devices &#x3D; false</span><br><span class="line">          base_runtime_spec &#x3D; &quot;&quot;</span><br><span class="line">          [plugins.&quot;io.containerd.grpc.v1.cri&quot;.containerd.runtimes.kata.options]</span><br><span class="line">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.cni]</span><br><span class="line">      bin_dir &#x3D; &quot;&#x2F;opt&#x2F;cni&#x2F;bin&quot;</span><br><span class="line">      conf_dir &#x3D; &quot;&#x2F;etc&#x2F;cni&#x2F;net.d&quot;</span><br><span class="line">      max_conf_num &#x3D; 1</span><br><span class="line">      conf_template &#x3D; &quot;&quot;</span><br><span class="line">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry]</span><br><span class="line">      [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors]</span><br><span class="line">        [plugins.&quot;io.containerd.grpc.v1.cri&quot;.registry.mirrors.&quot;docker.io&quot;]</span><br><span class="line">          endpoint &#x3D; [&quot;https:&#x2F;&#x2F;registry-1.docker.io&quot;]</span><br><span class="line">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.image_decryption]</span><br><span class="line">      key_model &#x3D; &quot;&quot;</span><br><span class="line">    [plugins.&quot;io.containerd.grpc.v1.cri&quot;.x509_key_pair_streaming]</span><br><span class="line">      tls_cert_file &#x3D; &quot;&quot;</span><br><span class="line">      tls_key_file &#x3D; &quot;&quot;</span><br><span class="line">  [plugins.&quot;io.containerd.internal.v1.opt&quot;]</span><br><span class="line">    path &#x3D; &quot;&#x2F;opt&#x2F;containerd&quot;</span><br><span class="line">  [plugins.&quot;io.containerd.internal.v1.restart&quot;]</span><br><span class="line">    interval &#x3D; &quot;10s&quot;</span><br><span class="line">  [plugins.&quot;io.containerd.metadata.v1.bolt&quot;]</span><br><span class="line">    content_sharing_policy &#x3D; &quot;shared&quot;</span><br><span class="line">  [plugins.&quot;io.containerd.monitor.v1.cgroups&quot;]</span><br><span class="line">    no_prometheus &#x3D; false</span><br><span class="line">  [plugins.&quot;io.containerd.runtime.v1.linux&quot;]</span><br><span class="line">    shim &#x3D; &quot;containerd-shim&quot;</span><br><span class="line">    runtime &#x3D; &quot;runc&quot;</span><br><span class="line">    runtime_root &#x3D; &quot;&quot;</span><br><span class="line">    no_shim &#x3D; false</span><br><span class="line">    shim_debug &#x3D; false</span><br><span class="line">  [plugins.&quot;io.containerd.runtime.v2.task&quot;]</span><br><span class="line">    platforms &#x3D; [&quot;linux&#x2F;amd64&quot;]</span><br><span class="line">  [plugins.&quot;io.containerd.service.v1.diff-service&quot;]</span><br><span class="line">    default &#x3D; [&quot;walking&quot;]</span><br><span class="line">  [plugins.&quot;io.containerd.snapshotter.v1.devmapper&quot;]</span><br><span class="line">    root_path &#x3D; &quot;&quot;</span><br><span class="line">    pool_name &#x3D; &quot;&quot;</span><br><span class="line">    base_image_size &#x3D; &quot;&quot;</span><br><span class="line">    async_remove &#x3D; false</span><br></pre></td></tr></table></figure>

<p>对于配置文件的说明，详细见：<a href="https://github.com/containerd/cri/blob/master/docs/config.md">https://github.com/containerd/cri/blob/master/docs/config.md</a></p>
<h1 id="重启containerd服务"><a href="#重启containerd服务" class="headerlink" title="重启containerd服务"></a>重启containerd服务</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# systemctl restart containerd.service </span><br><span class="line">root@ubuntu-001:~# systemctl status containerd.service </span><br><span class="line">● containerd.service - containerd container runtime</span><br><span class="line">   Loaded: loaded (&#x2F;lib&#x2F;systemd&#x2F;system&#x2F;containerd.service; enabled; vendor preset: enabled)</span><br><span class="line">   Active: active (running) since Sun 2021-02-07 14:47:19 CST; 7s ago</span><br><span class="line">     Docs: https:&#x2F;&#x2F;containerd.io</span><br><span class="line">  Process: 49492 ExecStartPre&#x3D;&#x2F;sbin&#x2F;modprobe overlay (code&#x3D;exited, status&#x3D;0&#x2F;SUCCESS)</span><br><span class="line"> Main PID: 49495 (containerd)</span><br><span class="line">    Tasks: 12</span><br><span class="line">   CGroup: &#x2F;system.slice&#x2F;containerd.service</span><br><span class="line">           └─49495 &#x2F;usr&#x2F;bin&#x2F;containerd</span><br><span class="line"></span><br><span class="line">Feb 07 14:47:19 ubuntu-001 containerd[49495]: time&#x3D;&quot;2021-02-07T14:47:19.601885424+08:00&quot; level&#x3D;info msg&#x3D;serving... address&#x3D;&#x2F;run&#x2F;containerd&#x2F;containerd.sock.ttrpc</span><br><span class="line">Feb 07 14:47:19 ubuntu-001 containerd[49495]: time&#x3D;&quot;2021-02-07T14:47:19.602030656+08:00&quot; level&#x3D;info msg&#x3D;serving... address&#x3D;&#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">Feb 07 14:47:19 ubuntu-001 systemd[1]: Started containerd container runtime.</span><br><span class="line">Feb 07 14:47:19 ubuntu-001 containerd[49495]: time&#x3D;&quot;2021-02-07T14:47:19.603347612+08:00&quot; level&#x3D;info msg&#x3D;&quot;containerd successfully booted in 0.038926s&quot;</span><br><span class="line">Feb 07 14:47:19 ubuntu-001 containerd[49495]: time&#x3D;&quot;2021-02-07T14:47:19.620111786+08:00&quot; level&#x3D;info msg&#x3D;&quot;Start subscribing containerd event&quot;</span><br><span class="line">Feb 07 14:47:19 ubuntu-001 containerd[49495]: time&#x3D;&quot;2021-02-07T14:47:19.620341044+08:00&quot; level&#x3D;info msg&#x3D;&quot;Start recovering state&quot;</span><br><span class="line">Feb 07 14:47:19 ubuntu-001 containerd[49495]: time&#x3D;&quot;2021-02-07T14:47:19.647143755+08:00&quot; level&#x3D;info msg&#x3D;&quot;Start event monitor&quot;</span><br><span class="line">Feb 07 14:47:19 ubuntu-001 containerd[49495]: time&#x3D;&quot;2021-02-07T14:47:19.647352452+08:00&quot; level&#x3D;info msg&#x3D;&quot;Start snapshots syncer&quot;</span><br><span class="line">Feb 07 14:47:19 ubuntu-001 containerd[49495]: time&#x3D;&quot;2021-02-07T14:47:19.647463902+08:00&quot; level&#x3D;info msg&#x3D;&quot;Start cni network conf syncer&quot;</span><br><span class="line">Feb 07 14:47:19 ubuntu-001 containerd[49495]: time&#x3D;&quot;2021-02-07T14:47:19.647544196+08:00&quot; level&#x3D;info msg&#x3D;&quot;Start streaming server&quot;</span><br></pre></td></tr></table></figure>

<h1 id="配置cri的命令行（crictl）配置"><a href="#配置cri的命令行（crictl）配置" class="headerlink" title="配置cri的命令行（crictl）配置"></a>配置cri的命令行（crictl）配置</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# cat &#x2F;etc&#x2F;crictl.yaml </span><br><span class="line">runtime-endpoint: unix:&#x2F;&#x2F;&#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">image-endpoint: unix:&#x2F;&#x2F;&#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">timeout: 10</span><br><span class="line">debug: false</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# crictl version</span><br><span class="line">Version:  0.1.0</span><br><span class="line">RuntimeName:  containerd</span><br><span class="line">RuntimeVersion:  1.4.3</span><br><span class="line">RuntimeApiVersion:  v1alpha2</span><br></pre></td></tr></table></figure>

<h1 id="使用containerd运行容器"><a href="#使用containerd运行容器" class="headerlink" title="使用containerd运行容器"></a>使用containerd运行容器</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pull镜像</span><br><span class="line">root@ubuntu-001:~# ctr image pull docker.io&#x2F;library&#x2F;busybox:latest</span><br><span class="line">docker.io&#x2F;library&#x2F;busybox:latest:                                                 resolved       |++++++++++++++++++++++++++++++++++++++| </span><br><span class="line">index-sha256:e1488cb900233d035575f0a7787448cb1fa93bed0ccc0d4efc1963d7d72a8f17:    done           |++++++++++++++++++++++++++++++++++++++| </span><br><span class="line">manifest-sha256:56853b711255f4a0bc7c44d2158167f03f64ef75a22a0249a9fae4703ec10f61: done           |++++++++++++++++++++++++++++++++++++++| </span><br><span class="line">layer-sha256:4c892f00285e3ea7b8a08a03d04df2bc021a11fe838aa23d8e4ed17081ea3c18:    done           |++++++++++++++++++++++++++++++++++++++| </span><br><span class="line">config-sha256:22667f53682a2920948d19c7133ab1c9c3f745805c14125859d20cede07f11f9:   done           |++++++++++++++++++++++++++++++++++++++| </span><br><span class="line">elapsed: 12.7s                                                                    total:  4.0 Ki (319.0 B&#x2F;s)                                       </span><br><span class="line">unpacking linux&#x2F;amd64 sha256:e1488cb900233d035575f0a7787448cb1fa93bed0ccc0d4efc1963d7d72a8f17...</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">查询镜像</span><br><span class="line">root@ubuntu-001:~# ctr images ls</span><br><span class="line">REF                              TYPE                                                      DIGEST                                                                  SIZE      PLATFORMS                                                                                                            LABELS </span><br><span class="line">docker.io&#x2F;library&#x2F;busybox:latest application&#x2F;vnd.docker.distribution.manifest.list.v2+json sha256:e1488cb900233d035575f0a7787448cb1fa93bed0ccc0d4efc1963d7d72a8f17 750.7 KiB linux&#x2F;386,linux&#x2F;amd64,linux&#x2F;arm&#x2F;v5,linux&#x2F;arm&#x2F;v6,linux&#x2F;arm&#x2F;v7,linux&#x2F;arm64&#x2F;v8,linux&#x2F;mips64le,linux&#x2F;ppc64le,linux&#x2F;s390x -</span><br><span class="line"></span><br><span class="line">运行容器</span><br><span class="line">root@ubuntu-001:~# ctr run --runtime io.containerd.run.kata.v2 -t -d --rm docker.io&#x2F;library&#x2F;busybox:latest hello-kata</span><br><span class="line">root@ubuntu-001:~# ctr run -t -d --rm docker.io&#x2F;library&#x2F;busybox:latest hello-runc</span><br><span class="line">&#x2F; # root@ubuntu-001:~# </span><br><span class="line">root@ubuntu-001:~# ps -ef|grep containerd</span><br><span class="line">root      42028   1725  0 14:13 ?        00:00:01 containerd</span><br><span class="line">root      49495      1  0 14:47 ?        00:00:04 &#x2F;usr&#x2F;bin&#x2F;containerd</span><br><span class="line">root      49524      1  0 14:47 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;dockerd -H fd:&#x2F;&#x2F; --containerd&#x3D;&#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      63129      1  0 15:43 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-kata-v2 -namespace default -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock -publish-binary &#x2F;usr&#x2F;bin&#x2F;containerd -id hello-kata</span><br><span class="line">root      63219      1  0 15:43 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace default -id hello-runc -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      63382  58536  0 15:43 pts&#x2F;2    00:00:00 grep --color&#x3D;auto containerd</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# ctr tasks list</span><br><span class="line">TASK          PID      STATUS    </span><br><span class="line">hello-kata    63129    RUNNING</span><br><span class="line">hello-runc    63241    RUNNING</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# ctr tasks exec -t --exec-id 63129 hello-kata sh</span><br><span class="line">&#x2F; # uname -a</span><br><span class="line">Linux clr-84d1dc4d9ae34cad92be6e6629a4f67a 5.4.60-52.container #1 SMP Sat Jan 16 05:49:34 UTC 2021 x86_64 GNU&#x2F;Linux</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# ctr tasks exec -t --exec-id 63241 hello-runc sh</span><br><span class="line">&#x2F; # uname -a</span><br><span class="line">Linux ubuntu-001 5.4.0-65-generic #73~18.04.1-Ubuntu SMP Tue Jan 19 09:02:24 UTC 2021 x86_64 GNU&#x2F;Linux</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# uname -a</span><br><span class="line">Linux ubuntu-001 5.4.0-65-generic #73~18.04.1-Ubuntu SMP Tue Jan 19 09:02:24 UTC 2021 x86_64 x86_64 x86_64 GNU&#x2F;Linux</span><br><span class="line"></span><br><span class="line">停止容器</span><br><span class="line">root@ubuntu-001:~# ctr tasks list</span><br><span class="line">TASK          PID      STATUS    </span><br><span class="line">hello-kata    63129    RUNNING</span><br><span class="line">hello-runc    63241    RUNNING </span><br><span class="line">root@ubuntu-001:~# </span><br><span class="line">root@ubuntu-001:~# ctr tasks kill --signal 9 hello-kata</span><br><span class="line">root@ubuntu-001:~# ctr tasks kill --signal 9 hello-runc</span><br><span class="line">root@ubuntu-001:~# ctr tasks list</span><br><span class="line">TASK          PID      STATUS    </span><br><span class="line">hello-kata    63129    STOPPED</span><br><span class="line">hello-runc    63241    STOPPED</span><br><span class="line"></span><br><span class="line">删除容器</span><br><span class="line">root@ubuntu-001:~# ctr container list</span><br><span class="line">CONTAINER     IMAGE                               RUNTIME                      </span><br><span class="line">hello-kata    docker.io&#x2F;library&#x2F;busybox:latest    io.containerd.run.kata.v2    </span><br><span class="line">hello-runc    docker.io&#x2F;library&#x2F;busybox:latest    io.containerd.runc.v2        </span><br><span class="line">root@ubuntu-001:~# ctr container rm hello-kata</span><br><span class="line">root@ubuntu-001:~# ctr container rm hello-runc</span><br><span class="line">root@ubuntu-001:~# ctr container list</span><br><span class="line">CONTAINER    IMAGE    RUNTIME    </span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>kata-containers</category>
        <category>containerd</category>
      </categories>
      <tags>
        <tag>kata-containers</tag>
        <tag>containerd</tag>
      </tags>
  </entry>
  <entry>
    <title>docker with kata-container</title>
    <url>/2021/02/07/docker-with-kata-container/</url>
    <content><![CDATA[<h1 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h1><ol>
<li>Ubuntu 18.04</li>
<li>已安装docker</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# lsb_release -a</span><br><span class="line">No LSB modules are available.</span><br><span class="line">Distributor ID:	Ubuntu</span><br><span class="line">Description:	Ubuntu 18.04.3 LTS</span><br><span class="line">Release:	18.04</span><br><span class="line">Codename:	bionic</span><br><span class="line">root@ubuntu-001:~# </span><br><span class="line">root@ubuntu-001:~# </span><br><span class="line">root@ubuntu-001:~# </span><br><span class="line">root@ubuntu-001:~# docker version</span><br><span class="line">Client: Docker Engine - Community</span><br><span class="line"> Version:           20.10.3</span><br><span class="line"> API version:       1.41</span><br><span class="line"> Go version:        go1.13.15</span><br><span class="line"> Git commit:        48d30b5</span><br><span class="line"> Built:             Fri Jan 29 14:33:13 2021</span><br><span class="line"> OS&#x2F;Arch:           linux&#x2F;amd64</span><br><span class="line"> Context:           default</span><br><span class="line"> Experimental:      true</span><br><span class="line"></span><br><span class="line">Server: Docker Engine - Community</span><br><span class="line"> Engine:</span><br><span class="line">  Version:          20.10.3</span><br><span class="line">  API version:      1.41 (minimum version 1.12)</span><br><span class="line">  Go version:       go1.13.15</span><br><span class="line">  Git commit:       46229ca</span><br><span class="line">  Built:            Fri Jan 29 14:31:25 2021</span><br><span class="line">  OS&#x2F;Arch:          linux&#x2F;amd64</span><br><span class="line">  Experimental:     false</span><br><span class="line"> containerd:</span><br><span class="line">  Version:          1.4.3</span><br><span class="line">  GitCommit:        269548fa27e0089a8b8278fc4fc781d7f65a939b</span><br><span class="line"> docker-init:</span><br><span class="line">  Version:          0.19.0</span><br><span class="line">  GitCommit:        de40ad0</span><br></pre></td></tr></table></figure>

<h1 id="安装Kata-Containers"><a href="#安装Kata-Containers" class="headerlink" title="安装Kata Containers"></a>安装Kata Containers</h1><p>参考：<a href="https://github.com/kata-containers/documentation/blob/master/install/README.md">https://github.com/kata-containers/documentation/blob/master/install/README.md</a></p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ ARCH=$(arch)</span><br><span class="line">$ BRANCH=<span class="string">&quot;<span class="variable">$&#123;BRANCH:-master&#125;</span>&quot;</span></span><br><span class="line">$ sudo sh -c <span class="string">&quot;echo &#x27;deb http://download.opensuse.org/repositories/home:/katacontainers:/releases:/<span class="variable">$&#123;ARCH&#125;</span>:/<span class="variable">$&#123;BRANCH&#125;</span>/xUbuntu_<span class="subst">$(lsb_release -rs)</span>/ /&#x27; &gt; /etc/apt/sources.list.d/kata-containers.list&quot;</span></span><br><span class="line">$ curl -sL  http://download.opensuse.org/repositories/home:/katacontainers:/releases:/<span class="variable">$&#123;ARCH&#125;</span>:/<span class="variable">$&#123;BRANCH&#125;</span>/xUbuntu_$(lsb_release -rs)/Release.key | sudo apt-key add -</span><br><span class="line">$ sudo -E apt-get update</span><br><span class="line">$ sudo -E apt-get -y install kata-runtime kata-proxy kata-shim</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# kata-runtime version</span><br><span class="line">kata-runtime  : 1.13.0-alpha0</span><br><span class="line">commit   : &lt;&lt;unknown&gt;&gt;</span><br><span class="line">OCI specs: 1.0.1-dev</span><br></pre></td></tr></table></figure>

<h1 id="配置docker使用Kata-Containers-两种方式二选一"><a href="#配置docker使用Kata-Containers-两种方式二选一" class="headerlink" title="配置docker使用Kata Containers(两种方式二选一)"></a>配置docker使用Kata Containers(两种方式二选一)</h1><ol>
<li>systemd (this is the default and is applied automatically if you select the <a href="https://github.com/kata-containers/documentation/tree/master/install#automatic-installation">automatic installation</a> option) <figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo mkdir -p /etc/systemd/system/docker.service.d/</span><br><span class="line">$ cat &lt;&lt;<span class="string">EOF | sudo tee /etc/systemd/system/docker.service.d/kata-containers.conf</span></span><br><span class="line"><span class="string">[Service]</span></span><br><span class="line"><span class="string">ExecStart=</span></span><br><span class="line"><span class="string">ExecStart=/usr/bin/dockerd -D --add-runtime kata-runtime=/usr/bin/kata-runtime --default-runtime=kata-runtime</span></span><br><span class="line"><span class="string">EOF</span></span><br></pre></td></tr></table></figure></li>
<li>配置/etc/docker/daemon.json文件<br> Create docker configuration folder. <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo mkdir -p &#x2F;etc&#x2F;docker</span><br></pre></td></tr></table></figure>
 Add the following definitions to <code>/etc/docker/daemon.json</code>: <figure class="highlight json"><table><tr><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">&quot;default-runtime&quot;</span>: <span class="string">&quot;kata-runtime&quot;</span>,</span><br><span class="line">    <span class="attr">&quot;runtimes&quot;</span>: &#123;</span><br><span class="line">        <span class="attr">&quot;kata-runtime&quot;</span>: &#123;</span><br><span class="line">            <span class="attr">&quot;path&quot;</span>: <span class="string">&quot;/usr/bin/kata-runtime&quot;</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

</li>
</ol>
<h1 id="重启docker服务"><a href="#重启docker服务" class="headerlink" title="重启docker服务"></a>重启docker服务</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ sudo systemctl daemon-reload</span><br><span class="line">$ sudo systemctl restart docker</span><br></pre></td></tr></table></figure>

<h1 id="运行-Kata-Containers"><a href="#运行-Kata-Containers" class="headerlink" title="运行 Kata Containers"></a>运行 Kata Containers</h1><p>You are now ready to run Kata Containers:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">$ sudo docker run -itd --name busybox1 busybox</span><br></pre></td></tr></table></figure>

<h1 id="查询使用kata-container创建的容器"><a href="#查询使用kata-container创建的容器" class="headerlink" title="查询使用kata container创建的容器"></a>查询使用kata container创建的容器</h1><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~<span class="comment"># kata-runtime list</span></span><br><span class="line">ID                                                                 PID         STATUS      BUNDLE                                                                                                                CREATED                          OWNER</span><br><span class="line">5dcc8dc243fd658317ce6ecc5163d9b936044123c4e0578cbd54567a953531a7   3965        running     /run/containerd/io.containerd.runtime.v2.task/moby/5dcc8dc243fd658317ce6ecc5163d9b936044123c4e0578cbd54567a953531a7   2021-02-07T02:44:50.342490835Z   <span class="comment">#0</span></span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~<span class="comment"># ps -ef|grep 3965</span></span><br><span class="line">root       3965   3892  0 10:44 ?        00:00:00 /usr/libexec/kata-containers/kata-shim -agent unix:///run/vc/sbs/5dcc8dc243fd658317ce6ecc5163d9b936044123c4e0578cbd54567a953531a7/proxy.sock -container 5dcc8dc243fd658317ce6ecc5163d9b936044123c4e0578cbd54567a953531a7 -exec-id 5dcc8dc243fd658317ce6ecc5163d9b936044123c4e0578cbd54567a953531a7 -terminal</span><br></pre></td></tr></table></figure>

<h1 id="比较Kata-Containers-与-runc-创建的容器不同"><a href="#比较Kata-Containers-与-runc-创建的容器不同" class="headerlink" title="比较Kata Containers 与 runc 创建的容器不同"></a>比较Kata Containers 与 runc 创建的容器不同</h1><p>默认安装docker，默认运行时是runc，上述配置操作后将默认运行时修改为kata-runtime</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># docker info|grep &quot;Runtime&quot;</span></span><br><span class="line">Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux runc</span><br><span class="line">Default Runtime: runc</span><br></pre></td></tr></table></figure>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="comment"># docker info|grep &quot;Runtime&quot;</span></span><br><span class="line">Runtimes: io.containerd.runc.v2 io.containerd.runtime.v1.linux kata-runtime runc</span><br><span class="line">Default Runtime: kata-runtime</span><br></pre></td></tr></table></figure>
<p>使用runc创建容器：docker run -itd –runtime runc –name busybox2 busybox</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~<span class="comment"># docker exec -it busybox1 sh</span></span><br><span class="line">/ <span class="comment"># uname -a</span></span><br><span class="line">Linux 5dcc8dc243fd 5.4.60-52.container <span class="comment">#1 SMP Sat Jan 16 05:49:34 UTC 2021 x86_64 GNU/Linux</span></span><br><span class="line">/ <span class="comment"># </span></span><br><span class="line">root@ubuntu-001:~<span class="comment"># docker exec -it busybox2 sh</span></span><br><span class="line">/ <span class="comment"># uname -a</span></span><br><span class="line">Linux d1e322ad99f9 5.4.0-65-generic <span class="comment">#73~18.04.1-Ubuntu SMP Tue Jan 19 09:02:24 UTC 2021 x86_64 GNU/Linux</span></span><br><span class="line">/ <span class="comment"># </span></span><br><span class="line">root@ubuntu-001:~<span class="comment"># uname -a</span></span><br><span class="line">Linux ubuntu-001 5.4.0-65-generic <span class="comment">#73~18.04.1-Ubuntu SMP Tue Jan 19 09:02:24 UTC 2021 x86_64 x86_64 x86_64 GNU/Linux</span></span><br></pre></td></tr></table></figure>
<p>可以看到runc创建的容器，和宿主机Kernel相同，而kata创建的容器使用的是自己kernel</p>
]]></content>
      <categories>
        <category>kata-containers</category>
        <category>docker</category>
      </categories>
      <tags>
        <tag>kata-containers</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>go 数据库操作</title>
    <url>/2021/02/24/go-%E6%95%B0%E6%8D%AE%E5%BA%93%E6%93%8D%E4%BD%9C/</url>
    <content><![CDATA[<h1 id="准备测试数据"><a href="#准备测试数据" class="headerlink" title="准备测试数据"></a>准备测试数据</h1><p>连接MySQL</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mysql -uroot -proot@12345</span><br></pre></td></tr></table></figure>
<p>选择数据库test</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">CREATE DATABASE IF NOT EXISTS &#96;test&#96;;</span><br><span class="line">USE &#96;test&#96;;</span><br></pre></td></tr></table></figure>

<p>创建测试用的user表和order表，并插入测试数据</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#创建user表</span><br><span class="line">DROP TABLE IF EXISTS &#96;order&#96;;</span><br><span class="line">DROP TABLE IF EXISTS &#96;user&#96;;</span><br><span class="line">CREATE TABLE IF NOT EXISTS &#96;user&#96; (&#96;uid&#96; SERIAL PRIMARY KEY, &#96;name&#96; VARCHAR(20) NOT NULL, &#96;password&#96; VARCHAR(20) NOT NULL) ENGINE&#x3D;&#96;innodb&#96;, CHARACTER SET&#x3D;utf8;</span><br><span class="line">#创建order表</span><br><span class="line">CREATE TABLE IF NOT EXISTS &#96;order&#96;(&#96;oid&#96; SERIAL PRIMARY KEY, &#96;uid&#96;  BIGINT(20) UNSIGNED NOT NULL, &#96;date&#96; TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, FOREIGN KEY (&#96;uid&#96;) REFERENCES &#96;user&#96;(&#96;uid&#96;))ENGINE&#x3D;innodb,CHARACTER SET&#x3D;utf8;</span><br><span class="line">#插入测试数据</span><br><span class="line">INSERT INTO  &#96;user&#96;(&#96;name&#96;,&#96;password&#96;) VALUES(&#39;nick&#39;, &#39;nick&#39;),(&#39;jacky&#39;, &#39;jacky&#39;);</span><br><span class="line">INSERT INTO &#96;order&#96;(&#96;uid&#96;) VALUES(1),(2);</span><br></pre></td></tr></table></figure>

<details>
  <summary>详细操作</summary>
  <pre><code> 
    root@mysql-5b49484695-28spp:/# mysql -uroot -proot@12345
    mysql: [Warning] Using a password on the command line interface can be insecure.
    Welcome to the MySQL monitor.  Commands end with ; or \g.
    Your MySQL connection id is 20
    Server version: 8.0.19 MySQL Community Server - GPL

<pre><code>Copyright (c) 2000, 2020, Oracle and/or its affiliates. All rights reserved.

Oracle is a registered trademark of Oracle Corporation and/or its
affiliates. Other names may be trademarks of their respective
owners.

Type &#39;help;&#39; or &#39;\h&#39; for help. Type &#39;\c&#39; to clear the current input statement.

mysql&gt; CREATE DATABASE IF NOT EXISTS `test`
Query OK, 1 row affected (0.00 sec)

mysql&gt; USE `test`;
Database changed
mysql&gt; DROP TABLE IF EXISTS `order`;
Query OK, 0 rows affected, 1 warning (0.00 sec)
mysql&gt; DROP TABLE IF EXISTS `order`;
Query OK, 0 rows affected, 1 warning (0.00 sec)

mysql&gt; DROP TABLE IF EXISTS `user`;
Query OK, 0 rows affected, 1 warning (0.00 sec)

mysql&gt; CREATE TABLE IF NOT EXISTS `order`(`oid` SERIAL PRIMARY KEY, `uid`  BIGINT(20) UNSIGNED NOT NULL, `date` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, FOREIGN KEY (`uid`) REFERENCES `user`(`uid`))ENGINE=innodb,CHARACTER SET=utf8;
ERROR 1824 (HY000): Failed to open the referenced table &#39;user&#39;
mysql&gt; CREATE TABLE IF NOT EXISTS `user` (`uid` SERIAL PRIMARY KEY, `name` VARCHAR(20) NOT NULL, `password` VARCHAR(20) NOT NULL) ENGINE=`innodb`, CHARACTER SET=utf8;
Query OK, 0 rows affected, 1 warning (0.01 sec)

mysql&gt; CREATE TABLE IF NOT EXISTS `order`(`oid` SERIAL PRIMARY KEY, `uid`  BIGINT(20) UNSIGNED NOT NULL, `date` TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP, FOREIGN KEY (`uid`) REFERENCES `user`(`uid`))ENGINE=innodb,CHARACTER SET=utf8;
Query OK, 0 rows affected, 2 warnings (0.01 sec)

mysql&gt; INSERT INTO  `user`(`name`,`password`) VALUES(&#39;nick&#39;, &#39;nick&#39;),(&#39;jacky&#39;, &#39;jacky&#39;);
Query OK, 2 rows affected (0.00 sec)
Records: 2  Duplicates: 0  Warnings: 0

mysql&gt; INSERT INTO `order`(`uid`) VALUES(1),(2);
Query OK, 2 rows affected (0.01 sec)
Records: 2  Duplicates: 0  Warnings: 0</code></pre>
<p>  </code></pre></p>
</details>

<h1 id="连接数据库"><a href="#连接数据库" class="headerlink" title="连接数据库"></a>连接数据库</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;database&#x2F;sql&quot;</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">    _ &quot;github.com&#x2F;go-sql-driver&#x2F;mysql&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">    &#x2F;&#x2F; 打开连接</span><br><span class="line">    db, err:&#x3D; sql.Open(&quot;mysql&quot;, &quot;root:root@12345@tcp(10.0.105.121:30591)&#x2F;test?charset&#x3D;utf8&quot;)</span><br><span class="line">    &#x2F;&#x2F; 打开连接  方法是 sql.Open 第一个参数是 数据库类型. 第二个是 用户名:密码@网络协议(ip:port)&#x2F;需要查询的数据库名</span><br><span class="line">    if err !&#x3D; nil &#123;</span><br><span class="line">        fmt.Println(&quot;failed to open database:&quot;, err.Error())</span><br><span class="line">        panic(err.Error())</span><br><span class="line">        return</span><br><span class="line">    &#125;</span><br><span class="line">    defer db.Close()</span><br><span class="line"></span><br><span class="line">    &#x2F;&#x2F;尝试连接数据库</span><br><span class="line">    err &#x3D; db.Ping()</span><br><span class="line">    if err !&#x3D; nil &#123;</span><br><span class="line">        fmt.Println(&quot;connect mysql failed：&quot;, err.Error())</span><br><span class="line">        panic(err.Error())</span><br><span class="line">        return</span><br><span class="line">    &#125;</span><br><span class="line">    fmt.Println(&quot;connect mysql success&quot;)</span><br></pre></td></tr></table></figure>

<p>返回的DB对象,实际封装了一个数据库连接池,对于goroutine是线程安全的，可以放心使用。这个数据库连接池由”database/sql”包负责自动创建和回收。连接池的大小可以由SetMaxIdleConns指定。<br>需要注意的是，创建DB对象成功，并不代表已经成功的连接了数据库，数据库连接只有在真正需要的时候才会被创建。因此如果，在创建DB对象后想验证数据库连接是否有效，可以调用Ping()或者通过</p>
<h1 id="关闭数据库"><a href="#关闭数据库" class="headerlink" title="关闭数据库"></a>关闭数据库</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">defer db.Close()</span><br></pre></td></tr></table></figure>
<p>关闭数据库并释放全部已打开的资源。实际上，很少需要进行关闭操作，DB对象实际上长期存活并在所有的goroutine之间共享</p>
<h1 id="CRUD"><a href="#CRUD" class="headerlink" title="CRUD"></a>CRUD</h1><p>DB中执行SQL通过Exec和Query方法，查询操作是通过Query完成，它会返回一个sql.Rows的结果集，包含一个游标用来遍历查询结果；Exec方法返回的是sql.Result对象，用于检测操作结果，及被影响记录数</p>
<h2 id="查询"><a href="#查询" class="headerlink" title="查询"></a>查询</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 获取USERS表中的记录</span><br><span class="line">rows, err :&#x3D; db.Query(&quot;SELECT * FROM user&quot;)</span><br><span class="line">if err !&#x3D; nil &#123;</span><br><span class="line">    fmt.Println(&quot;fetech data failed:&quot;, err.Error())</span><br><span class="line">    return</span><br><span class="line">&#125;</span><br><span class="line">defer rows.Close()</span><br><span class="line">for rows.Next() &#123;</span><br><span class="line">    var uid int</span><br><span class="line">    var name, password string</span><br><span class="line">    rows.Scan(&amp;uid, &amp;name, &amp;password)</span><br><span class="line">    fmt.Println(&quot;uid:&quot;, uid, &quot;name:&quot;, name, &quot;password:&quot;, password)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="插入"><a href="#插入" class="headerlink" title="插入"></a>插入</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 插入一条新数据</span><br><span class="line">result, err :&#x3D; db.Exec(&quot;INSERT INTO &#96;user&#96;(&#96;name&#96;,&#96;password&#96;) VALUES(&#39;tom&#39;, &#39;tom&#39;)&quot;)</span><br><span class="line">if err !&#x3D; nil &#123;</span><br><span class="line">    fmt.Println(&quot;insert data failed:&quot;, err.Error())</span><br><span class="line">    return</span><br><span class="line">&#125;</span><br><span class="line">id, err :&#x3D; result.LastInsertId()</span><br><span class="line">if err !&#x3D; nil &#123;</span><br><span class="line">    fmt.Println(&quot;fetch last insert id failed:&quot;, err.Error())</span><br><span class="line">    return</span><br><span class="line">&#125;</span><br><span class="line">fmt.Println(&quot;insert new record&quot;, id)</span><br></pre></td></tr></table></figure>

<h2 id="更新"><a href="#更新" class="headerlink" title="更新"></a>更新</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 更新一条数据</span><br><span class="line">result, err &#x3D; db.Exec(&quot;UPDATE &#96;user&#96; SET &#96;password&#96;&#x3D;? WHERE &#96;name&#96;&#x3D;?&quot;, &quot;tom_new_password&quot;, &quot;tom&quot;)</span><br><span class="line">if err !&#x3D; nil &#123;</span><br><span class="line">    fmt.Println(&quot;update data failed:&quot;, err.Error())</span><br><span class="line">    return</span><br><span class="line">&#125;</span><br><span class="line">num, err :&#x3D; result.RowsAffected()</span><br><span class="line">if err !&#x3D; nil &#123;</span><br><span class="line">    fmt.Println(&quot;fetch row affected failed:&quot;, err.Error())</span><br><span class="line">    return</span><br><span class="line">&#125;</span><br><span class="line">fmt.Println(&quot;update recors number&quot;, num)</span><br></pre></td></tr></table></figure>

<h2 id="删除"><a href="#删除" class="headerlink" title="删除"></a>删除</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;&#x2F; 删除数据</span><br><span class="line">result, err &#x3D; db.Exec(&quot;DELETE FROM &#96;user&#96; WHERE &#96;name&#96;&#x3D;?&quot;, &quot;tom&quot;)</span><br><span class="line">if err !&#x3D; nil &#123;</span><br><span class="line">    fmt.Println(&quot;delete data failed:&quot;, err.Error())</span><br><span class="line">    return</span><br><span class="line">&#125;</span><br><span class="line">num, err &#x3D; result.RowsAffected()</span><br><span class="line">if err !&#x3D; nil &#123;</span><br><span class="line">    fmt.Println(&quot;fetch row affected failed:&quot;, err.Error())</span><br><span class="line">    return</span><br><span class="line">&#125;</span><br><span class="line">fmt.Println(&quot;delete record number&quot;, num)</span><br></pre></td></tr></table></figure>

<details>
  <summary>完整代码及运行结果</summary>
  <pre><code> 
    完整代码
    package main

<pre><code>import (
    &quot;database/sql&quot;
    &quot;fmt&quot;
    _ &quot;github.com/go-sql-driver/mysql&quot;
)

func main() &#123;
    // 打开连接
    db, err:= sql.Open(&quot;mysql&quot;, &quot;root:root@12345@tcp(10.0.105.121:30591)/test?charset=utf8&quot;)
    // 打开连接  方法是 sql.Open 第一个参数是 数据库类型. 第二个是 用户名:密码@网络协议(ip:port)/需要查询的数据库名
    if err != nil &#123;
        fmt.Println(&quot;failed to open database:&quot;, err.Error())
        panic(err.Error())
        return
    &#125;
    defer db.Close()

    // 尝试连接数据库
    err = db.Ping()
    if err != nil &#123;
        fmt.Println(&quot;connect mysql failed：&quot;, err.Error())
        panic(err.Error())
        return
    &#125;
    fmt.Println(&quot;connect mysql success&quot;)

    // 获取USERS表中的记录
    rows, err := db.Query(&quot;SELECT * FROM user&quot;)
    if err != nil &#123;
        fmt.Println(&quot;fetech data failed:&quot;, err.Error())
        panic(err.Error())
        return
    &#125;
    defer rows.Close()
    for rows.Next() &#123;
        var uid int
        var name, password string
        rows.Scan(&amp;uid, &amp;name, &amp;password)
        fmt.Println(&quot;uid:&quot;, uid, &quot;name:&quot;, name, &quot;password:&quot;, password)
    &#125;

    // 插入一条新数据
    result, err := db.Exec(&quot;INSERT INTO `user`(`name`,`password`) VALUES(&#39;tom&#39;, &#39;tom&#39;)&quot;)
    if err != nil &#123;
        fmt.Println(&quot;insert data failed:&quot;, err.Error())
        panic(err.Error())
        return
    &#125;
    id, err := result.LastInsertId()
    if err != nil &#123;
        fmt.Println(&quot;fetch last insert id failed:&quot;, err.Error())
        panic(err.Error())
        return
    &#125;
    fmt.Println(&quot;insert new record&quot;, id)

    // 更新一条数据
    result, err = db.Exec(&quot;UPDATE `user` SET `password`=? WHERE `name`=?&quot;, &quot;tom_new_password&quot;, &quot;tom&quot;)
    if err != nil &#123;
        fmt.Println(&quot;update data failed:&quot;, err.Error())
        panic(err.Error())
        return
    &#125;
    num, err := result.RowsAffected()
    if err != nil &#123;
        fmt.Println(&quot;fetch row affected failed:&quot;, err.Error())
        panic(err.Error())
        return
    &#125;
    fmt.Println(&quot;update record number&quot;, num)

    // 删除数据
    result, err = db.Exec(&quot;DELETE FROM `user` WHERE `name`=?&quot;, &quot;tom&quot;)
    if err != nil &#123;
        fmt.Println(&quot;delete data failed:&quot;, err.Error())
        panic(err.Error())
        return
    &#125;
    num, err = result.RowsAffected()
    if err != nil &#123;
        fmt.Println(&quot;fetch row affected failed:&quot;, err.Error())
        panic(err.Error())
        return
    &#125;
    fmt.Println(&quot;delete record number&quot;, num)

    # 运行结果
    connect mysql success
    uid: 1 name: nick password: nick
    uid: 2 name: jacky password: jacky
    insert new record 3
    update record number 1
    delete record number 1
    Process finished with exit code 0</code></pre>
<p>  </code></pre></p>
</details>


<h1 id="参考"><a href="#参考" class="headerlink" title="参考"></a>参考</h1><p>1、<a href="https://studygolang.com/articles/10629">https://studygolang.com/articles/10629</a></p>
<p>2、<a href="https://tutorialedge.net/golang/golang-mysql-tutorial/">https://tutorialedge.net/golang/golang-mysql-tutorial/</a></p>
]]></content>
      <categories>
        <category>go</category>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>mysql</tag>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>go中String Int转换</title>
    <url>/2021/02/24/go%E4%B8%ADString-Int%E8%BD%AC%E6%8D%A2/</url>
    <content><![CDATA[<p>GO数值和字符串的相互转换，借助strconv包的函数来转换</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">package main</span><br><span class="line"></span><br><span class="line">import (</span><br><span class="line">    &quot;fmt&quot;</span><br><span class="line">    &quot;strconv&quot;</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">func main() &#123;</span><br><span class="line">    myAgeString :&#x3D; &quot;26&quot;</span><br><span class="line">    fmt.Printf(&quot;My Age: %s\n&quot;, myAgeString)</span><br><span class="line"></span><br><span class="line">    ageValue1, err :&#x3D; strconv.Atoi(myAgeString)</span><br><span class="line">    if err !&#x3D; nil &#123;</span><br><span class="line">        fmt.Println(err)</span><br><span class="line">    &#125;</span><br><span class="line">    fmt.Printf(&quot;Age: %d\n&quot;, ageValue1)</span><br><span class="line"></span><br><span class="line">    ageValue2 :&#x3D; strconv.Itoa(ageValue1)</span><br><span class="line">    fmt.Printf(&quot;New Age: %s\n&quot;,ageValue2)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>go中的defer关键字</title>
    <url>/2021/01/27/go%E4%B8%AD%E7%9A%84defer%E5%85%B3%E9%94%AE%E5%AD%97/</url>
    <content><![CDATA[<p>摘自：<a href="https://tiancaiamao.gitbooks.io/go-internals/content/zh/03.4.html">https://tiancaiamao.gitbooks.io/go-internals/content/zh/03.4.html</a></p>
<p>defer和go一样都是Go语言提供的关键字。defer用于资源的释放，会在函数返回之前进行调用。一般采用如下模式：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">f,err :&#x3D; os.Open(filename)</span><br><span class="line">if err !&#x3D; nil &#123;</span><br><span class="line">    panic(err)</span><br><span class="line">&#125;</span><br><span class="line">defer f.Close()</span><br></pre></td></tr></table></figure>

<p>如果有多个defer表达式，调用顺序类似于栈，越后面的defer表达式越先被调用。</p>
<p>不过如果对defer的了解不够深入，使用起来可能会踩到一些坑，尤其是跟带命名的返回参数一起使用时。在讲解defer的实现之前先看一看使用defer容易遇到的问题。</p>
<h1 id="defer使用时的坑"><a href="#defer使用时的坑" class="headerlink" title="defer使用时的坑"></a>defer使用时的坑</h1><p>先来看看几个例子。<br>例1：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func f() (result int) &#123;</span><br><span class="line">    defer func() &#123;</span><br><span class="line">        result++</span><br><span class="line">    &#125;()</span><br><span class="line">    return 0</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>例2：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func f() (r int) &#123;</span><br><span class="line">     t :&#x3D; 5</span><br><span class="line">     defer func() &#123;</span><br><span class="line">       t &#x3D; t + 5</span><br><span class="line">     &#125;()</span><br><span class="line">     return t</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>例3：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func f() (r int) &#123;</span><br><span class="line">    defer func(r int) &#123;</span><br><span class="line">          r &#x3D; r + 5</span><br><span class="line">    &#125;(r)</span><br><span class="line">    return 1</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>请读者先不要运行代码，在心里跑一遍结果，然后去验证。</p>
<p><strong>例1的正确答案不是0，例2的正确答案不是10，如果例3的正确答案不是6……</strong></p>
<p>defer是在return之前执行的。这个在 官方文档中是明确说明了的。要使用defer时不踩坑，最重要的一点就是要明白，<strong>return xxx这一条语句并不是一条原子指令!</strong></p>
<p>函数返回的过程是这样的：<strong>先给返回值赋值，然后调用defer表达式，最后才是返回到调用函数中</strong>。</p>
<p>defer表达式可能会在设置函数返回值之后，在返回到调用函数之前，修改返回值，使最终的函数返回值与你想象的不一致。</p>
<p>其实使用defer时，用一个简单的转换规则改写一下，就不会迷糊了。改写规则是将return语句拆成两句写，return xxx会被改写成:</p>
<p><strong>返回值 = xxx</strong><br><strong>调用defer函数</strong><br><strong>空的return</strong></p>
<p>先看例1，它可以改写成这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func f() (result int) &#123;</span><br><span class="line">     result &#x3D; 0  &#x2F;&#x2F;return语句不是一条原子调用，return xxx其实是赋值＋ret指令</span><br><span class="line">     func() &#123; &#x2F;&#x2F;defer被插入到return之前执行，也就是赋返回值和ret指令之间</span><br><span class="line">         result++</span><br><span class="line">     &#125;()</span><br><span class="line">     return</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以这个返回值是1。</p>
<p>再看例2，它可以改写成这样：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func f() (r int) &#123;</span><br><span class="line">     t :&#x3D; 5</span><br><span class="line">     r &#x3D; t &#x2F;&#x2F;赋值指令</span><br><span class="line">     func() &#123;        &#x2F;&#x2F;defer被插入到赋值与返回之间执行，这个例子中返回值r没被修改过</span><br><span class="line">         t &#x3D; t + 5</span><br><span class="line">     &#125;</span><br><span class="line">     return        &#x2F;&#x2F;空的return指令</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以这个的结果是5。</p>
<p>最后看例3，它改写后变成：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">func f() (r int) &#123;</span><br><span class="line">     r &#x3D; 1  &#x2F;&#x2F;给返回值赋值</span><br><span class="line">     func(r int) &#123;        &#x2F;&#x2F;这里改的r是传值传进去的r，不会改变要返回的那个r值</span><br><span class="line">          r &#x3D; r + 5</span><br><span class="line">     &#125;(r)</span><br><span class="line">     return        &#x2F;&#x2F;空的return</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>所以这个例子的结果是1。</p>
<p><strong>总结：defer确实是在return之前调用的。但表现形式上却可能不像。本质原因是return xxx语句并不是一条原子指令，defer被插入到了赋值 与 ret之间，因此可能有机会改变最终的返回值。</strong></p>
]]></content>
      <categories>
        <category>go</category>
      </categories>
      <tags>
        <tag>go</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes的python-client体验</title>
    <url>/2020/12/01/kubernetes%E7%9A%84python-client%E4%BD%93%E9%AA%8C/</url>
    <content><![CDATA[<p>kubernetes提供了丰富的API接口，用户不仅可以通过CLI命令行去调用API接口，而且还可以通过client库方便的调用API接口</p>
<p><a href="https://kubernetes.io/docs/reference/using-api/client-libraries/%EF%BC%8C%E6%8F%90%E4%BE%9B%E4%BA%86%E5%AE%98%E6%96%B9%E6%94%AF%E6%8C%81%E7%9A%84%E5%92%8C%E7%A4%BE%E5%8C%BA%E6%94%AF%E6%8C%81%E7%9A%84%E4%B8%8D%E5%90%8C%E8%AF%AD%E8%A8%80%E7%9A%84client%E5%BA%93">https://kubernetes.io/docs/reference/using-api/client-libraries/，提供了官方支持的和社区支持的不同语言的client库</a></p>
<p>本文将着重体验python版本的client库的使用</p>
<h2 id="源码安装"><a href="#源码安装" class="headerlink" title="源码安装"></a>源码安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">git clone --recursive https:&#x2F;&#x2F;github.com&#x2F;kubernetes-client&#x2F;python.git</span><br><span class="line">cd python</span><br><span class="line">python setup.py install</span><br></pre></td></tr></table></figure>

<h2 id="pip安装"><a href="#pip安装" class="headerlink" title="pip安装"></a>pip安装</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip install kubernetes</span><br></pre></td></tr></table></figure>

<h2 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from kubernetes import client, config</span><br><span class="line"></span><br><span class="line"># Configs can be set in Configuration class directly or using helper utility</span><br><span class="line">config.load_kube_config()</span><br><span class="line"></span><br><span class="line">v1 &#x3D; client.CoreV1Api()</span><br><span class="line">print(&quot;Listing pods with their IPs:&quot;)</span><br><span class="line">ret &#x3D; v1.list_pod_for_all_namespaces(watch&#x3D;False)</span><br><span class="line">for i in ret.items:</span><br><span class="line">    print(&quot;%s\t%s\t%s&quot; % (i.status.pod_ip, i.metadata.namespace, i.metadata.name))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~&#x2F;python# python test_list_pods.py</span><br><span class="line">Listing pods with their IPs:</span><br><span class="line">10.244.0.8      default edgex-app-service-configurable-rules-d66d779c7-ksw7c</span><br><span class="line">10.244.0.9      default edgex-core-command-6f7cd7d57f-tnmmp</span><br><span class="line">10.244.0.18     default edgex-core-consul-64b88766-7q7kr</span><br><span class="line">10.244.0.10     default edgex-core-data-57b99d7f89-gpfgs</span><br><span class="line">10.244.0.11     default edgex-core-metadata-58dcc95ff4-9ssjg</span><br><span class="line">10.244.0.12     default edgex-device-rest-7944449548-nzcfj</span><br><span class="line">10.244.0.13     default edgex-device-virtual-6c7b8d7499-csz85</span><br><span class="line">10.244.0.14     default edgex-redis-67fffb7666-pvqgw</span><br><span class="line">10.244.0.15     default edgex-support-notifications-58dc7f76b4-tsdxn</span><br><span class="line">10.244.0.16     default edgex-support-rulesengine-8657b7c988-zr8pv</span><br><span class="line">10.244.0.17     default edgex-support-scheduler-d6fd467db-tfqw7</span><br><span class="line">10.244.0.19     default edgex-sys-mgmt-agent-79b476d8dc-2jdw4</span><br><span class="line">10.244.0.20     default edgex-ui-8cfc7f95d-vvhzc</span><br><span class="line">10.244.0.4      kube-system     coredns-f9fd979d6-cqpss</span><br><span class="line">10.244.0.3      kube-system     coredns-f9fd979d6-kvbzm</span><br><span class="line">172.18.0.2      kube-system     etcd-kind-control-plane</span><br><span class="line">172.18.0.2      kube-system     kindnet-br2qc</span><br><span class="line">172.18.0.2      kube-system     kube-apiserver-kind-control-plane</span><br><span class="line">172.18.0.2      kube-system     kube-controller-manager-kind-control-plane</span><br><span class="line">172.18.0.2      kube-system     kube-proxy-4chlm</span><br><span class="line">172.18.0.2      kube-system     kube-scheduler-kind-control-plane</span><br><span class="line">10.244.0.7      kube-system     metrics-server-8dc97c749-h6w8n</span><br><span class="line">10.244.0.6      kubernetes-dashboard    dashboard-metrics-scraper-7b59f7d4df-5rwc6</span><br><span class="line">10.244.0.5      kubernetes-dashboard    kubernetes-dashboard-665f4c5ff-l6tqn</span><br><span class="line">10.244.0.2      local-path-storage      local-path-provisioner-78776bfc44-xl4ht</span><br><span class="line">root@ubuntu:~&#x2F;python#</span><br><span class="line">root@ubuntu:~&#x2F;python# kubectl get pods --all-namespaces -o wide</span><br><span class="line">NAMESPACE              NAME                                                   READY   STATUS             RESTARTS   AGE    IP            NODE                 NOMINATED NODE   READINESS GATES</span><br><span class="line">default                edgex-app-service-configurable-rules-d66d779c7-ksw7c   1&#x2F;1     Running            14         21h    10.244.0.8    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-core-command-6f7cd7d57f-tnmmp                    1&#x2F;1     Running            21         21h    10.244.0.9    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-core-consul-64b88766-7q7kr                       1&#x2F;1     Running            0          21h    10.244.0.18   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-core-data-57b99d7f89-gpfgs                       1&#x2F;1     Running            19         21h    10.244.0.10   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-core-metadata-58dcc95ff4-9ssjg                   1&#x2F;1     Running            21         21h    10.244.0.11   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-device-rest-7944449548-nzcfj                     1&#x2F;1     Running            15         21h    10.244.0.12   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-device-virtual-6c7b8d7499-csz85                  1&#x2F;1     Running            13         21h    10.244.0.13   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-redis-67fffb7666-pvqgw                           1&#x2F;1     Running            0          21h    10.244.0.14   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-support-notifications-58dc7f76b4-tsdxn           1&#x2F;1     Running            13         21h    10.244.0.15   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-support-rulesengine-8657b7c988-zr8pv             1&#x2F;1     Running            0          21h    10.244.0.16   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-support-scheduler-d6fd467db-tfqw7                1&#x2F;1     Running            14         21h    10.244.0.17   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-sys-mgmt-agent-79b476d8dc-2jdw4                  1&#x2F;1     Running            0          21h    10.244.0.19   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">default                edgex-ui-8cfc7f95d-vvhzc                               0&#x2F;1     ImagePullBackOff   0          21h    10.244.0.20   kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            coredns-f9fd979d6-cqpss                                1&#x2F;1     Running            0          4d6h   10.244.0.4    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            coredns-f9fd979d6-kvbzm                                1&#x2F;1     Running            0          4d6h   10.244.0.3    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            etcd-kind-control-plane                                1&#x2F;1     Running            0          4d6h   172.18.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            kindnet-br2qc                                          1&#x2F;1     Running            0          4d6h   172.18.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            kube-apiserver-kind-control-plane                      1&#x2F;1     Running            0          23h    172.18.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            kube-controller-manager-kind-control-plane             1&#x2F;1     Running            1          4d6h   172.18.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            kube-proxy-4chlm                                       1&#x2F;1     Running            0          4d6h   172.18.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            kube-scheduler-kind-control-plane                      1&#x2F;1     Running            1          4d6h   172.18.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system            metrics-server-8dc97c749-h6w8n                         1&#x2F;1     Running            0          21h    10.244.0.7    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard   dashboard-metrics-scraper-7b59f7d4df-5rwc6             1&#x2F;1     Running            0          21h    10.244.0.6    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kubernetes-dashboard   kubernetes-dashboard-665f4c5ff-l6tqn                   1&#x2F;1     Running            0          21h    10.244.0.5    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">local-path-storage     local-path-provisioner-78776bfc44-xl4ht                1&#x2F;1     Running            1          4d6h   10.244.0.2    kind-control-plane   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>更多的examples：<a href="https://github.com/kubernetes-client/python/tree/master/example">https://github.com/kubernetes-client/python/tree/master/example</a></p>
]]></content>
      <categories>
        <category>python</category>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>python</tag>
      </tags>
  </entry>
  <entry>
    <title>如何使用github作为Helm的chart仓库</title>
    <url>/2020/12/01/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8github%E4%BD%9C%E4%B8%BAHelm%E7%9A%84chart%E4%BB%93%E5%BA%93/</url>
    <content><![CDATA[<h1 id="前提条件："><a href="#前提条件：" class="headerlink" title="前提条件："></a>前提条件：</h1><p>1、已安装git<br>2、已注册github账户<br>3、安装helm（推荐helm3，下载地址：[<a href="https://github.com/helm/helm/releases/(https://github.com/helm/helm/releases/)%EF%BC%89">https://github.com/helm/helm/releases/(https://github.com/helm/helm/releases/)）</a></p>
<h1 id="操作步骤："><a href="#操作步骤：" class="headerlink" title="操作步骤："></a>操作步骤：</h1><h2 id="1、在github创建仓库，取名为helm-chart"><a href="#1、在github创建仓库，取名为helm-chart" class="headerlink" title="1、在github创建仓库，取名为helm-chart"></a>1、在github创建仓库，取名为helm-chart</h2><p><img src="https://upload-images.jianshu.io/upload_images/10839544-8d2d6093ca2aff4c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="创建仓库"></p>
<h2 id="2、下载helm-chart仓库到本地"><a href="#2、下载helm-chart仓库到本地" class="headerlink" title="2、下载helm-chart仓库到本地"></a>2、下载helm-chart仓库到本地</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zj@zj-Z390-UD:~&#x2F;code$ git clone https:&#x2F;&#x2F;github.com&#x2F;yushanjin&#x2F;helm-chart.git</span><br><span class="line">正克隆到 &#39;helm-chart&#39;...</span><br><span class="line">remote: Enumerating objects: 3, done.</span><br><span class="line">remote: Counting objects: 100% (3&#x2F;3), done.</span><br><span class="line">remote: Compressing objects: 100% (2&#x2F;2), done.</span><br><span class="line">remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0</span><br><span class="line">展开对象中: 100% (3&#x2F;3), 完成.</span><br></pre></td></tr></table></figure>
<h2 id="3、创建chart目录"><a href="#3、创建chart目录" class="headerlink" title="3、创建chart目录"></a>3、创建chart目录</h2> <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zj@zj-Z390-UD:~&#x2F;code$ cd helm-chart&#x2F;</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ ll</span><br><span class="line">总用量 16</span><br><span class="line">drwxr-xr-x 3 zj zj 4096 4月   8 14:04 .&#x2F;</span><br><span class="line">drwxr-xr-x 4 zj zj 4096 4月   8 14:04 ..&#x2F;</span><br><span class="line">drwxr-xr-x 8 zj zj 4096 4月   8 14:04 .git&#x2F;</span><br><span class="line">-rw-r--r-- 1 zj zj   77 4月   8 14:04 README.md</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm create test</span><br><span class="line">Creating test</span><br><span class="line"></span><br><span class="line">注意：可以在终端执行 source &lt;(helm completion bash)，启动helm命令自动补全功能</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ tree test&#x2F;</span><br><span class="line">test&#x2F;</span><br><span class="line">├── charts</span><br><span class="line">├── Chart.yaml</span><br><span class="line">├── templates</span><br><span class="line">│   ├── deployment.yaml</span><br><span class="line">│   ├── _helpers.tpl</span><br><span class="line">│   ├── ingress.yaml</span><br><span class="line">│   ├── NOTES.txt</span><br><span class="line">│   ├── serviceaccount.yaml</span><br><span class="line">│   ├── service.yaml</span><br><span class="line">│   └── tests</span><br><span class="line">│       └── test-connection.yaml</span><br><span class="line">└── values.yaml</span><br><span class="line"></span><br><span class="line">3 directories, 9 files</span><br></pre></td></tr></table></figure>
<h2 id="4、打包chart包"><a href="#4、打包chart包" class="headerlink" title="4、打包chart包"></a>4、打包chart包</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm package test&#x2F;</span><br><span class="line">Successfully packaged chart and saved it to: &#x2F;home&#x2F;zj&#x2F;code&#x2F;helm-chart&#x2F;test-0.1.0.tgz</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm repo index --url https:&#x2F;&#x2F;yushanjin.github.io&#x2F;helm-chart&#x2F; .</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ cat index.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">entries:</span><br><span class="line">  test:</span><br><span class="line">  - apiVersion: v2</span><br><span class="line">    appVersion: 1.16.0</span><br><span class="line">    created: &quot;2020-04-08T14:19:31.079089336+08:00&quot;</span><br><span class="line">    description: A Helm chart for Kubernetes</span><br><span class="line">    digest: 6de7ab4f2da011db9ef8e1def8d2fca7d4e79bb4e81e46152a8d3a2969b73820</span><br><span class="line">    name: test</span><br><span class="line">    type: application</span><br><span class="line">    urls:</span><br><span class="line">    - https:&#x2F;&#x2F;yushanjin.github.io&#x2F;helm-chart&#x2F;test-0.1.0.tgz</span><br><span class="line">    version: 0.1.0</span><br><span class="line">generated: &quot;2020-04-08T14:19:31.078672885+08:00&quot;</span><br></pre></td></tr></table></figure>
<h2 id="5、push到github"><a href="#5、push到github" class="headerlink" title="5、push到github"></a>5、push到github</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ git status</span><br><span class="line">位于分支 master</span><br><span class="line">您的分支与上游分支 &#39;origin&#x2F;master&#39; 一致。</span><br><span class="line"></span><br><span class="line">未跟踪的文件:</span><br><span class="line">  （使用 &quot;git add &lt;文件&gt;...&quot; 以包含要提交的内容）</span><br><span class="line"></span><br><span class="line">	index.yaml</span><br><span class="line">	test-0.1.0.tgz</span><br><span class="line">	test&#x2F;</span><br><span class="line"></span><br><span class="line">提交为空，但是存在尚未跟踪的文件（使用 &quot;git add&quot; 建立跟踪）</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ git add .</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ git status</span><br><span class="line">位于分支 master</span><br><span class="line">您的分支与上游分支 &#39;origin&#x2F;master&#39; 一致。</span><br><span class="line"></span><br><span class="line">要提交的变更：</span><br><span class="line">  （使用 &quot;git reset HEAD &lt;文件&gt;...&quot; 以取消暂存）</span><br><span class="line"></span><br><span class="line">	新文件：   index.yaml</span><br><span class="line">	新文件：   test-0.1.0.tgz</span><br><span class="line">	新文件：   test&#x2F;.helmignore</span><br><span class="line">	新文件：   test&#x2F;Chart.yaml</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;NOTES.txt</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;_helpers.tpl</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;deployment.yaml</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;ingress.yaml</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;service.yaml</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;serviceaccount.yaml</span><br><span class="line">	新文件：   test&#x2F;templates&#x2F;tests&#x2F;test-connection.yaml</span><br><span class="line">	新文件：   test&#x2F;values.yaml</span><br><span class="line"></span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ git commit -m &quot;创建test的chart包&quot;</span><br><span class="line">[master 5ae813c] 创建test的chart包</span><br><span class="line"> 12 files changed, 348 insertions(+)</span><br><span class="line"> create mode 100644 index.yaml</span><br><span class="line"> create mode 100644 test-0.1.0.tgz</span><br><span class="line"> create mode 100644 test&#x2F;.helmignore</span><br><span class="line"> create mode 100644 test&#x2F;Chart.yaml</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;NOTES.txt</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;_helpers.tpl</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;deployment.yaml</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;ingress.yaml</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;service.yaml</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;serviceaccount.yaml</span><br><span class="line"> create mode 100644 test&#x2F;templates&#x2F;tests&#x2F;test-connection.yaml</span><br><span class="line"> create mode 100644 test&#x2F;values.yaml</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ git push origin master</span><br><span class="line">Username for &#39;https:&#x2F;&#x2F;github.com&#39;: yushanjin</span><br><span class="line">Password for &#39;https:&#x2F;&#x2F;yushanjin@github.com&#39;: </span><br><span class="line">对象计数中: 17, 完成.</span><br><span class="line">Delta compression using up to 8 threads.</span><br><span class="line">压缩对象中: 100% (16&#x2F;16), 完成.</span><br><span class="line">写入对象中: 100% (17&#x2F;17), 8.32 KiB | 1.66 MiB&#x2F;s, 完成.</span><br><span class="line">Total 17 (delta 0), reused 0 (delta 0)</span><br><span class="line">To https:&#x2F;&#x2F;github.com&#x2F;yushanjin&#x2F;helm-chart.git</span><br><span class="line">   48e2023..5ae813c  master -&gt; master</span><br></pre></td></tr></table></figure>
<p>注意： 我这里本地没有创建其他分支，所以直接push到master分支了</p>
<h2 id="6、设置github上helm-chart仓库的GitHub-Pages（在仓库的settings里设置）"><a href="#6、设置github上helm-chart仓库的GitHub-Pages（在仓库的settings里设置）" class="headerlink" title="6、设置github上helm-chart仓库的GitHub Pages（在仓库的settings里设置）"></a>6、设置github上helm-chart仓库的GitHub Pages（在仓库的settings里设置）</h2><p><img src="https://upload-images.jianshu.io/upload_images/10839544-fece4f06795a48d7.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"><br>登录：<a href="https://yushanjin.github.io/helm-chart%EF%BC%8C%E9%A1%B5%E9%9D%A2%E5%86%85%E5%AE%B9%E4%B8%BAREADME.md%E7%9A%84%E5%86%85%E5%AE%B9">https://yushanjin.github.io/helm-chart，页面内容为README.md的内容</a><br><img src="https://upload-images.jianshu.io/upload_images/10839544-9f84217a461cce46.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="image.png"></p>
<p><strong>注意</strong>： 我修改过README.md文件</p>
<h2 id="7、本地添加自己的chart仓库"><a href="#7、本地添加自己的chart仓库" class="headerlink" title="7、本地添加自己的chart仓库"></a>7、本地添加自己的chart仓库</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm repo list</span><br><span class="line">Error: no repositories to show</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm repo add myrepo https:&#x2F;&#x2F;yushanjin.github.io&#x2F;helm-chart</span><br><span class="line">&quot;myrepo&quot; has been added to your repositories</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm repo list</span><br><span class="line">NAME  	URL                                   </span><br><span class="line">myrepo	https:&#x2F;&#x2F;yushanjin.github.io&#x2F;helm-chart</span><br><span class="line">zj@zj-Z390-UD:~&#x2F;code&#x2F;helm-chart$ helm search repo test</span><br><span class="line">NAME       	CHART VERSION	APP VERSION	DESCRIPTION                </span><br><span class="line">myrepo&#x2F;test	0.1.0        	1.16.0     	A Helm chart for Kubernetes</span><br></pre></td></tr></table></figure>

<p>至此，就可以使用<strong>helm install xxx myrepo/test</strong> 安装test了</p>
]]></content>
      <categories>
        <category>helm</category>
      </categories>
      <tags>
        <tag>GitHub</tag>
        <tag>helm</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes部署httpd服务</title>
    <url>/2020/12/02/kubernetes%E9%83%A8%E7%BD%B2httpd%E6%9C%8D%E5%8A%A1/</url>
    <content><![CDATA[<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# cat httpd.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: httpd</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      run: httpd</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        run: httpd</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: httpd</span><br><span class="line">        image: httpd</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: httpd-svc</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    run: httpd</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    nodePort: 30000</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl apply -f httpd.yaml</span><br><span class="line">deployment.apps&#x2F;httpd created</span><br><span class="line">service&#x2F;httpd-svc created</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl get pods</span><br><span class="line">NAME                    READY   STATUS    RESTARTS   AGE</span><br><span class="line">httpd-ff8d77b9b-zz4d4   1&#x2F;1     Running   0          10s</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">httpd-svc    NodePort    10.97.219.48   &lt;none&gt;        8080:30000&#x2F;TCP   14s</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443&#x2F;TCP          69d</span><br><span class="line"></span><br><span class="line">进入pod</span><br><span class="line">root@ubuntu-001:~# kubectl exec -it httpd-ff8d77b9b-zz4d4 -- &#x2F;bin&#x2F;bash</span><br><span class="line">root@httpd-ff8d77b9b-zz4d4:&#x2F;usr&#x2F;local&#x2F;apache2#</span><br><span class="line"></span><br><span class="line">root@httpd-ff8d77b9b-zz4d4:&#x2F;usr&#x2F;local&#x2F;apache2# cat htdocs&#x2F;index.html</span><br><span class="line">&lt;html&gt;&lt;body&gt;&lt;h1&gt;It works!&lt;&#x2F;h1&gt;&lt;&#x2F;body&gt;&lt;&#x2F;html&gt;</span><br></pre></td></tr></table></figure>
<p>浏览器输入：http://主机IP:30000/</p>
<p><img src="/images/2020-12-02/1.jpg" alt="1"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# cat httpd.yaml</span><br><span class="line">apiVersion: apps&#x2F;v1</span><br><span class="line">kind: Deployment</span><br><span class="line">metadata:</span><br><span class="line">  name: httpd</span><br><span class="line">spec:</span><br><span class="line">  replicas: 1</span><br><span class="line">  selector:</span><br><span class="line">    matchLabels:</span><br><span class="line">      run: httpd</span><br><span class="line">  template:</span><br><span class="line">    metadata:</span><br><span class="line">      labels:</span><br><span class="line">        run: httpd</span><br><span class="line">    spec:</span><br><span class="line">      containers:</span><br><span class="line">      - name: httpd</span><br><span class="line">        image: httpd</span><br><span class="line">        ports:</span><br><span class="line">        - containerPort: 80</span><br><span class="line">        volumeMounts:</span><br><span class="line">        - name: host-path</span><br><span class="line">          mountPath: &#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs</span><br><span class="line">      volumes:</span><br><span class="line">      - name: host-path</span><br><span class="line">        hostPath:</span><br><span class="line">           path: &#x2F;root&#x2F;test</span><br><span class="line">---</span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Service</span><br><span class="line">metadata:</span><br><span class="line">  name: httpd-svc</span><br><span class="line">spec:</span><br><span class="line">  type: NodePort</span><br><span class="line">  selector:</span><br><span class="line">    run: httpd</span><br><span class="line">  ports:</span><br><span class="line">  - protocol: TCP</span><br><span class="line">    nodePort: 30000</span><br><span class="line">    port: 8080</span><br><span class="line">    targetPort: 80</span><br><span class="line"></span><br><span class="line">这里进一步使用hostPath将本地test目录，映射到容器的&#x2F;usr&#x2F;local&#x2F;apache2&#x2F;htdocs目录下</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl apply -f httpd.yaml</span><br><span class="line">deployment.apps&#x2F;httpd configured</span><br><span class="line">service&#x2F;httpd-svc configured</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl get pods</span><br><span class="line">NAME                     READY   STATUS    RESTARTS   AGE</span><br><span class="line">httpd-589bbcf648-28n9x   1&#x2F;1     Running   0          7m19s</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl get svc</span><br><span class="line">NAME         TYPE        CLUSTER-IP     EXTERNAL-IP   PORT(S)          AGE</span><br><span class="line">httpd-svc    NodePort    10.104.70.26   &lt;none&gt;        8080:30000&#x2F;TCP   3h52m</span><br><span class="line">kubernetes   ClusterIP   10.96.0.1      &lt;none&gt;        443&#x2F;TCP          69d</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>浏览器输入：http://主机IP:30000/</p>
<p><img src="/images/2020-12-02/2.jpg" alt="2"></p>
<p>特别说明：这里我使用了hostPath，且httpd副本我修改成了1，实际上，httpd pod可能落在其他node上，所以严格来说，hostPath难以保证所在node上test的存在</p>
<p>可以考虑使用glusterfs做统一的后端存储</p>
]]></content>
      <categories>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
      </tags>
  </entry>
  <entry>
    <title>如何配置kubernetes的pod从私有仓库拉取镜像</title>
    <url>/2020/12/01/%E5%A6%82%E4%BD%95%E9%85%8D%E7%BD%AEkubernetes%E7%9A%84pod%E4%BB%8E%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93%E6%8B%89%E5%8F%96%E9%95%9C%E5%83%8F/</url>
    <content><![CDATA[<p>在实际使用中，用户往往搭建了自己的私有镜像仓库。kubernetes用户创建pod的过程中，如何从私有镜像仓库拉取容器镜像？</p>
<p>本文重点要介绍：如何让使用secret从私有的 Docker 镜像仓库或代码仓库拉取镜像来创建 Pod</p>
<h1 id="前提条件："><a href="#前提条件：" class="headerlink" title="前提条件："></a>前提条件：</h1><ol>
<li><p>kubernetes集群</p>
</li>
<li><p>docker私有镜像仓库或者docker hub上有Docker ID（这里使用Docker ID来演示）</p>
</li>
</ol>
<p>创建secret有两种方法：</p>
<ol>
<li><p>使用config.json文件</p>
</li>
<li><p>直接使用用户名+密码</p>
</li>
</ol>
<h1 id="生成config-json文件"><a href="#生成config-json文件" class="headerlink" title="生成config.json文件"></a>生成config.json文件</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# docker login</span><br><span class="line">Login with your Docker ID to push and pull images from Docker Hub. If you don&#39;t have a Docker ID, head over to https:&#x2F;&#x2F;hub.docker.com to create one.</span><br><span class="line">Username: shayu</span><br><span class="line">Password:</span><br><span class="line">WARNING! Your password will be stored unencrypted in &#x2F;root&#x2F;.docker&#x2F;config.json.</span><br><span class="line">Configure a credential helper to remove this warning. See</span><br><span class="line">https:&#x2F;&#x2F;docs.docker.com&#x2F;engine&#x2F;reference&#x2F;commandline&#x2F;login&#x2F;#credentials-store</span><br><span class="line"></span><br><span class="line">Login Succeeded</span><br></pre></td></tr></table></figure>
<p>根据提示，输入用户名及密码。</p>
<p>用户名及密码会被存储到本地文件config.json中。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# cat &#x2F;root&#x2F;.docker&#x2F;config.json</span><br><span class="line">&#123;</span><br><span class="line">        &quot;auths&quot;: &#123;</span><br><span class="line">                &quot;https:&#x2F;&#x2F;index.docker.io&#x2F;v1&#x2F;&quot;: &#123;</span><br><span class="line">                        &quot;auth&quot;: &quot;c2hheXU6MDI4Mxxxxxxx4&#x3D;&quot;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;HttpHeaders&quot;: &#123;</span><br><span class="line">                &quot;User-Agent&quot;: &quot;Docker-Client&#x2F;19.03.13 (linux)&quot;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# echo &quot;&quot;c2hheXU6MDI4Mxxxxxxx4&#x3D;&quot; |base64 -d</span><br><span class="line">shayu:xxxxxxx</span><br></pre></td></tr></table></figure>

<h1 id="创建secret"><a href="#创建secret" class="headerlink" title="创建secret"></a>创建secret</h1><p>方法一：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kubectl create secret generic regcred --from-file&#x3D;.dockerconfigjson&#x3D;.docker&#x2F;config.json --type&#x3D;kubernetes.io&#x2F;dockerconfigjson</span><br><span class="line">secret&#x2F;regcred created</span><br><span class="line">root@ubuntu:~#</span><br><span class="line">root@ubuntu:~# kubectl get secret regcred -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  .dockerconfigjson: ewoJImF1dGhzIjogewoJCSJodHRwczovL2luZGV4LmRvY2tlci5pby92MS8iOiB7CgkWFkZXJzIjogewoJCSJVc2VyLUFnZW50IjogIkRvY2tlci1DbGllbnQvMTkuMDMuMTMgKGxpbnV4KSIKCX0KfQ&#x3D;&#x3D;</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2020-12-01T07:09:19Z&quot;</span><br><span class="line">  managedFields:</span><br><span class="line">  - apiVersion: v1</span><br><span class="line">    fieldsType: FieldsV1</span><br><span class="line">    fieldsV1:</span><br><span class="line">      f:data:</span><br><span class="line">        .: &#123;&#125;</span><br><span class="line">        f:.dockerconfigjson: &#123;&#125;</span><br><span class="line">      f:type: &#123;&#125;</span><br><span class="line">    manager: kubectl-create</span><br><span class="line">    operation: Update</span><br><span class="line">    time: &quot;2020-12-01T07:09:19Z&quot;</span><br><span class="line">  name: regcred</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;974703&quot;</span><br><span class="line">  selfLink: &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;default&#x2F;secrets&#x2F;regcred</span><br><span class="line">  uid: 96ed63a0-90d8-47e3-a1d9-aa841a9d1813</span><br><span class="line">type: kubernetes.io&#x2F;dockerconfigjson</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get secret regcred --output&#x3D;&quot;jsonpath&#x3D;&#123;.data.\.dockerconfigjson&#125;&quot; | base64 --decode</span><br><span class="line">&#123;</span><br><span class="line">        &quot;auths&quot;: &#123;</span><br><span class="line">                &quot;https:&#x2F;&#x2F;index.docker.io&#x2F;v1&#x2F;&quot;: &#123;</span><br><span class="line">                        &quot;auth&quot;: &quot;c2hheXU6MDI4Mxxxxxxxxxx4&#x3D;&quot;</span><br><span class="line">                &#125;</span><br><span class="line">        &#125;,</span><br><span class="line">        &quot;HttpHeaders&quot;: &#123;</span><br><span class="line">                &quot;User-Agent&quot;: &quot;Docker-Client&#x2F;19.03.13 (linux)&quot;</span><br><span class="line">        &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>方法二：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kubectl create secret docker-registry regcred1 --docker-username&#x3D;shayu --docker-password&#x3D;xxxxxx --docker-email&#x3D;xxxxxxx@163.com</span><br><span class="line">secret&#x2F;regcred1 created</span><br><span class="line"></span><br><span class="line">其中:</span><br><span class="line">regcred1: 指定密钥的键名称, 可自行定义</span><br><span class="line">--docker-server: 指定docker仓库地址</span><br><span class="line">--docker-username: 指定docker仓库账号</span><br><span class="line">--docker-password: 指定docker仓库密码</span><br><span class="line">--docker-email: 指定邮件地址</span><br><span class="line"></span><br><span class="line">root@ubuntu:~#kubectl get secret regcred1 -o yaml</span><br><span class="line">apiVersion: v1</span><br><span class="line">data:</span><br><span class="line">  .dockerconfigjson: eyJhdXRocyI6eyJodHRwczovL2luZGV4LmRvY2tlci5pbxxxxxxxxxxxxxFpbCI6Inl1c2hhbmppbjA3NjdAMTYzLmNvbSIsImF1dGgiOiJjMmhoZVhVNk1ESTRNVEV3ZVhOcUxDND0ifX19</span><br><span class="line">kind: Secret</span><br><span class="line">metadata:</span><br><span class="line">  creationTimestamp: &quot;2020-12-01T07:26:30Z&quot;</span><br><span class="line">  managedFields:</span><br><span class="line">  - apiVersion: v1</span><br><span class="line">    fieldsType: FieldsV1</span><br><span class="line">    fieldsV1:</span><br><span class="line">      f:data:</span><br><span class="line">        .: &#123;&#125;</span><br><span class="line">        f:.dockerconfigjson: &#123;&#125;</span><br><span class="line">      f:type: &#123;&#125;</span><br><span class="line">    manager: kubectl-create</span><br><span class="line">    operation: Update</span><br><span class="line">    time: &quot;2020-12-01T07:26:30Z&quot;</span><br><span class="line">  name: regcred1</span><br><span class="line">  namespace: default</span><br><span class="line">  resourceVersion: &quot;977436&quot;</span><br><span class="line">  selfLink: &#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;default&#x2F;secrets&#x2F;regcred1</span><br><span class="line">  uid: b552c5ac-92eb-424e-bf4b-2a2c281cb75a</span><br><span class="line">type: kubernetes.io&#x2F;dockerconfigjson</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get secret regcred1 --output&#x3D;&quot;jsonpath&#x3D;&#123;.data.\.dockerconfigjson&#125;&quot; | base64 --decode</span><br><span class="line">&#123;&quot;auths&quot;:&#123;&quot;https:&#x2F;&#x2F;index.docker.io&#x2F;v1&#x2F;&quot;:&#123;&quot;username&quot;:&quot;shayu&quot;,&quot;password&quot;:&quot;xxxxxxx&quot;,&quot;email&quot;:&quot;xxxxxxxx@163.com&quot;,&quot;auth&quot;:&quot;c2hheXU6MDI4xxxxxxxxx4&#x3D;&quot;&#125;&#125;&#125;</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<h1 id="pod指定imagePullSecrets为上述创建的secret"><a href="#pod指定imagePullSecrets为上述创建的secret" class="headerlink" title="pod指定imagePullSecrets为上述创建的secret"></a>pod指定imagePullSecrets为上述创建的secret</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: private-reg</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: private-reg-container</span><br><span class="line">    image: xxxxxxx:yyyy</span><br><span class="line">  imagePullSecrets:</span><br><span class="line">  - name: regcred</span><br></pre></td></tr></table></figure>

<p>参考：</p>
<p>(1) <a href="https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry">https://kubernetes.io/docs/tasks/configure-pod-container/pull-image-private-registry</a></p>
<p>(2) <a href="https://kubernetes.io/docs/concepts/containers/images/">https://kubernetes.io/docs/concepts/containers/images/</a></p>
]]></content>
      <categories>
        <category>registry</category>
        <category>docker</category>
        <category>kubernetes</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>docker</tag>
      </tags>
  </entry>
  <entry>
    <title>有用的链接</title>
    <url>/2020/12/24/%E6%9C%89%E7%94%A8%E7%9A%84%E9%93%BE%E6%8E%A5/</url>
    <content><![CDATA[<table>
<thead>
<tr>
<th>技术</th>
<th align="right">链接</th>
<th align="center">简介</th>
</tr>
</thead>
<tbody><tr>
<td>SimpleCV</td>
<td align="right"><a href="http://simplecv.org/">http://simplecv.org/</a></td>
<td align="center">开源的机器视觉库（对OpenCV python库的封装，对python3不支持）</td>
</tr>
<tr>
<td>xxx</td>
<td align="right">$1</td>
<td align="center">6</td>
</tr>
<tr>
<td>草莓</td>
<td align="right">$1</td>
<td align="center">7</td>
</tr>
</tbody></table>
]]></content>
  </entry>
  <entry>
    <title>kind快速部署Kubernetes环境</title>
    <url>/2020/11/26/kind%E5%BF%AB%E9%80%9F%E9%83%A8%E7%BD%B2Kubernetes%E7%8E%AF%E5%A2%83/</url>
    <content><![CDATA[<h1 id="什么是kind"><a href="#什么是kind" class="headerlink" title="什么是kind"></a>什么是kind</h1><p>kind：Kubernetes In Docker，顾名思义，就是将kubernetes所需要的所有组件，全部部署在一个docker容器中，是一套开箱即用的kubernetes环境搭建方案。使用kind搭建的集群无法在生产中使用，但是如果你只是想在本地测试或者开发使用，不想占用太多的资源，那么使用kind是不错的选择。同样，kind还可以很方便的帮你本地的kubernetes源代码打成对应的镜像，方便测试。</p>
<p>GitHub: <a href="https://github.com/kubernetes-sigs/kind">https://github.com/kubernetes-sigs/kind</a></p>
<p>Documentation: <a href="https://kind.sigs.k8s.io/">https://kind.sigs.k8s.io/</a></p>
<h1 id="安装kind"><a href="#安装kind" class="headerlink" title="安装kind"></a>安装kind</h1><p>以Linux下安装为例：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl -Lo .&#x2F;kind https:&#x2F;&#x2F;kind.sigs.k8s.io&#x2F;dl&#x2F;v0.9.0&#x2F;kind-linux-amd64</span><br><span class="line">chmod +x .&#x2F;kind</span><br><span class="line">mv .&#x2F;kind &#x2F;usr&#x2F;local&#x2F;bin&#x2F;kind</span><br></pre></td></tr></table></figure>

<h1 id="创建、查询集群"><a href="#创建、查询集群" class="headerlink" title="创建、查询集群"></a>创建、查询集群</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kind create cluster</span><br></pre></td></tr></table></figure>
<p>该命令将默认创建名为kind的集群</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kind create cluster</span><br><span class="line">Creating cluster &quot;kind&quot; ...</span><br><span class="line"> ✓ Ensuring node image (kindest&#x2F;node:v1.19.1) 🖼</span><br><span class="line"> ✓ Preparing nodes 📦</span><br><span class="line"> ✓ Writing configuration 📜</span><br><span class="line"> ✓ Starting control-plane 🕹️</span><br><span class="line"> ✓ Installing CNI 🔌</span><br><span class="line"> ✓ Installing StorageClass 💾</span><br><span class="line">Set kubectl context to &quot;kind-kind&quot;</span><br><span class="line">You can now use your cluster with:</span><br><span class="line"></span><br><span class="line">kubectl cluster-info --context kind-kind</span><br><span class="line"></span><br><span class="line">Have a question, bug, or feature request? Let us know! https:&#x2F;&#x2F;kind.sigs.k8s.io&#x2F;#community 🙂</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kind get clusters</span><br><span class="line">kind</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kind create cluster --name test</span><br></pre></td></tr></table></figure>
<p>该命令将创建名为test的集群</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kind create cluster --name test</span><br><span class="line">Creating cluster &quot;test&quot; ...</span><br><span class="line"> ✓ Ensuring node image (kindest&#x2F;node:v1.19.1) 🖼</span><br><span class="line"> ✓ Preparing nodes 📦</span><br><span class="line"> ✓ Writing configuration 📜</span><br><span class="line"> ✓ Starting control-plane 🕹️</span><br><span class="line"> ✓ Installing CNI 🔌</span><br><span class="line"> ✓ Installing StorageClass 💾</span><br><span class="line">Set kubectl context to &quot;kind-test&quot;</span><br><span class="line">You can now use your cluster with:</span><br><span class="line"></span><br><span class="line">kubectl cluster-info --context kind-test</span><br><span class="line"></span><br><span class="line">Thanks for using kind! 😊</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kind get clusters</span><br><span class="line">kind</span><br><span class="line">test</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl cluster-info --context kind-test</span><br><span class="line">Kubernetes master is running at https:&#x2F;&#x2F;127.0.0.1:44543</span><br><span class="line">KubeDNS is running at https:&#x2F;&#x2F;127.0.0.1:44543&#x2F;api&#x2F;v1&#x2F;namespaces&#x2F;kube-system&#x2F;services&#x2F;kube-dns:dns&#x2F;proxy</span><br><span class="line"></span><br><span class="line">To further debug and diagnose cluster problems, use &#39;kubectl cluster-info dump&#39;.</span><br></pre></td></tr></table></figure>

<p>kubectl安装：<a href="https://kubernetes.io/docs/tasks/tools/install-kubectl/">https://kubernetes.io/docs/tasks/tools/install-kubectl/</a></p>
<p>查询集群节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kubectl get nodes --context kind-test</span><br><span class="line">NAME                 STATUS   ROLES    AGE     VERSION</span><br><span class="line">test-control-plane   Ready    master   4m55s   v1.19.1</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get nodes --context kind-kind</span><br><span class="line">NAME                 STATUS   ROLES    AGE     VERSION</span><br><span class="line">kind-control-plane   Ready    master   9m14s   v1.19.1</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# docker ps</span><br><span class="line">CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                       NAMES</span><br><span class="line">7fa7f8313453        kindest&#x2F;node:v1.19.1   &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;entr…&quot;   16 minutes ago      Up 16 minutes       127.0.0.1:44543-&gt;6443&#x2F;tcp   test-control-plane</span><br><span class="line">6fee527273e2        kindest&#x2F;node:v1.19.1   &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;entr…&quot;   20 minutes ago      Up 20 minutes       127.0.0.1:36585-&gt;6443&#x2F;tcp   kind-control-plane</span><br><span class="line">root@ubuntu:~# docker ps -a</span><br><span class="line">CONTAINER ID        IMAGE                  COMMAND                  CREATED             STATUS              PORTS                       NAMES</span><br><span class="line">7fa7f8313453        kindest&#x2F;node:v1.19.1   &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;entr…&quot;   16 minutes ago      Up 16 minutes       127.0.0.1:44543-&gt;6443&#x2F;tcp   test-control-plane</span><br><span class="line">6fee527273e2        kindest&#x2F;node:v1.19.1   &quot;&#x2F;usr&#x2F;local&#x2F;bin&#x2F;entr…&quot;   20 minutes ago      Up 20 minutes       127.0.0.1:36585-&gt;6443&#x2F;tcp   kind-control-plane</span><br></pre></td></tr></table></figure>
<p>test-control-plane、kind-control-plane是运行在容器内的kubernetes节点，也正是kubernetes in Docker的意思</p>
<p>查询集群内运行的pod</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kubectl get pods --all-namespaces --context kind-kind</span><br><span class="line">NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system          coredns-f9fd979d6-cqpss                      1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          coredns-f9fd979d6-kvbzm                      1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          etcd-kind-control-plane                      1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          kindnet-br2qc                                1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          kube-apiserver-kind-control-plane            1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          kube-controller-manager-kind-control-plane   1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          kube-proxy-4chlm                             1&#x2F;1     Running   0          22m</span><br><span class="line">kube-system          kube-scheduler-kind-control-plane            1&#x2F;1     Running   0          22m</span><br><span class="line">local-path-storage   local-path-provisioner-78776bfc44-xl4ht      1&#x2F;1     Running   0          22m</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get pods --all-namespaces --context kind-test</span><br><span class="line">NAMESPACE            NAME                                         READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system          coredns-f9fd979d6-f7wsz                      1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          coredns-f9fd979d6-j9xtx                      1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          etcd-test-control-plane                      1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          kindnet-jq867                                1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          kube-apiserver-test-control-plane            1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          kube-controller-manager-test-control-plane   1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          kube-proxy-swn5j                             1&#x2F;1     Running   0          18m</span><br><span class="line">kube-system          kube-scheduler-test-control-plane            1&#x2F;1     Running   0          18m</span><br><span class="line">local-path-storage   local-path-provisioner-78776bfc44-nwqjq      1&#x2F;1     Running   0          18m</span><br></pre></td></tr></table></figure>

<p>查询pod使用的镜像</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# docker exec -it kind-control-plane crictl images</span><br><span class="line">IMAGE                                      TAG                  IMAGE ID            SIZE</span><br><span class="line">docker.io&#x2F;kindest&#x2F;kindnetd                 v20200725-4d6bea59   b77790820d015       119MB</span><br><span class="line">docker.io&#x2F;rancher&#x2F;local-path-provisioner   v0.0.14              e422121c9c5f9       42MB</span><br><span class="line">k8s.gcr.io&#x2F;build-image&#x2F;debian-base         v2.1.0               c7c6c86897b63       53.9MB</span><br><span class="line">k8s.gcr.io&#x2F;coredns                         1.7.0                bfe3a36ebd252       45.4MB</span><br><span class="line">k8s.gcr.io&#x2F;etcd                            3.4.13-0             0369cf4303ffd       255MB</span><br><span class="line">k8s.gcr.io&#x2F;kube-apiserver                  v1.19.1              8cba89a89aaa8       95MB</span><br><span class="line">k8s.gcr.io&#x2F;kube-controller-manager         v1.19.1              7dafbafe72c90       84.1MB</span><br><span class="line">k8s.gcr.io&#x2F;kube-proxy                      v1.19.1              47e289e332426       136MB</span><br><span class="line">k8s.gcr.io&#x2F;kube-scheduler                  v1.19.1              4d648fc900179       65.1MB</span><br><span class="line">k8s.gcr.io&#x2F;pause                           3.3                  0184c1613d929       686kB</span><br><span class="line">root@ubuntu:~# docker exec -it kind-control-plane crictl ps</span><br><span class="line">CONTAINER           IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID</span><br><span class="line">1c881f7f0d306       bfe3a36ebd252       25 minutes ago      Running             coredns                   0                   75e161b37ab2d</span><br><span class="line">c64dd6bb666f3       bfe3a36ebd252       25 minutes ago      Running             coredns                   0                   a8305dc572af5</span><br><span class="line">fea2b69f5472d       e422121c9c5f9       26 minutes ago      Running             local-path-provisioner    0                   fb704b6340b63</span><br><span class="line">52f0995ba00f8       47e289e332426       26 minutes ago      Running             kube-proxy                0                   4c787e616d5a7</span><br><span class="line">b87cdcc514f59       b77790820d015       26 minutes ago      Running             kindnet-cni               0                   14eb70f9ca549</span><br><span class="line">ce2c4e5b2b57f       0369cf4303ffd       27 minutes ago      Running             etcd                      0                   744a99a558714</span><br><span class="line">93b5084a29992       8cba89a89aaa8       27 minutes ago      Running             kube-apiserver            0                   91f88afc5a39b</span><br><span class="line">8b9579313058f       7dafbafe72c90       27 minutes ago      Running             kube-controller-manager   0                   8d54fdffef86e</span><br><span class="line">10fbb8244ad3b       4d648fc900179       27 minutes ago      Running             kube-scheduler            0                   a985ae4a105bc</span><br></pre></td></tr></table></figure>
<p>其中，crictl命令可以理解为docker命令</p>
<h1 id="删除集群"><a href="#删除集群" class="headerlink" title="删除集群"></a>删除集群</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">kind delete cluster --name test</span><br></pre></td></tr></table></figure>
<p>删除名为test的集群，–name未指定的话，将默认删除kind集群</p>
<h1 id="创建高可用kubernetes集群"><a href="#创建高可用kubernetes集群" class="headerlink" title="创建高可用kubernetes集群"></a>创建高可用kubernetes集群</h1><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# cat kind-config.yaml</span><br><span class="line">kind: Cluster</span><br><span class="line">apiVersion: kind.x-k8s.io&#x2F;v1alpha4</span><br><span class="line">nodes:</span><br><span class="line">- role: control-plane</span><br><span class="line">- role: control-plane</span><br><span class="line">- role: control-plane</span><br><span class="line">- role: worker</span><br><span class="line">- role: worker</span><br><span class="line">- role: worker</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kind create cluster --config kind-config.yaml --name test2</span><br><span class="line">Creating cluster &quot;test2&quot; ...</span><br><span class="line"> ✓ Ensuring node image (kindest&#x2F;node:v1.19.1) 🖼</span><br><span class="line"> ✓ Preparing nodes 📦 📦 📦 📦 📦 📦</span><br><span class="line"> ✓ Configuring the external load balancer ⚖️</span><br><span class="line"> ✓ Writing configuration 📜</span><br><span class="line"> ✓ Starting control-plane 🕹️</span><br><span class="line"> ✓ Installing CNI 🔌</span><br><span class="line"> ✓ Installing StorageClass 💾</span><br><span class="line"> ✓ Joining more control-plane nodes 🎮</span><br><span class="line"> ✓ Joining worker nodes 🚜</span><br><span class="line">Set kubectl context to &quot;kind-test2&quot;</span><br><span class="line">You can now use your cluster with:</span><br><span class="line"></span><br><span class="line">kubectl cluster-info --context kind-test2</span><br><span class="line"></span><br><span class="line">Have a nice day! 👋</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get nodes --context kind-test2</span><br><span class="line">NAME                   STATUS   ROLES    AGE     VERSION</span><br><span class="line">test2-control-plane    Ready    master   5m12s   v1.19.1</span><br><span class="line">test2-control-plane2   Ready    master   4m38s   v1.19.1</span><br><span class="line">test2-control-plane3   Ready    master   3m22s   v1.19.1</span><br><span class="line">test2-worker           Ready    &lt;none&gt;   2m5s    v1.19.1</span><br><span class="line">test2-worker2          Ready    &lt;none&gt;   2m5s    v1.19.1</span><br><span class="line">test2-worker3          Ready    &lt;none&gt;   2m5s    v1.19.1</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get pods --all-namespaces --context kind-test2</span><br><span class="line">NAMESPACE            NAME                                           READY   STATUS    RESTARTS   AGE</span><br><span class="line">kube-system          coredns-f9fd979d6-jbpcz                        1&#x2F;1     Running   0          5m16s</span><br><span class="line">kube-system          coredns-f9fd979d6-ng4qp                        1&#x2F;1     Running   0          5m16s</span><br><span class="line">kube-system          etcd-test2-control-plane                       1&#x2F;1     Running   0          5m15s</span><br><span class="line">kube-system          etcd-test2-control-plane2                      1&#x2F;1     Running   0          4m54s</span><br><span class="line">kube-system          etcd-test2-control-plane3                      1&#x2F;1     Running   0          2m59s</span><br><span class="line">kube-system          kindnet-gxpjn                                  1&#x2F;1     Running   0          4m55s</span><br><span class="line">kube-system          kindnet-jqnx5                                  1&#x2F;1     Running   0          2m20s</span><br><span class="line">kube-system          kindnet-lczmx                                  1&#x2F;1     Running   0          2m18s</span><br><span class="line">kube-system          kindnet-q8bcn                                  1&#x2F;1     Running   0          2m19s</span><br><span class="line">kube-system          kindnet-q9ng2                                  1&#x2F;1     Running   0          3m37s</span><br><span class="line">kube-system          kindnet-s7kfb                                  1&#x2F;1     Running   0          5m14s</span><br><span class="line">kube-system          kube-apiserver-test2-control-plane             1&#x2F;1     Running   0          5m15s</span><br><span class="line">kube-system          kube-apiserver-test2-control-plane2            1&#x2F;1     Running   0          4m54s</span><br><span class="line">kube-system          kube-apiserver-test2-control-plane3            1&#x2F;1     Running   1          3m9s</span><br><span class="line">kube-system          kube-controller-manager-test2-control-plane    1&#x2F;1     Running   2          5m14s</span><br><span class="line">kube-system          kube-controller-manager-test2-control-plane2   1&#x2F;1     Running   0          4m54s</span><br><span class="line">kube-system          kube-controller-manager-test2-control-plane3   1&#x2F;1     Running   0          2m8s</span><br><span class="line">kube-system          kube-proxy-47nc7                               1&#x2F;1     Running   0          5m16s</span><br><span class="line">kube-system          kube-proxy-5799m                               1&#x2F;1     Running   0          4m55s</span><br><span class="line">kube-system          kube-proxy-cvm49                               1&#x2F;1     Running   0          2m18s</span><br><span class="line">kube-system          kube-proxy-s7rsp                               1&#x2F;1     Running   0          2m18s</span><br><span class="line">kube-system          kube-proxy-sxwgl                               1&#x2F;1     Running   0          3m37s</span><br><span class="line">kube-system          kube-proxy-wvskh                               1&#x2F;1     Running   0          2m20s</span><br><span class="line">kube-system          kube-scheduler-test2-control-plane             0&#x2F;1     Running   2          5m15s</span><br><span class="line">kube-system          kube-scheduler-test2-control-plane2            1&#x2F;1     Running   0          4m54s</span><br><span class="line">kube-system          kube-scheduler-test2-control-plane3            1&#x2F;1     Running   0          2m31s</span><br><span class="line">local-path-storage   local-path-provisioner-78776bfc44-fkdwq        1&#x2F;1     Running   1          5m12s</span><br></pre></td></tr></table></figure>

<p>上述过程，我们创建了两个kubernetes集群，kind和test2；可以看到我们在使用kubectl访问集群时，增加了参数：–context，这在本地的配置文件里指定了</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# cat .kube&#x2F;config</span><br><span class="line">apiVersion: v1</span><br><span class="line">clusters:</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01URXlOekF3TkRJeU9Wb1hEVE13TVRFeU5UQXdOREl5T1Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBTERTCm5TU1NHS05PUUlCVWpQeXpYOFRDZm5IamExV2JxeXZJeHV3Y1Aybmh6Zi9EdG9wVDRnSTVjMlBMV3Qrd05BUFUKUHA2dGV3ZkhNQ3N6MnJLbnhaRmtWS2c5NXVFdW53V1ZuZnlmeGV0TjlOU1JTZ2s2dkJuVUt6SFliRXIybEY0LwpicFVxT2IzWnkxQXdYNlpyRTN3Y1I1RjdLV2trT0FZbHdobUtiLzIwOVZJRG4yMW9CMHMzNXgrM3Z2L2gzQ3VaCkJCQjJnNVBKMm4xc1pwd05scnZDMmh0RmJDSjQwQVNXZmNsUksyejBYUEIvdzdNQlBXMHp1cnpEMUVBcGdZVUcKUGUwOEx3dW1zMllZR3Y5TDJXMERhWW90c2JoVXlWZWRDNnpjeWtUZ0hOb1B6LytvZlBHQmxYTko3S2lwd2Z6ZQo2dzdHZGJhbjlpRms4cjAyaVQ4Q0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZEY2liT2RWZTN4U0w0RmMxOWVJdmZnSTBHcjZNQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFCS1lKSVFaenhSblFWWWp4TnlTaXJualFvZUM3OGtDMERNYjZpRFBvMFdEbmdTUjhRYgpqRjZxR2ZPMTFTcDNjeVl3V1IrM3UzU2pzQUJUUG5jcWZpWnEyN3VDMlRYNjdMQzRPNVFoVkdlRlBDdXJNNHluCi8weDZuOUZrMko5YmpTT0o1aDB2NmQ1eXBibk5sNVgvN1czWFVyK2tjOSswWG1sMFN0dmJVZ0hCaWVmWGxtZ3AKUXl5TlFtNU1yWlRFcWJOT0JubW1RWUJYWDdydWVLZXhVYUJ4QXQrRVJVWHBVOWNhbkNWWWhuZFBuMVNBYVladApsQkZyamRSNzg1SE1EV21qTW5UalJXSUhOSWd2NUgwKyt2MmN5cjRSK1lUWGo3S0JrdEZTRDA2U0I0RDZVeFk2CmRUZ25UMngzRXJvSERPZURjWWRrVmt2TkF5aC9JUGFTd2R6cwotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg&#x3D;&#x3D;</span><br><span class="line">    server: https:&#x2F;&#x2F;127.0.0.1:36585</span><br><span class="line">  name: kind-kind</span><br><span class="line">- cluster:</span><br><span class="line">    certificate-authority-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUM1ekNDQWMrZ0F3SUJBZ0lCQURBTkJna3Foa2lHOXcwQkFRc0ZBREFWTVJNd0VRWURWUVFERXdwcmRXSmwKY201bGRHVnpNQjRYRFRJd01URXlOekF6TWpJeE9Gb1hEVE13TVRFeU5UQXpNakl4T0Zvd0ZURVRNQkVHQTFVRQpBeE1LYTNWaVpYSnVaWFJsY3pDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRGdnRVBBRENDQVFvQ2dnRUJBSi85CjlZQ0VseTNRLzRuc2t5dGZZY2Zpby9jcDJoaSsxRWpPOGMrVmF4YTkrbTIrdldBT1VhU0xQZnQ0VmRSL1hIWjYKT3FQSEY5K3ozNEZIb1p4RTJIY3kxd3R0Y1hOTXk1Qy9VMUdnakgramZLaWNmbVJmS3luS1M4SU80T3pLRHBKUQpodHQzb0JxeDVHdU0wSUpkVmsydVg5RjhmVHEyTllaN2lzK0NOVWdXM2dxMXA4SzNkZWxjaDYyM2NBSHhSS0JLCmZEZC9iZnZiZitvR0ZQS29BNWhLcXVKb3BDVFcrN1VZdnA4ZCs0QTgwYkZ2cG5CSmlUK1pGU0sxRExKZHBuRlgKV0pDOHdwNzNBYU9KSHAydnNTZ0p6ajFvUHUvL1lnSWNPNStoSGl6bThVcnVGQWxkWmE2ZzN4VnFCazNDZnducApsaU50MzdiblJrZ3VUNUU5cFFNQ0F3RUFBYU5DTUVBd0RnWURWUjBQQVFIL0JBUURBZ0trTUE4R0ExVWRFd0VCCi93UUZNQU1CQWY4d0hRWURWUjBPQkJZRUZNdnpFOXJORVFUWGNyUmJHcllmQ0J6VEpZby9NQTBHQ1NxR1NJYjMKRFFFQkN3VUFBNElCQVFBMjJZS1htS29kTkVmUklGS1ZOMXk1alRjQ2FIVnAyd3QvR1RNT3dHU1pYQXhlN2xBOQpIZDVCWHJoaXB6bkNPR1hxakRoZkVkaWVIa3VlcDNQWGxVTWxxa20yUGZkdUp0cTJ3T0piR0ZTVWZRR2xpS09MCkJ0eDRwVmd1dC9EQW11ZHNNNFlSeGRTS1R3bkppcjBBbkNnSWprd0gvZzdzekFYTVppMmZkYk9oL3NzcVFlZU4KREYrc01ld0czK2pSaVVqTEM2ck9sb0UySzArVjNhMDlMbmNiYlRCNDBVM0pZR0VvallzVExEdXZPakhuZElkKwpyM0FHMlNXMDRYek8wcHF4VDhlRjlkUE5qVG83SlpIeDc3RUtuNVNYU1YweGRuTTR1eHVBWDBoVExLenFyMlRNCmVJTzlZNGZWeHdCQk9BaFBQVmFTMW5SZy9GMXFIWWNKK20wUgotLS0tLUVORCBDRVJUSUZJQ0FURS0tLS0tCg&#x3D;&#x3D;</span><br><span class="line">    server: https:&#x2F;&#x2F;127.0.0.1:34927</span><br><span class="line">  name: kind-test2</span><br><span class="line">contexts:</span><br><span class="line">- context:</span><br><span class="line">    cluster: kind-kind</span><br><span class="line">    user: kind-kind</span><br><span class="line">  name: kind-kind</span><br><span class="line">- context:</span><br><span class="line">    cluster: kind-test2</span><br><span class="line">    user: kind-test2</span><br><span class="line">  name: kind-test2</span><br><span class="line">current-context: kind-test2</span><br><span class="line">kind: Config</span><br><span class="line">preferences: &#123;&#125;</span><br><span class="line">users:</span><br><span class="line">- name: kind-kind</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURFekNDQWZ1Z0F3SUJBZ0lJTHRQZkozeVliVHN3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TURFeE1qY3dNRFF5TWpsYUZ3MHlNVEV4TWpjd01EUXlNekZhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXJEaURMOWhSUGduVzNwdlcKQ1pNekVlTG0vRTJuVmlXdG9icTVoZUdrdG1Udi9aWEUydG9uKy96ZWROOThvM3J4ZEdneDNTZDRndjZtSnpQbgpFZ3hTRDcrQnErSmt5WG5oQThPM1RQakN0aHdES1JIZjEzbkI4NDNlbjZ3S3E5K3A1RHB2UFNNSkgyQkJLckkrCjlHWTFvTWp2SnRWSjMyZ1dhTnlJV0hzTkE0QmdPM0RRT2Y2cGN6bXZhbllKZFliYTFpQmpKUEVaRURPNEpFdHIKZWYrTUFHb1diSFlaeDdySWo5eTJoNzdOQi9DV0k1alVSUXp6MWQ0ZlFnV05WR1cyblVXbGlGZjlEWlRVR25CMwpHMHBUTjc5N1U0bUNYdWVBUjFxY25idS9YNjBMb2l6bEpKVWJzcjZYUWFaRmtKS2FzSGx2Tmdydm11cWhnSjZTCmxCWWNIUUlEQVFBQm8wZ3dSakFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0h3WURWUjBqQkJnd0ZvQVVOeUpzNTFWN2ZGSXZnVnpYMTRpOStBalFhdm93RFFZSktvWklodmNOQVFFTApCUUFEZ2dFQkFBay8xbjJqLzhGdlhISm94V0Q3dUl4WUxTUmlrTjcxWWdOOW1yckVYRjVFUkxBMjI5eEtyT1c1ClV2Tm9nZGlqdWpEbDhoVjRoU1hkY3M0dE9ZNmZYbExkN3JhN25vL054UUtHM1lPdktWM2g0eS9wUEYxMENQL0sKTTFhSnNkY1U2aG8wbnZrL1dQSDB2ckxtNE1jUHpBbFUvazdvd3FiemcybHZQdDBCMTJjZHh4bk44UHp5VU4zZwpQQVBzT3hab1hwQnVpNjkxcEt2a1VDUm1MY0dUTHowT0Y5YXVOaUhRRG1qMG1hSEg2ckI2c0kvQzBuZ08vaCtSCmNsUnBGYk9uQnZRbW1nRjZER0E4cGxUSUM2ZE1JOGtXRzVaQWFSbzMrblFLME5sMERaUDVBTjM1cE1yS0M1R3cKeTdHaVlRQkFCVlJOZ2FJemg1bktHUXFJVVFXRUU5TT0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo&#x3D;</span><br><span class="line">    client-key-data: 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&#x3D;&#x3D;</span><br><span class="line">- name: kind-test2</span><br><span class="line">  user:</span><br><span class="line">    client-certificate-data: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSURFekNDQWZ1Z0F3SUJBZ0lJR2toZno1Qy9Yc2d3RFFZSktvWklodmNOQVFFTEJRQXdGVEVUTUJFR0ExVUUKQXhNS2EzVmlaWEp1WlhSbGN6QWVGdzB5TURFeE1qY3dNekl5TVRoYUZ3MHlNVEV4TWpjd016SXlNakJhTURReApGekFWQmdOVkJBb1REbk41YzNSbGJUcHRZWE4wWlhKek1Sa3dGd1lEVlFRREV4QnJkV0psY201bGRHVnpMV0ZrCmJXbHVNSUlCSWpBTkJna3Foa2lHOXcwQkFRRUZBQU9DQVE4QU1JSUJDZ0tDQVFFQXJ6azZ2RFJsMjI4V1RndUEKYnNNN0tLVUVaaWRmbVBKVW1kaitmdCs1UTZFSHFvUitzOHVycDVVc0tOY0krZzdoYkNJa2VzdzE4OGZ0NDVlSwp4eGpmV1NiUXNiSjYyb3RQRUVoSGpPZElRTHVxQlBqR1J1N3NwUE8yRzFnMFZTMm9UY0VUVlYzQ2RCUFFPRjNOCm9WblhPdzQwSWttWXVURDZHUEdNT2VwN05Rak4vZERybVphMy94WWxJK2loVHZJbnZXNjVYYmd0NTNwS203YXgKOUNSVGhjNUtONjg2TzVLekR6ZnhPZGtpUUhJdzNXL3BpTXRvOVV5QUF3V2RoUmc0K0VacW5OTjZXREl4RlNPNQpJN0Z6c3dtT2s0L29wcW55ZHhmNnFYamF6MGlvTTNyM0I4bUY0c09VakhtaDJyUnB3a3ZpQUxkV3AvbVo0ek5uClJYYVp3UUlEQVFBQm8wZ3dSakFPQmdOVkhROEJBZjhFQkFNQ0JhQXdFd1lEVlIwbEJBd3dDZ1lJS3dZQkJRVUgKQXdJd0h3WURWUjBqQkJnd0ZvQVV5L01UMnMwUkJOZHl0RnNhdGg4SUhOTWxpajh3RFFZSktvWklodmNOQVFFTApCUUFEZ2dFQkFIMHg2am5uajNSQWxBY0dOZmxyY1Zob01XVUpOZCtVak5OSXNSTHRaSzJTYTh6SThCK0VIeUxFCndhRjJGMzZ0VTV6L1JiSFluK3d3Q3ByQVJjLzhYTUx2OEpnMVU2aDg2SkxpNm9qc29Vb0dhUFNoWmI3S1YyOUQKSE5JQ2p5M2QrenhjdEV4OWFTRDB3WkR0cFpVaS9tZlQ3MklRekVUazQ2QTQ4bldYdkx0MWJ6TEZ4T1hQZVB1WgpiZG1lcWR5QmN1TXI0MUppcEJVUlpOWDgxQ0xIQ1ZQU2tsRHkySVl0OUgxeVNmb3RHMUV4ZHlDZE1vRjZTVUdhCkpLY0psNGFSQ3JNLzNqSGx3YVpGcWR0WkFFZ2pzM3lOYXllTW9SYi93bEdpWkdaQTFhQ3JNZlU5bXRnTlMrMWIKbmE1M0RxVmRKZHBTSnhWSjNPQXpUY2s0OVV1Z2c4ND0KLS0tLS1FTkQgQ0VSVElGSUNBVEUtLS0tLQo&#x3D;</span><br><span class="line">    client-key-data: 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&#x3D;&#x3D;</span><br></pre></td></tr></table></figure>
<p>可以看到两个cluster，两个context。context指定了cluster及user，users字段列出了两个user</p>
<p>context名为kind-kind，cluser为kind-kind，user为kind-kind，意思就是当指定–context为kind-kind时，将使用kind-kind的user去访问cluster为kind-kind的集群。</p>
<p>current-context: kind-test2说明当前的context使用的是kind-test2，如果不指定–context，将默认使用kind-test2</p>
<p>当然，用户可以修改当然默认的current-context</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu:~# kubectl get node</span><br><span class="line">NAME                   STATUS   ROLES    AGE     VERSION</span><br><span class="line">test2-control-plane    Ready    master   2d23h   v1.19.1</span><br><span class="line">test2-control-plane2   Ready    master   2d23h   v1.19.1</span><br><span class="line">test2-control-plane3   Ready    master   2d23h   v1.19.1</span><br><span class="line">test2-worker           Ready    &lt;none&gt;   2d23h   v1.19.1</span><br><span class="line">test2-worker2          Ready    &lt;none&gt;   2d23h   v1.19.1</span><br><span class="line">test2-worker3          Ready    &lt;none&gt;   2d23h   v1.19.1</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl config</span><br><span class="line">Modify kubeconfig files using subcommands like &quot;kubectl config set current-context my-context&quot;</span><br><span class="line"></span><br><span class="line"> The loading order follows these rules:</span><br><span class="line"></span><br><span class="line">  1.  If the --kubeconfig flag is set, then only that file is loaded. The flag may only be set once and no merging takes</span><br><span class="line">place.</span><br><span class="line">  2.  If $KUBECONFIG environment variable is set, then it is used as a list of paths (normal path delimiting rules for</span><br><span class="line">your system). These paths are merged. When a value is modified, it is modified in the file that defines the stanza. When</span><br><span class="line">a value is created, it is created in the first file that exists. If no files in the chain exist, then it creates the</span><br><span class="line">last file in the list.</span><br><span class="line">  3.  Otherwise, $&#123;HOME&#125;&#x2F;.kube&#x2F;config is used and no merging takes place.</span><br><span class="line"></span><br><span class="line">Available Commands:</span><br><span class="line">  current-context Displays the current-context</span><br><span class="line">  delete-cluster  Delete the specified cluster from the kubeconfig</span><br><span class="line">  delete-context  Delete the specified context from the kubeconfig</span><br><span class="line">  get-clusters    Display clusters defined in the kubeconfig</span><br><span class="line">  get-contexts    Describe one or many contexts</span><br><span class="line">  rename-context  Renames a context from the kubeconfig file.</span><br><span class="line">  set             Sets an individual value in a kubeconfig file</span><br><span class="line">  set-cluster     Sets a cluster entry in kubeconfig</span><br><span class="line">  set-context     Sets a context entry in kubeconfig</span><br><span class="line">  set-credentials Sets a user entry in kubeconfig</span><br><span class="line">  unset           Unsets an individual value in a kubeconfig file</span><br><span class="line">  use-context     Sets the current-context in a kubeconfig file</span><br><span class="line">  view            Display merged kubeconfig settings or a specified kubeconfig file</span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">  kubectl config SUBCOMMAND [options]</span><br><span class="line"></span><br><span class="line">Use &quot;kubectl &lt;command&gt; --help&quot; for more information about a given command.</span><br><span class="line">Use &quot;kubectl options&quot; for a list of global command-line options (applies to all commands).</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl config use-context</span><br><span class="line">Sets the current-context in a kubeconfig file</span><br><span class="line"></span><br><span class="line">Aliases:</span><br><span class="line">use-context, use</span><br><span class="line"></span><br><span class="line">Examples:</span><br><span class="line">  # Use the context for the minikube cluster</span><br><span class="line">  kubectl config use-context minikube</span><br><span class="line"></span><br><span class="line">Usage:</span><br><span class="line">  kubectl config use-context CONTEXT_NAME [options]</span><br><span class="line"></span><br><span class="line">Use &quot;kubectl options&quot; for a list of global command-line options (applies to all commands).</span><br><span class="line">error: Unexpected args: []</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl config use-context kind-kind</span><br><span class="line">Switched to context &quot;kind-kind&quot;.</span><br><span class="line"></span><br><span class="line">root@ubuntu:~# kubectl get node</span><br><span class="line">NAME                 STATUS   ROLES    AGE    VERSION</span><br><span class="line">kind-control-plane   Ready    master   3d1h   v1.19.1</span><br></pre></td></tr></table></figure>
<p>实际上，kind内部创建集群过程中，也是使用kubeadm，从上述创建过程中的打印也似乎能猜到这一点</p>
<p>后面，就可以使用搭建好的集群环境，开始你的开发、测试工作了~~~</p>
]]></content>
      <categories>
        <category>云原生</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>云原生</tag>
      </tags>
  </entry>
  <entry>
    <title>kubernetes with containerd</title>
    <url>/2021/02/08/kubernetes-with-containerd/</url>
    <content><![CDATA[<p>参考：<a href="https://www.hi-linux.com/posts/10148.html">https://www.hi-linux.com/posts/10148.html</a></p>
<h1 id="前提条件"><a href="#前提条件" class="headerlink" title="前提条件"></a>前提条件</h1><p>已安装kata container及containerd with CRI plugin</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# kata-runtime --version</span><br><span class="line">kata-runtime  : 1.13.0-alpha0</span><br><span class="line">   commit   : &lt;&lt;unknown&gt;&gt;</span><br><span class="line">   OCI specs: 1.0.1-dev</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# containerd --version</span><br><span class="line">containerd containerd.io 1.4.3 269548fa27e0089a8b8278fc4fc781d7f65a939b</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# ctr --version</span><br><span class="line">ctr containerd.io 1.4.3</span><br></pre></td></tr></table></figure>
<p>注意：</p>
<p>containerd在安装docker（新版本docker）已经作为docker依赖项安装了</p>
<p>ctr是containerd的命令行工具（containerd CLI）</p>
<p>cri is a native plugin of containerd 1.1 and above. It is built into containerd and enabled by default. You do not need to install cri if you have containerd 1.1 or above. Just remove the cri plugin from the list of disabled_plugins in the containerd configuration file (/etc/containerd/config.toml).</p>
<p><strong>目前的主要问题是，Kata 不支持 host 网络。而 Kubernetes 中，etcd、nodelocaldns、kube-apiserver、kube-scheduler、metrics-server、node-exporter、kube-proxy、calico、kube-controller-manager 等，也就是 Static Pod 和 Daemonset 都会使用 host 网络。所以在安装部署时，依然使用 runc 作为默认的运行时，而将 kata-runtime 作为可选的运行时给特定的负载使用。</strong></p>
<h1 id="基于kubeadm搭建kubernetes集群"><a href="#基于kubeadm搭建kubernetes集群" class="headerlink" title="基于kubeadm搭建kubernetes集群"></a>基于kubeadm搭建kubernetes集群</h1><h2 id="1、安装kubelet、kubeadm-和-kubectl"><a href="#1、安装kubelet、kubeadm-和-kubectl" class="headerlink" title="1、安装kubelet、kubeadm 和 kubectl"></a>1、安装kubelet、kubeadm 和 kubectl</h2><p><strong>配置kubernetes.repo的源，由于官方源国内无法访问，这里使用阿里云源</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">curl https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt&#x2F;doc&#x2F;apt-key.gpg | apt-key add - </span><br><span class="line">cat &lt;&lt;EOF &gt;&#x2F;etc&#x2F;apt&#x2F;sources.list.d&#x2F;kubernetes.list</span><br><span class="line">deb https:&#x2F;&#x2F;mirrors.aliyun.com&#x2F;kubernetes&#x2F;apt&#x2F; kubernetes-xenial main</span><br><span class="line">EOF  </span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>在所有节点上安装指定版本 kubelet、kubeadm 和 kubectl</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">apt-get update</span><br><span class="line">apt-cache madison kubectl</span><br><span class="line">apt-cache madison kubeadm</span><br><span class="line">apt-cache madison kubelet</span><br><span class="line">apt-get install -y kubelet&#x3D;1.19.3-00 kubeadm&#x3D;1.19.3-00 kubectl&#x3D;1.19.3-00</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p><strong>启动kubelet服务</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">systemctl enable kubelet &amp;&amp; systemctl start kubelet </span><br></pre></td></tr></table></figure>

<h2 id="2、关闭swap分区"><a href="#2、关闭swap分区" class="headerlink" title="2、关闭swap分区"></a>2、关闭swap分区</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">swapoff -a</span><br></pre></td></tr></table></figure>

<h2 id="3、初始化master节点-–control-plane-endpoint指定vip，搭建HA集群，–cri-socket指定containerd为容器运行时，默认为docker"><a href="#3、初始化master节点-–control-plane-endpoint指定vip，搭建HA集群，–cri-socket指定containerd为容器运行时，默认为docker" class="headerlink" title="3、初始化master节点(–control-plane-endpoint指定vip，搭建HA集群，–cri-socket指定containerd为容器运行时，默认为docker)"></a>3、初始化master节点(–control-plane-endpoint指定vip，搭建HA集群，–cri-socket指定containerd为容器运行时，默认为docker)</h2><p>kubeadm init <strong>–control-plane-endpoint 10.0.105.121</strong> –image-repository registry.aliyuncs.com/google_containers <strong>–cri-socket /run/containerd/containerd.sock</strong> –kubernetes-version v1.19.3 –pod-network-cidr 10.244.0.0/16 –token-ttl 0 –ignore-preflight-errors Swap –upload-certs</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# kubeadm init --control-plane-endpoint 10.0.105.121 --image-repository registry.aliyuncs.com&#x2F;google_containers --cri-socket &#x2F;run&#x2F;containerd&#x2F;containerd.sock --kubernetes-version v1.19.3 --pod-network-cidr 10.244.0.0&#x2F;16 --token-ttl 0 --ignore-preflight-errors Swap --upload-certs</span><br><span class="line">W0208 10:57:44.075039   30107 configset.go:348] WARNING: kubeadm cannot validate component configs for API groups [kubelet.config.k8s.io kubeproxy.config.k8s.io]</span><br><span class="line">[init] Using Kubernetes version: v1.19.3</span><br><span class="line">[preflight] Running pre-flight checks</span><br><span class="line">[preflight] Pulling images required for setting up a Kubernetes cluster</span><br><span class="line">[preflight] This might take a minute or two, depending on the speed of your internet connection</span><br><span class="line">[preflight] You can also perform this action in beforehand using &#39;kubeadm config images pull&#39;</span><br><span class="line">[certs] Using certificateDir folder &quot;&#x2F;etc&#x2F;kubernetes&#x2F;pki&quot;</span><br><span class="line">[certs] Generating &quot;ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver&quot; certificate and key</span><br><span class="line">[certs] apiserver serving cert is signed for DNS names [kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local ubuntu-001] and IPs [10.96.0.1 10.0.105.121]</span><br><span class="line">[certs] Generating &quot;apiserver-kubelet-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;front-proxy-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd&#x2F;ca&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;etcd&#x2F;server&quot; certificate and key</span><br><span class="line">[certs] etcd&#x2F;server serving cert is signed for DNS names [localhost ubuntu-001] and IPs [10.0.105.121 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd&#x2F;peer&quot; certificate and key</span><br><span class="line">[certs] etcd&#x2F;peer serving cert is signed for DNS names [localhost ubuntu-001] and IPs [10.0.105.121 127.0.0.1 ::1]</span><br><span class="line">[certs] Generating &quot;etcd&#x2F;healthcheck-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;apiserver-etcd-client&quot; certificate and key</span><br><span class="line">[certs] Generating &quot;sa&quot; key and public key</span><br><span class="line">[kubeconfig] Using kubeconfig folder &quot;&#x2F;etc&#x2F;kubernetes&quot;</span><br><span class="line">[kubeconfig] Writing &quot;admin.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;kubelet.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;controller-manager.conf&quot; kubeconfig file</span><br><span class="line">[kubeconfig] Writing &quot;scheduler.conf&quot; kubeconfig file</span><br><span class="line">[kubelet-start] Writing kubelet environment file with flags to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;kubeadm-flags.env&quot;</span><br><span class="line">[kubelet-start] Writing kubelet configuration to file &quot;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml&quot;</span><br><span class="line">[kubelet-start] Starting the kubelet</span><br><span class="line">[control-plane] Using manifest folder &quot;&#x2F;etc&#x2F;kubernetes&#x2F;manifests&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-apiserver&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-controller-manager&quot;</span><br><span class="line">[control-plane] Creating static Pod manifest for &quot;kube-scheduler&quot;</span><br><span class="line">[etcd] Creating static Pod manifest for local etcd in &quot;&#x2F;etc&#x2F;kubernetes&#x2F;manifests&quot;</span><br><span class="line">[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory &quot;&#x2F;etc&#x2F;kubernetes&#x2F;manifests&quot;. This can take up to 4m0s</span><br><span class="line">[apiclient] All control plane components are healthy after 13.501962 seconds</span><br><span class="line">[upload-config] Storing the configuration used in ConfigMap &quot;kubeadm-config&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[kubelet] Creating a ConfigMap &quot;kubelet-config-1.19&quot; in namespace kube-system with the configuration for the kubelets in the cluster</span><br><span class="line">[upload-certs] Storing the certificates in Secret &quot;kubeadm-certs&quot; in the &quot;kube-system&quot; Namespace</span><br><span class="line">[upload-certs] Using certificate key:</span><br><span class="line">4e30c9182268879cf56b505c9b4e173325d06ab7e1ef2ad8332ef448a9297333</span><br><span class="line">[mark-control-plane] Marking the node ubuntu-001 as control-plane by adding the label &quot;node-role.kubernetes.io&#x2F;master&#x3D;&#39;&#39;&quot;</span><br><span class="line">[mark-control-plane] Marking the node ubuntu-001 as control-plane by adding the taints [node-role.kubernetes.io&#x2F;master:NoSchedule]</span><br><span class="line">[bootstrap-token] Using token: nh9l3h.4ufk3uoiyedne5mv</span><br><span class="line">[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to get nodes</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token</span><br><span class="line">[bootstrap-token] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster</span><br><span class="line">[bootstrap-token] Creating the &quot;cluster-info&quot; ConfigMap in the &quot;kube-public&quot; namespace</span><br><span class="line">[kubelet-finalize] Updating &quot;&#x2F;etc&#x2F;kubernetes&#x2F;kubelet.conf&quot; to point to a rotatable kubelet client certificate and key</span><br><span class="line">[addons] Applied essential addon: CoreDNS</span><br><span class="line">[addons] Applied essential addon: kube-proxy</span><br><span class="line"></span><br><span class="line">Your Kubernetes control-plane has initialized successfully!</span><br><span class="line"></span><br><span class="line">To start using your cluster, you need to run the following as a regular user:</span><br><span class="line"></span><br><span class="line">  mkdir -p $HOME&#x2F;.kube</span><br><span class="line">  sudo cp -i &#x2F;etc&#x2F;kubernetes&#x2F;admin.conf $HOME&#x2F;.kube&#x2F;config</span><br><span class="line">  sudo chown $(id -u):$(id -g) $HOME&#x2F;.kube&#x2F;config</span><br><span class="line"></span><br><span class="line">You should now deploy a pod network to the cluster.</span><br><span class="line">Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at:</span><br><span class="line">  https:&#x2F;&#x2F;kubernetes.io&#x2F;docs&#x2F;concepts&#x2F;cluster-administration&#x2F;addons&#x2F;</span><br><span class="line"></span><br><span class="line">You can now join any number of the control-plane node running the following command on each as root:</span><br><span class="line"></span><br><span class="line">  kubeadm join 10.0.105.121:6443 --token nh9l3h.4ufk3uoiyedne5mv \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:0ab2bf0feb7ba1ee5ebdbdc5d81f542a952e708044db5f6f2124ae5b5fe8a7cd \</span><br><span class="line">    --control-plane --certificate-key 4e30c9182268879cf56b505c9b4e173325d06ab7e1ef2ad8332ef448a9297333</span><br><span class="line"></span><br><span class="line">Please note that the certificate-key gives access to cluster sensitive data, keep it secret!</span><br><span class="line">As a safeguard, uploaded-certs will be deleted in two hours; If necessary, you can use</span><br><span class="line">&quot;kubeadm init phase upload-certs --upload-certs&quot; to reload certs afterward.</span><br><span class="line"></span><br><span class="line">Then you can join any number of worker nodes by running the following on each as root:</span><br><span class="line"></span><br><span class="line">kubeadm join 10.0.105.121:6443 --token nh9l3h.4ufk3uoiyedne5mv \</span><br><span class="line">    --discovery-token-ca-cert-hash sha256:0ab2bf0feb7ba1ee5ebdbdc5d81f542a952e708044db5f6f2124ae5b5fe8a7cd</span><br></pre></td></tr></table></figure>

<p>按照kubeadm init成功后打印提示，安装网络插件flannel，加入其它master节点及node节点</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# kubectl get node -o wide</span><br><span class="line">NAME         STATUS   ROLES    AGE   VERSION   INTERNAL-IP    EXTERNAL-IP   OS-IMAGE             KERNEL-VERSION     CONTAINER-RUNTIME</span><br><span class="line">ubuntu-001   Ready    master   53s   v1.19.3   10.0.105.121   &lt;none&gt;        Ubuntu 18.04.3 LTS   5.4.0-65-generic   containerd:&#x2F;&#x2F;1.4.3</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl get pods --all-namespaces -o wide</span><br><span class="line">NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE   IP             NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">kube-system   coredns-6d56c8448f-9fnlx             1&#x2F;1     Running   0          15m   10.244.0.2     ubuntu-001   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   coredns-6d56c8448f-vfvcg             1&#x2F;1     Running   0          15m   10.244.0.3     ubuntu-001   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   etcd-ubuntu-001                      1&#x2F;1     Running   0          16m   10.0.105.121   ubuntu-001   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-apiserver-ubuntu-001            1&#x2F;1     Running   0          16m   10.0.105.121   ubuntu-001   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-controller-manager-ubuntu-001   1&#x2F;1     Running   0          16m   10.0.105.121   ubuntu-001   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-flannel-ds-8j8cl                1&#x2F;1     Running   0          22s   10.0.105.121   ubuntu-001   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-proxy-cg2ln                     1&#x2F;1     Running   0          15m   10.0.105.121   ubuntu-001   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">kube-system   kube-scheduler-ubuntu-001            1&#x2F;1     Running   0          16m   10.0.105.121   ubuntu-001   &lt;none&gt;           &lt;none&gt;</span><br></pre></td></tr></table></figure>

<p>可以看到容器运行时使用的是containerd://1.4.3</p>
<p>默认master节点不会调度pod，去掉此限制(可选做)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# kubectl taint node --all node-role.kubernetes.io&#x2F;master:NoSchedule-</span><br><span class="line">node&#x2F;ubuntu-001 untainted</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# crictl pods</span><br><span class="line">POD ID              CREATED             STATE               NAME                                 NAMESPACE           ATTEMPT</span><br><span class="line">9cc752c5fede8       22 seconds ago      Ready               coredns-6d56c8448f-vfvcg             kube-system         0</span><br><span class="line">980935e9ea8f2       24 seconds ago      Ready               coredns-6d56c8448f-9fnlx             kube-system         0</span><br><span class="line">a4ce86d80fa28       31 seconds ago      Ready               kube-flannel-ds-8j8cl                kube-system         0</span><br><span class="line">1aa88e7a6e40c       16 minutes ago      Ready               kube-proxy-cg2ln                     kube-system         0</span><br><span class="line">bd1f9e7022de5       16 minutes ago      Ready               kube-scheduler-ubuntu-001            kube-system         0</span><br><span class="line">2dd7e343fdaaa       16 minutes ago      Ready               etcd-ubuntu-001                      kube-system         0</span><br><span class="line">96f52cba3a131       16 minutes ago      Ready               kube-controller-manager-ubuntu-001   kube-system         0</span><br><span class="line">6ab9ae42f6ff3       16 minutes ago      Ready               kube-apiserver-ubuntu-001            kube-system         0</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# docker ps</span><br><span class="line">CONTAINER ID   IMAGE     COMMAND   CREATED   STATUS    PORTS     NAMES</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# crictl ps</span><br><span class="line">CONTAINER ID        IMAGE               CREATED              STATE               NAME                      ATTEMPT             POD ID</span><br><span class="line">0cd56ff487375       bfe3a36ebd252       About a minute ago   Running             coredns                   0                   9cc752c5fede8</span><br><span class="line">d48bacaa9c210       bfe3a36ebd252       About a minute ago   Running             coredns                   0                   980935e9ea8f2</span><br><span class="line">dc2be48b4143d       f03a23d55e578       About a minute ago   Running             kube-flannel              0                   a4ce86d80fa28</span><br><span class="line">4dd35f6ad4ce3       cdef7632a242b       16 minutes ago       Running             kube-proxy                0                   1aa88e7a6e40c</span><br><span class="line">dc1baf15016e0       aaefbfa906bd8       17 minutes ago       Running             kube-scheduler            0                   bd1f9e7022de5</span><br><span class="line">f37feb9c32ad3       0369cf4303ffd       17 minutes ago       Running             etcd                      0                   2dd7e343fdaaa</span><br><span class="line">08823e6edbb1c       9b60aca1d8180       17 minutes ago       Running             kube-controller-manager   0                   96f52cba3a131</span><br><span class="line">fc78e9051b6f0       a301be0cd44bb       17 minutes ago       Running             kube-apiserver            0                   6ab9ae42f6ff3</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# ps -ef|grep containerd</span><br><span class="line">root      29906      1  0 10:57 ?        00:00:03 &#x2F;usr&#x2F;bin&#x2F;containerd</span><br><span class="line">root      29928      1  0 10:57 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;dockerd -H fd:&#x2F;&#x2F; --containerd&#x3D;&#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      30559      1  0 10:57 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 6ab9ae42f6ff3505762ce6c7676dbd74a45b6f2902c6eb32e82d70cb1d853be7 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      30631      1  0 10:57 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 96f52cba3a13165938769b136f4f786d7d073dcb85d120c9a1d45da653101545 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      30707      1  0 10:57 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 2dd7e343fdaaa0b0d54e704a429134584c5f85a8ead0623c41e242d8c92e0e50 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      30913      1  0 10:57 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id bd1f9e7022de5b8585afd4bfeabe6189eb21fd013643dff3c44dc71c5379dfa5 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      31322      1  1 10:58 ?        00:00:15 &#x2F;usr&#x2F;bin&#x2F;kubelet --bootstrap-kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;bootstrap-kubelet.conf --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;kubelet.conf --config&#x3D;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml --container-runtime&#x3D;remote --container-runtime-endpoint&#x3D;&#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      31667      1  0 10:58 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 1aa88e7a6e40cea8cb3386841b9204e0ad17ba258efcd40e28bfbcdf2f4300be -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      40524      1  0 11:13 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id a4ce86d80fa28dd289186c104698b36fc112de9195121c0d52a1b50e68007471 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      40920      1  0 11:13 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 980935e9ea8f29b3e77d4128a13da7cead0b0db068b04ac26076fe8ac9d17694 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      41153      1  0 11:14 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 9cc752c5fede80ceb14c1e1f66ed87460e3c396b881b4d53d02b5f858cbe134e -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line"></span><br><span class="line">可以看到底层的运行时还是runc，因为在我们的containerd配置文件里是这样配置的</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# crictl images ls</span><br><span class="line">IMAGE                                                             TAG                 IMAGE ID            SIZE</span><br><span class="line">docker.io&#x2F;library&#x2F;nginx                                           latest              f6d0b4767a6c4       53.6MB</span><br><span class="line">quay.io&#x2F;coreos&#x2F;flannel                                            v0.13.1-rc1         f03a23d55e578       20.7MB</span><br><span class="line">registry.aliyuncs.com&#x2F;google_containers&#x2F;coredns                   1.7.0               bfe3a36ebd252       14MB</span><br><span class="line">registry.aliyuncs.com&#x2F;google_containers&#x2F;etcd                      3.4.13-0            0369cf4303ffd       86.7MB</span><br><span class="line">registry.aliyuncs.com&#x2F;google_containers&#x2F;kube-apiserver            v1.19.3             a301be0cd44bb       29.7MB</span><br><span class="line">registry.aliyuncs.com&#x2F;google_containers&#x2F;kube-controller-manager   v1.19.3             9b60aca1d8180       28MB</span><br><span class="line">registry.aliyuncs.com&#x2F;google_containers&#x2F;kube-proxy                v1.19.3             cdef7632a242b       49.3MB</span><br><span class="line">registry.aliyuncs.com&#x2F;google_containers&#x2F;kube-scheduler            v1.19.3             aaefbfa906bd8       13.8MB</span><br><span class="line">registry.aliyuncs.com&#x2F;google_containers&#x2F;pause                     3.2                 80d28bedfe5de       300kB</span><br></pre></td></tr></table></figure>

<h2 id="使用runc运行pod"><a href="#使用runc运行pod" class="headerlink" title="使用runc运行pod"></a>使用runc运行pod</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# cat pod.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: nginx</span><br><span class="line">spec:</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl apply -f pod.yaml</span><br><span class="line">pod&#x2F;nginx created</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl get pods --all-namespaces</span><br><span class="line">NAMESPACE     NAME                                 READY   STATUS    RESTARTS   AGE</span><br><span class="line">default       nginx                                1&#x2F;1     Running   0          2m56s</span><br><span class="line">kube-system   coredns-6d56c8448f-9fnlx             1&#x2F;1     Running   0          33m</span><br><span class="line">kube-system   coredns-6d56c8448f-vfvcg             1&#x2F;1     Running   0          33m</span><br><span class="line">kube-system   etcd-ubuntu-001                      1&#x2F;1     Running   0          33m</span><br><span class="line">kube-system   kube-apiserver-ubuntu-001            1&#x2F;1     Running   0          33m</span><br><span class="line">kube-system   kube-controller-manager-ubuntu-001   1&#x2F;1     Running   0          33m</span><br><span class="line">kube-system   kube-flannel-ds-8j8cl                1&#x2F;1     Running   0          17m</span><br><span class="line">kube-system   kube-proxy-cg2ln                     1&#x2F;1     Running   0          33m</span><br><span class="line">kube-system   kube-scheduler-ubuntu-001            1&#x2F;1     Running   0          33m</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# crictl pods</span><br><span class="line">POD ID              CREATED             STATE               NAME                                 NAMESPACE           ATTEMPT</span><br><span class="line">1a7b030c70a52       26 seconds ago      Ready               nginx                                default             0</span><br><span class="line">9cc752c5fede8       17 minutes ago      Ready               coredns-6d56c8448f-vfvcg             kube-system         0</span><br><span class="line">980935e9ea8f2       17 minutes ago      Ready               coredns-6d56c8448f-9fnlx             kube-system         0</span><br><span class="line">a4ce86d80fa28       17 minutes ago      Ready               kube-flannel-ds-8j8cl                kube-system         0</span><br><span class="line">1aa88e7a6e40c       33 minutes ago      Ready               kube-proxy-cg2ln                     kube-system         0</span><br><span class="line">bd1f9e7022de5       33 minutes ago      Ready               kube-scheduler-ubuntu-001            kube-system         0</span><br><span class="line">2dd7e343fdaaa       33 minutes ago      Ready               etcd-ubuntu-001                      kube-system         0</span><br><span class="line">96f52cba3a131       33 minutes ago      Ready               kube-controller-manager-ubuntu-001   kube-system         0</span><br><span class="line">6ab9ae42f6ff3       33 minutes ago      Ready               kube-apiserver-ubuntu-001            kube-system         0</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# crictl ps</span><br><span class="line">CONTAINER ID        IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID</span><br><span class="line">8aa19e57c4507       f6d0b4767a6c4       26 seconds ago      Running             nginx                     0                   1a7b030c70a52</span><br><span class="line">0cd56ff487375       bfe3a36ebd252       17 minutes ago      Running             coredns                   0                   9cc752c5fede8</span><br><span class="line">d48bacaa9c210       bfe3a36ebd252       17 minutes ago      Running             coredns                   0                   980935e9ea8f2</span><br><span class="line">dc2be48b4143d       f03a23d55e578       17 minutes ago      Running             kube-flannel              0                   a4ce86d80fa28</span><br><span class="line">4dd35f6ad4ce3       cdef7632a242b       33 minutes ago      Running             kube-proxy                0                   1aa88e7a6e40c</span><br><span class="line">dc1baf15016e0       aaefbfa906bd8       33 minutes ago      Running             kube-scheduler            0                   bd1f9e7022de5</span><br><span class="line">f37feb9c32ad3       0369cf4303ffd       33 minutes ago      Running             etcd                      0                   2dd7e343fdaaa</span><br><span class="line">08823e6edbb1c       9b60aca1d8180       33 minutes ago      Running             kube-controller-manager   0                   96f52cba3a131</span><br><span class="line">fc78e9051b6f0       a301be0cd44bb       33 minutes ago      Running             kube-apiserver            0                   6ab9ae42f6ff3</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# ps -ef|grep containerd</span><br><span class="line">root      29906      1  0 10:57 ?        00:00:05 &#x2F;usr&#x2F;bin&#x2F;containerd</span><br><span class="line">root      29928      1  0 10:57 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;dockerd -H fd:&#x2F;&#x2F; --containerd&#x3D;&#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      30559      1  0 10:57 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 6ab9ae42f6ff3505762ce6c7676dbd74a45b6f2902c6eb32e82d70cb1d853be7 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      30631      1  0 10:57 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 96f52cba3a13165938769b136f4f786d7d073dcb85d120c9a1d45da653101545 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      30707      1  0 10:57 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 2dd7e343fdaaa0b0d54e704a429134584c5f85a8ead0623c41e242d8c92e0e50 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      30913      1  0 10:57 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id bd1f9e7022de5b8585afd4bfeabe6189eb21fd013643dff3c44dc71c5379dfa5 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      31322      1  1 10:58 ?        00:00:26 &#x2F;usr&#x2F;bin&#x2F;kubelet --bootstrap-kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;bootstrap-kubelet.conf --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;kubelet.conf --config&#x3D;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml --container-runtime&#x3D;remote --container-runtime-endpoint&#x3D;&#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      31667      1  0 10:58 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 1aa88e7a6e40cea8cb3386841b9204e0ad17ba258efcd40e28bfbcdf2f4300be -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      40524      1  0 11:13 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id a4ce86d80fa28dd289186c104698b36fc112de9195121c0d52a1b50e68007471 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      40920      1  0 11:13 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 980935e9ea8f29b3e77d4128a13da7cead0b0db068b04ac26076fe8ac9d17694 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      41153      1  0 11:14 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 9cc752c5fede80ceb14c1e1f66ed87460e3c396b881b4d53d02b5f858cbe134e -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      46631      1  0 11:31 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 1a7b030c70a524c2ce71b07b63119c11a84738818d6986d58521a9fd7cde9c69 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# ps -ef|grep 46631</span><br><span class="line">root      46631      1  0 11:31 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 1a7b030c70a524c2ce71b07b63119c11a84738818d6986d58521a9fd7cde9c69 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      46660  46631  0 11:31 ?        00:00:00 &#x2F;pause</span><br><span class="line">root      46737  46631  0 11:31 ?        00:00:00 nginx: master process nginx -g daemon off;</span><br><span class="line">root      47687  58536  0 11:34 pts&#x2F;2    00:00:00 grep --color&#x3D;auto 46631</span><br><span class="line"></span><br><span class="line">pause容器及pod 容器以containerd-shim子进程形式存在</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# ps -ef|grep 40920</span><br><span class="line">root      40920      1  0 11:13 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 980935e9ea8f29b3e77d4128a13da7cead0b0db068b04ac26076fe8ac9d17694 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      40960  40920  0 11:13 ?        00:00:00 &#x2F;pause</span><br><span class="line">root      41042  40920  0 11:13 ?        00:00:01 &#x2F;coredns -conf &#x2F;etc&#x2F;coredns&#x2F;Corefile</span><br><span class="line">root      47867  58536  0 11:34 pts&#x2F;2    00:00:00 grep --color&#x3D;auto 40920</span><br></pre></td></tr></table></figure>

<h2 id="使用kata-container运行pod"><a href="#使用kata-container运行pod" class="headerlink" title="使用kata container运行pod"></a>使用kata container运行pod</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">root@ubuntu-001:~# cat kata-runtime.yaml </span><br><span class="line">kind: RuntimeClass</span><br><span class="line">apiVersion: node.k8s.io&#x2F;v1beta1</span><br><span class="line">metadata:</span><br><span class="line">  name: kata-containers</span><br><span class="line">handler: kata</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# cat kata-pod.yaml </span><br><span class="line">apiVersion: v1</span><br><span class="line">kind: Pod</span><br><span class="line">metadata:</span><br><span class="line">  name: kata-nginx</span><br><span class="line">spec:</span><br><span class="line">  runtimeClassName: kata-containers</span><br><span class="line">  containers:</span><br><span class="line">  - name: nginx</span><br><span class="line">    image: nginx</span><br><span class="line">    ports:</span><br><span class="line">    - containerPort: 80</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl apply -f kata-runtime.yaml </span><br><span class="line">runtimeclass.node.k8s.io&#x2F;kata-containers created</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl apply -f kata-pod.yaml </span><br><span class="line">pod&#x2F;kata-nginx created</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kubectl get pods -o wide</span><br><span class="line">NAME         READY   STATUS    RESTARTS   AGE     IP           NODE         NOMINATED NODE   READINESS GATES</span><br><span class="line">kata-nginx   1&#x2F;1     Running   0          28s     10.244.0.5   ubuntu-001   &lt;none&gt;           &lt;none&gt;</span><br><span class="line">nginx        1&#x2F;1     Running   0          4h12m   10.244.0.4   ubuntu-001   &lt;none&gt;           &lt;none&gt;</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# crictl pods</span><br><span class="line">POD ID              CREATED             STATE               NAME                                 NAMESPACE           ATTEMPT</span><br><span class="line">f7f69925f3bdf       43 seconds ago      Ready               kata-nginx                           default             0</span><br><span class="line">1a7b030c70a52       4 hours ago         Ready               nginx                                default             0</span><br><span class="line">9cc752c5fede8       4 hours ago         Ready               coredns-6d56c8448f-vfvcg             kube-system         0</span><br><span class="line">980935e9ea8f2       4 hours ago         Ready               coredns-6d56c8448f-9fnlx             kube-system         0</span><br><span class="line">a4ce86d80fa28       4 hours ago         Ready               kube-flannel-ds-8j8cl                kube-system         0</span><br><span class="line">1aa88e7a6e40c       5 hours ago         Ready               kube-proxy-cg2ln                     kube-system         0</span><br><span class="line">bd1f9e7022de5       5 hours ago         Ready               kube-scheduler-ubuntu-001            kube-system         0</span><br><span class="line">2dd7e343fdaaa       5 hours ago         Ready               etcd-ubuntu-001                      kube-system         0</span><br><span class="line">96f52cba3a131       5 hours ago         Ready               kube-controller-manager-ubuntu-001   kube-system         0</span><br><span class="line">6ab9ae42f6ff3       5 hours ago         Ready               kube-apiserver-ubuntu-001            kube-system         0</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# crictl ps</span><br><span class="line">CONTAINER ID        IMAGE               CREATED             STATE               NAME                      ATTEMPT             POD ID</span><br><span class="line">4487c2621c1b4       f6d0b4767a6c4       41 seconds ago      Running             nginx                     0                   f7f69925f3bdf</span><br><span class="line">8aa19e57c4507       f6d0b4767a6c4       4 hours ago         Running             nginx                     0                   1a7b030c70a52</span><br><span class="line">0cd56ff487375       bfe3a36ebd252       4 hours ago         Running             coredns                   0                   9cc752c5fede8</span><br><span class="line">d48bacaa9c210       bfe3a36ebd252       4 hours ago         Running             coredns                   0                   980935e9ea8f2</span><br><span class="line">dc2be48b4143d       f03a23d55e578       4 hours ago         Running             kube-flannel              0                   a4ce86d80fa28</span><br><span class="line">4dd35f6ad4ce3       cdef7632a242b       5 hours ago         Running             kube-proxy                0                   1aa88e7a6e40c</span><br><span class="line">dc1baf15016e0       aaefbfa906bd8       5 hours ago         Running             kube-scheduler            0                   bd1f9e7022de5</span><br><span class="line">f37feb9c32ad3       0369cf4303ffd       5 hours ago         Running             etcd                      0                   2dd7e343fdaaa</span><br><span class="line">08823e6edbb1c       9b60aca1d8180       5 hours ago         Running             kube-controller-manager   0                   96f52cba3a131</span><br><span class="line">fc78e9051b6f0       a301be0cd44bb       5 hours ago         Running             kube-apiserver            0                   6ab9ae42f6ff3</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# ps -ef|grep containerd</span><br><span class="line">root      29906      1  0 12:21 ?        00:00:29 &#x2F;usr&#x2F;bin&#x2F;containerd</span><br><span class="line">root      29928      1  0 12:21 ?        00:00:03 &#x2F;usr&#x2F;bin&#x2F;dockerd -H fd:&#x2F;&#x2F; --containerd&#x3D;&#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      30559      1  0 12:21 ?        00:00:01 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 6ab9ae42f6ff3505762ce6c7676dbd74a45b6f2902c6eb32e82d70cb1d853be7 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      30631      1  0 12:21 ?        00:00:01 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 96f52cba3a13165938769b136f4f786d7d073dcb85d120c9a1d45da653101545 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      30707      1  0 12:21 ?        00:00:01 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 2dd7e343fdaaa0b0d54e704a429134584c5f85a8ead0623c41e242d8c92e0e50 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      30913      1  0 12:21 ?        00:00:01 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id bd1f9e7022de5b8585afd4bfeabe6189eb21fd013643dff3c44dc71c5379dfa5 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      31322      1  1 12:22 ?        00:02:30 &#x2F;usr&#x2F;bin&#x2F;kubelet --bootstrap-kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;bootstrap-kubelet.conf --kubeconfig&#x3D;&#x2F;etc&#x2F;kubernetes&#x2F;kubelet.conf --config&#x3D;&#x2F;var&#x2F;lib&#x2F;kubelet&#x2F;config.yaml --container-runtime&#x3D;remote --container-runtime-endpoint&#x3D;&#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      31667      1  0 12:22 ?        00:00:01 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 1aa88e7a6e40cea8cb3386841b9204e0ad17ba258efcd40e28bfbcdf2f4300be -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      40524      1  0 12:37 ?        00:00:01 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id a4ce86d80fa28dd289186c104698b36fc112de9195121c0d52a1b50e68007471 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      40920      1  0 12:38 ?        00:00:01 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 980935e9ea8f29b3e77d4128a13da7cead0b0db068b04ac26076fe8ac9d17694 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      41153      1  0 12:38 ?        00:00:01 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 9cc752c5fede80ceb14c1e1f66ed87460e3c396b881b4d53d02b5f858cbe134e -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      46631      1  0 12:55 ?        00:00:01 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 1a7b030c70a524c2ce71b07b63119c11a84738818d6986d58521a9fd7cde9c69 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      96541      1  0 15:40 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-kata-v2 -namespace k8s.io -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock -publish-binary &#x2F;usr&#x2F;bin&#x2F;containerd -id f7f69925f3bdfccc823f0672d7d554e4d2b7f3f7d1bf15c7db8802c03a162cff</span><br><span class="line">root      97121  58536  0 15:42 pts&#x2F;2    00:00:00 grep --color&#x3D;auto containerd</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# ps -ef|grep 96541</span><br><span class="line">root      96541      1  0 15:40 ?        00:00:00 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-kata-v2 -namespace k8s.io -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock -publish-binary &#x2F;usr&#x2F;bin&#x2F;containerd -id f7f69925f3bdfccc823f0672d7d554e4d2b7f3f7d1bf15c7db8802c03a162cff</span><br><span class="line">root      97226  58536  0 15:42 pts&#x2F;2    00:00:00 grep --color&#x3D;auto 96541</span><br><span class="line"></span><br><span class="line">运行的是轻量的虚拟机，所以不存在pause进程</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# ps -ef|grep 46631</span><br><span class="line">root      46631      1  0 12:55 ?        00:00:01 &#x2F;usr&#x2F;bin&#x2F;containerd-shim-runc-v2 -namespace k8s.io -id 1a7b030c70a524c2ce71b07b63119c11a84738818d6986d58521a9fd7cde9c69 -address &#x2F;run&#x2F;containerd&#x2F;containerd.sock</span><br><span class="line">root      46660  46631  0 12:55 ?        00:00:00 &#x2F;pause</span><br><span class="line">root      46737  46631  0 12:55 ?        00:00:00 nginx: master process nginx -g daemon off;</span><br><span class="line">root      97379  58536  0 15:43 pts&#x2F;2    00:00:00 grep --color&#x3D;auto 46631</span><br><span class="line"></span><br><span class="line">root@ubuntu-001:~# kata-runtime list</span><br><span class="line">ID                                                                 PID         STATUS      BUNDLE                                                                                                                  CREATED                          OWNER</span><br><span class="line">f7f69925f3bdfccc823f0672d7d554e4d2b7f3f7d1bf15c7db8802c03a162cff   -1          running     &#x2F;run&#x2F;containerd&#x2F;io.containerd.runtime.v2.task&#x2F;k8s.io&#x2F;f7f69925f3bdfccc823f0672d7d554e4d2b7f3f7d1bf15c7db8802c03a162cff   2021-02-08T07:40:46.667379896Z   #0</span><br><span class="line">4487c2621c1b4b2fe6214ea8d3916f1ce3fd0c691024ac947474603af7a51ea3   -1          running     &#x2F;run&#x2F;containerd&#x2F;io.containerd.runtime.v2.task&#x2F;k8s.io&#x2F;4487c2621c1b4b2fe6214ea8d3916f1ce3fd0c691024ac947474603af7a51ea3   2021-02-08T07:40:52.142467623Z   #0</span><br><span class="line"></span><br><span class="line">ID分别为kata-nginx的pod id及容器 id</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>kubernetes</category>
        <category>containerd</category>
      </categories>
      <tags>
        <tag>kubernetes</tag>
        <tag>containerd</tag>
      </tags>
  </entry>
</search>
